<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Evolution of Google Camera | LI, Jiacheng (李 家丞) </title> <meta name="author" content="Jiacheng LI"> <meta name="description" content="A deep dive into the evolution of Google Camera's algorithms and features, exploring key milestones like HDR+, Night Sight, and Super Res Zoom."> <meta name="keywords" content="Jiacheng Li, Sony Research, Sony AI, Sony, USTC"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ddlee-cn.github.io/blog/2022/Google-Camera/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> LI, Jiacheng (李 家丞) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/computational-photography/">Computational Photography</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rendering-&amp;-generative-ai/">Rendering &amp; Genenerative AI</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/streaming-&amp;-display/">Streaming &amp; Display</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/intelligent-sensing/">Intelligent Sensing</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Evolution of Google Camera</h1> <p class="post-meta"> Created on December 24, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/all"> <i class="fa-solid fa-hashtag fa-sm"></i> all</a>   <a href="/blog/tag/computational-photography"> <i class="fa-solid fa-hashtag fa-sm"></i> computational-photography</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#i-introduction">I. Introduction</a></li> <li class="toc-entry toc-h2"> <a href="#ii-hdr-and-live-hdr">II. HDR+ and Live HDR+</a> <ul> <li class="toc-entry toc-h3"><a href="#hdr-burst-photography">HDR+: Burst Photography</a></li> <li class="toc-entry toc-h3"><a href="#live-hdr-from-bilateral-filtering-to-hdrnet">Live HDR+: From Bilateral Filtering to HDRNet</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#iii-low-light-night-sight-and-astrophotography">III. Low Light: Night Sight and Astrophotography</a> <ul> <li class="toc-entry toc-h3"><a href="#night-sight">Night Sight</a></li> <li class="toc-entry toc-h3"><a href="#astrophotography">Astrophotography</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#iv-better-detail-super-res-zoom">IV. Better Detail: Super Res Zoom</a></li> <li class="toc-entry toc-h2"> <a href="#v-depth-portraits-and-semantic-understanding">V. Depth, Portraits, and Semantic Understanding</a> <ul> <li class="toc-entry toc-h3"><a href="#depth-estimation-segmentation-and-portrait-mode">Depth Estimation, Segmentation, and Portrait Mode</a></li> <li class="toc-entry toc-h3"><a href="#more-applications-alpha-matting-relighting">More applications: Alpha Matting, Relighting</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#vi-other-topics">VI. Other Topics</a> <ul> <li class="toc-entry toc-h3"><a href="#video-stabilization">Video Stabilization</a></li> <li class="toc-entry toc-h3"><a href="#denoise-and-deblur">Denoise and Deblur</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#vii-final-remarks">VII. Final Remarks</a></li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="i-introduction"><strong>I. Introduction</strong></h2> <p>Google’s approach to mobile photography has been consistently characterized by a software-first philosophy, leveraging computational photography to transcend the physical limitations of small smartphone sensors and optics. The Google Camera application, particularly on Pixel devices, stands as a testament to this philosophy, repeatedly demonstrating that algorithmic innovation can drive significant advancements in image quality and user-facing features.</p> <h2 id="ii-hdr-and-live-hdr"><strong>II. HDR+ and Live HDR+</strong></h2> <h3 id="hdr-burst-photography">HDR+: Burst Photography</h3> <p>The foundation of Google Camera’s image quality prowess was established with High Dynamic Range Plus (HDR+)<sup id="fnref:HDR"><a href="#fn:HDR" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. Introduced initially on Nexus devices and significantly enhanced on Pixel phones, HDR+ tackled the fundamental challenge of capturing scenes with a wide disparity between the darkest shadows and brightest highlights – a common scenario where small mobile sensors typically struggle.</p> <p><img src="https://i.imgur.com/XEiRljo.jpeg" alt=""></p> <p>The core algorithmic principle of HDR+ is <strong>burst photography</strong><sup id="fnref:HDRPaper"><a href="#fn:HDRPaper" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Instead of a single exposure, the camera captures a rapid sequence of deliberately underexposed frames. Underexposing protects highlight detail that would otherwise be clipped. These short-exposure frames also minimize motion blur. The captured burst, typically consisting of 2 to 15 raw images depending on conditions, then undergoes a sophisticated alignment and merging process. Alignment algorithms compensate for minor hand shake and subtle scene movements between frames. The aligned frames are then merged into an intermediate, high bit-depth computational raw image. This merging process effectively averages out noise, particularly read noise and shot noise, which are significant in underexposed shots from small sensors. The result is an image with significantly reduced noise and increased dynamic range compared to any single frame. Finally, advanced tone mapping algorithms are applied to render the high dynamic range data into a visually pleasing image that preserves detail in both shadows and highlights for standard displays.</p> <p><img src="https://i.imgur.com/ikoNItM.jpeg" alt=""></p> <p><img src="https://i.imgur.com/uLCF7m5.jpeg" alt=""></p> <p>As detailed in the release of the HDR+ Burst Photography Dataset<sup id="fnref:HDRDataset"><a href="#fn:HDRDataset" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, key improvements included transitioning to processing <strong>raw images</strong> directly from the sensor, which provided more data for the pipeline and improved image quality. Another crucial advancement was the elimination of <strong>shutter lag</strong>, ensuring the captured photo corresponded to the exact moment the shutter button was pressed. This was often achieved by utilizing frames already buffered by the camera system (Zero Shutter Lag - ZSL). Processing times and power consumption were also optimized through implementation on specialized hardware accelerators like the Qualcomm Hexagon DSP and, later, Google’s custom-designed Pixel Visual Core<sup id="fnref:VisualCore"><a href="#fn:VisualCore" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>.</p> <p><img src="https://i.imgur.com/mdtRThi.jpeg" alt=""></p> <p>The introduction of <strong>HDR+ with Bracketing</strong><sup id="fnref:HDRBracket"><a href="#fn:HDRBracket" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> addressed a limitation of the original HDR+ system: noisy shadows in very high dynamic range scenes due to all frames being underexposed.1 HDR+ with Bracketing strategically incorporates one or more longer exposure frames into the burst. For the default camera mode, this typically means capturing an additional long exposure frame <em>after</em> the shutter press (since ZSL uses pre-shutter frames). In Night Sight, multiple long exposures can be captured. The merging algorithm was updated to handle these bracketed exposures, choosing a short frame as the reference to avoid motion blur and clipped highlights from the long exposure. A sophisticated spatial merge algorithm, similar to that used in Super Res Zoom, performs deghosting to prevent artifacts from scene motion between frames of different exposures—a non-trivial task given differing noise characteristics and motion blur. This evolution resulted in improved shadow detail, more natural colors, better texture, and reduced noise, with the merging process also becoming 40% faster. Users of computational RAW also benefited from these enhancements, as the merging happens early in the pipeline using RAW data.</p> <p><img src="https://i.imgur.com/abxgoqR.jpeg" alt=""></p> <h3 id="live-hdr-from-bilateral-filtering-to-hdrnet">Live HDR+: From Bilateral Filtering to HDRNet</h3> <p><img src="https://i.imgur.com/mDKmCqS.jpeg" alt=""></p> <p>The Live HDR+<sup id="fnref:LiveHDR"><a href="#fn:LiveHDR" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> provides a <strong>real-time preview</strong> of the final result of HDR+, making HDR imaging more predictable. Thus, it is a fast approximation for multi-frame and bracket exposure process of HDR+ process, producing a HDR+ look at real-time for preview. They divide the input image into “tiles” of size roughly equal to the red patch in the figure below, and approximate HDR+ using a curve for each tile. Since these curves vary gradually, blending between curves is a good way to approximate the optimal curve at any pixel. The behind algorithm is the famous HDRNet<sup id="fnref:HDRNet"><a href="#fn:HDRNet" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>, which originates from the seminal <strong>bilateral filter</strong>.</p> <p><img src="https://i.imgur.com/AadSqTr.jpeg" alt=""></p> <p>A bilateral filter is an image processing technique that smooths images while preserving sharp edges. Unlike traditional blurring filters that can soften important details, the bilateral filter selectively averages pixels, effectively reducing noise in flat areas without affecting the clarity of boundaries. For each pixel, it considers two factors:</p> <ul> <li>Spatial Distance: How far away the neighboring pixels are.</li> <li>Intensity Difference: How different the brightness or color of the neighboring pixels is.</li> </ul> <p>A neighboring pixel is only given a high weight in the average if it is both physically close and has a similar color. This prevents pixels on one side of an edge from being averaged with pixels on the other side, thus keeping the edge sharp.</p> <p>A naive implementation of the bilateral filter calculates the value of each output pixel by iterating over all of its neighbors within a specified spatial radius. For each neighbor, it computes a weight based on both spatial distance and intensity difference, then calculates a weighted average. This process is repeated for every pixel in the image, leading to a high computational complexity, especially with large filter kernels (i.e., a large spatial sigma).</p> <p><img src="https://i.imgur.com/2bZFkv9.jpeg" alt=""></p> <p>The <strong>bilateral grid</strong><sup id="fnref:BiGrid2"><a href="#fn:BiGrid2" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> accelerates this process by performing the filtering in a much smaller, lower-dimensional space. Instead of operating directly on the 2D image, it uses a 3D data structure that represents the image’s two spatial dimensions (x, y) and its range dimension (intensity or color).</p> <p>The first step is to “splat” the information from the original, full-resolution image onto the smaller, downsampled bilateral grid. This is analogous to creating a 3D histogram. For each pixel in the input image, its value is distributed among the nearest vertices in the 3D grid.</p> <p><img src="https://i.imgur.com/NkdvU8g.jpeg" alt=""></p> <p>The second step is to apply a fast, simple 3D Gaussian blur directly on the bilateral grid. This is where the edge-preserving smoothing happens. In the grid, pixels that were close in both space and intensity in the original image are now close to each other in the 3D grid. The blur, therefore, averages their values together. Conversely, pixels that were on opposite sides of an edge (spatially close but with a large intensity difference) are far apart along the grid’s intensity dimension and are not blurred together.</p> <p><img src="https://i.imgur.com/hZq2Qw3.jpeg" alt=""></p> <p>The final step is to “slice” the blurred grid to produce the final, filtered output image. This step uses the original image as a “guidance map” to read the smoothed values back from the grid.</p> <p>For each pixel in the original image at coordinates (x, y) with intensity z, we find its corresponding position in the now-blurred 3D grid. Since the original pixel’s coordinates and intensity will not align perfectly with the grid’s discrete vertices, we perform a tri-linear interpolation between the surrounding blurred grid cells. This interpolation retrieves the final, smoothed pixel value.</p> <p>The act of looking up and interpolating a value from the grid for every pixel of the input image is what is referred to as “slicing.”<sup id="fnref:BiGrid"><a href="#fn:BiGrid" class="footnote" rel="footnote" role="doc-noteref">9</a></sup> It effectively creates a 2D “slice” of the 3D grid to form the output image. The final value is obtained by dividing the interpolated intensity sum by the interpolated weight sum from the grid.</p> <p>The other origin concept of HDRNet comes from <strong>Joint Bilateral Upsampling</strong>, a combination of principles of both bilateral filtering and guided filtering.</p> <p><img src="https://i.imgur.com/EcFttaO.jpeg" alt=""></p> <p>Instead of using a non-linear weighting scheme, the <strong>guided filter</strong><sup id="fnref:Guided"><a href="#fn:Guided" class="footnote" rel="footnote" role="doc-noteref">10</a></sup> is based on a local linear model. It assumes that the filtered output image can be expressed as a linear transformation of a guidance image within any local window. This guidance image can be the input image itself or a different image</p> <p><img src="https://i.imgur.com/nIwPAVB.jpeg" alt=""></p> <p>The primary purpose of Joint Bilateral Upsampling<sup id="fnref:BiGuided"><a href="#fn:BiGuided" class="footnote" rel="footnote" role="doc-noteref">11</a></sup> is to upsample a low-resolution image using a corresponding high-resolution image as a guide. It adapts the bilateral filter by decoupling the two kernels. When filtering the low-resolution input image:</p> <ul> <li>The Spatial Weight is calculated from the pixel coordinates in the high-resolution grid.</li> <li>The Range (Intensity) Weight is calculated using the pixel intensity values from the high-resolution guidance image.</li> </ul> <p><img src="https://i.imgur.com/7F2cDx7.jpeg" alt=""></p> <p>HDRNet’s architecture is fundamentally a two-stream design that mirrors the bilateral grid’s logic:</p> <ul> <li> <strong>A Low-Resolution “Processing” Path (The Grid)</strong>: The input image is first downsampled significantly. This low-resolution preview is fed into a deep but lightweight CNN. This network does the heavy lifting, analyzing the image content and learning the desired enhancement (e.g., tone mapping, color correction, etc.). The output of this network is not an image, but a small 3D grid of affine transformation matrices (e.g., 16x16x8). Each 3x4 matrix in this grid represents the ideal color transformation for a specific spatial location and intensity level. This low-resolution grid of learned transformations is the bilateral grid. It’s where the expensive computation happens efficiently.</li> <li> <strong>A Full-Resolution “Guidance” Path (The Input Image)</strong>: The original, full-resolution input image is kept aside and used as the “guidance map.” It provides the crucial high-frequency edge information that must be preserved.</li> </ul> <p><img src="https://i.imgur.com/nZi4kXP.jpeg" alt=""></p> <p>The magic of HDRNet lies in its custom “slicing” layer, which is a direct implementation of the joint bilateral upsampling principle. This layer’s job is to apply the learned, low-resolution transformations to the full-resolution image without introducing artifacts like halos or blurred edges.</p> <ul> <li>Lookup: For every pixel in the full-resolution input image, the slicing layer performs a lookup into the low-resolution grid of affine matrices.</li> <li>Guidance: The lookup coordinates are determined by the pixel’s properties: <ul> <li>Its spatial (x, y) position determines where to look in the grid’s spatial dimensions.</li> <li>Its intensity (brightness) value determines where to look along the grid’s depth (intensity) dimension.</li> </ul> </li> <li>Interpolation (Upsampling): Since the pixel’s coordinates and intensity won’t perfectly align with the grid’s discrete points, the slicing layer performs a trilinear interpolation between the neighboring affine transformation matrices in the grid. This step effectively “upsamples” the learned transformations, creating a unique, custom affine matrix for every single pixel in the high-resolution image.</li> <li>Application: The newly interpolated, full-resolution affine matrix is then applied to the original pixel’s color value to produce the final, enhanced output pixel.</li> </ul> <p><img src="https://i.imgur.com/aRXWy1F.jpeg" alt=""></p> <p>In essence, HDRNet’s brilliance is in this combination based on a <strong>10+ year</strong> efforts by the academic community:</p> <ul> <li>It leverages the bilateral grid as a computational framework to perform complex, expensive learning tasks in a small, low-resolution space, which is the key to its real-time speed.</li> <li>It replaces the grid’s simple blur with a powerful CNN that can learn any stylistic enhancement from data.</li> <li>It uses the principle of joint bilateral upsampling in its “slicing” layer to apply these learned, low-resolution enhancements back to the high-resolution image. The original image guides this upsampling process, ensuring that the final result has sharp, clean edges, perfectly preserving the structural integrity of the original while applying a sophisticated new look.</li> </ul> <h2 id="iii-low-light-night-sight-and-astrophotography"><strong>III. Low Light: Night Sight and Astrophotography</strong></h2> <p>Building upon the multi-frame merging principles of HDR+, Google Camera introduced Night Sight<sup id="fnref:NightSight"><a href="#fn:NightSight" class="footnote" rel="footnote" role="doc-noteref">12</a></sup>, revolutionizing low-light mobile photography without requiring flash or a tripod. Night Sight aimed to solve the inherent challenges of low-light imaging: insufficient photons leading to noise, and long exposures leading to motion blur.</p> <h3 id="night-sight">Night Sight</h3> <p><img src="https://i.imgur.com/L1pWIpq.jpeg" alt=""></p> <p>The core of Night Sight involves capturing significantly more light than a standard shot by using longer effective exposure times, achieved by merging a burst of frames.</p> <p><img src="https://i.imgur.com/FpLvFtY.jpeg" alt=""></p> <p>Key algorithmic components are:</p> <ul> <li> <p><strong>Motion Metering and Adaptive Exposure Strategy:</strong> Before capture, Night Sight measures natural hand shake and scene motion with the combination of motion estimation based on adjacement frames and angular rate measurements from the gyroscope<sup id="fnref:NightSightPaper"><a href="#fn:NightSightPaper" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>. If the phone is stable and the scene is still, it uses fewer, longer exposures (up to 1 second per frame if on a tripod, or up to 333ms handheld with minimal motion). If motion is detected, it uses more, shorter exposures (e.g., 15 frames of 1/15s or less) to minimize motion blur. This adaptive strategy is crucial for balancing noise reduction (favoring longer exposures) and sharpness (favoring shorter exposures). <img src="https://i.imgur.com/Hq7vZvc.jpeg" alt=""></p> </li> <li> <p><strong>Multi-Frame Merging and Denoising:</strong> The captured burst of dark but sharp frames is carefully aligned and merged. On Pixel 1 and 2, this utilized a modified HDR+ merging algorithm, retuned for very noisy scenes. Pixel 3 leveraged the Super Res Zoom merging algorithm, which also excels at noise reduction through averaging. This process significantly improves the signal-to-noise ratio (SNR).<br> <img src="https://i.imgur.com/i3tzKMP.jpeg" alt=""></p> </li> <li> <p><strong>Learning-Based Auto White Balance (AWB):</strong> Traditional AWB often fails in very low light. Night Sight introduced a learning-based AWB algorithm, based on FCCC<sup id="fnref:FCCC"><a href="#fn:FCCC" class="footnote" rel="footnote" role="doc-noteref">14</a></sup>, trained to recognize and correct color casts, ensuring natural color rendition even in challenging mixed lighting. This model was trained by manually correcting the white balance of numerous low-light scenes, with a newly introduced error metric for more accurate and balanced target.</p> </li> </ul> <h3 id="astrophotography">Astrophotography</h3> <p><img src="https://i.imgur.com/6sfg2VF.jpeg" alt=""></p> <ul> <li> <strong>Extended Multi-Frame Exposures:</strong> To capture enough light from faint celestial objects, total exposure is split into a sequence of frames, each with an exposure time short enough (e.g., up to 16 seconds per frame) to render stars as points rather than trails caused by Earth’s rotation.</li> <li> <strong>Advanced Noise Reduction:</strong> <ul> <li> <strong>Dark Current and Hot Pixel Correction:</strong> Long exposures exacerbate sensor artifacts like dark current (spurious signal even with no light) and hot/warm pixels (pixels that incorrectly report high values). These are identified by comparing neighboring pixel values within a frame and across the sequence, and outliers are concealed by averaging neighbors<sup id="fnref:Astrophotography"><a href="#fn:Astrophotography" class="footnote" rel="footnote" role="doc-noteref">15</a></sup>.</li> <li> <strong>Sky-Specific Denoising:</strong> The algorithm recognizes that noise characteristics can differ between the sky and foreground.</li> </ul> </li> <li> <strong>Sky Segmentation and Optimization:</strong> An on-device Convolutional Neural Network (CNN), trained on over 100,000 manually labeled images, identifies sky regions in the photograph. This allows for selective processing, such as targeted contrast enhancement<sup id="fnref:Sky"><a href="#fn:Sky" class="footnote" rel="footnote" role="doc-noteref">16</a></sup> or darkening of the sky to counteract the tendency of low-light amplification to make the night sky appear unnaturally bright. This segmentation is crucial for realistic rendering.</li> </ul> <h2 id="iv-better-detail-super-res-zoom">IV. Better Detail: Super Res Zoom</h2> <p>Smartphones traditionally struggled with zoom, as physical space constraints limit the inclusion of complex optical zoom lens systems found in DSLR cameras. Digital zoom, which typically involves cropping and upscaling a single image, results in significant loss of detail and often introduces artifacts. Google addressed this with Super Res Zoom<sup id="fnref:SuperRes"><a href="#fn:SuperRes" class="footnote" rel="footnote" role="doc-noteref">17</a></sup>, a computational approach to achieve optical-like zoom quality without traditional optical zoom hardware (for modest zoom factors).</p> <p><img src="https://i.imgur.com/iA0yZbQ.jpeg" alt=""></p> <p>The core algorithmic principle of Super Res Zoom is <strong>multi-frame super-resolution</strong><sup id="fnref:SuperRes:1"><a href="#fn:SuperRes" class="footnote" rel="footnote" role="doc-noteref">17</a></sup>. Instead of relying on a single frame, it leverages the burst of frames captured by HDR+. The key insight is that natural hand tremor, even when imperceptible, causes slight shifts in the camera’s viewpoint between successive frames in a burst. When combined with Optical Image Stabilization (OIS) that can actively introduce tiny, controlled sub-pixel shifts, each frame captures a slightly different sampling of the scene. By aligning these multiple, slightly offset low-resolution frames and merging them onto a higher-resolution grid, Super Res Zoom can reconstruct details that would be lost in any single frame.</p> <p><img src="https://i.imgur.com/8oGyqVz.jpeg" alt=""></p> <p>This multi-frame approach is fundamentally different from single-frame upscaling techniques like RAISR (Rapid and Accurate Image Super-Resolution)<sup id="fnref:RAISR"><a href="#fn:RAISR" class="footnote" rel="footnote" role="doc-noteref">18</a></sup>, which Google also developed and uses for enhancing visual quality. While RAISR can improve the appearance of an already captured image, the primary resolution gain in Super Res Zoom (especially for modest zoom factors like 2-3x) comes from the multi-frame merging process itself.</p> <p><img src="https://i.imgur.com/TIqaiJr.jpeg" alt=""></p> <p>Implementing multi-frame super-resolution on a handheld device presents significant challenges:</p> <ul> <li> <strong>Noise:</strong> Single frames in a burst, especially if underexposed for HDR+, can be noisy. The algorithm must be robust to this noise and aim to produce a less noisy, higher-resolution result</li> <li> <strong>Complex Scene Motion:</strong> Objects in the scene (leaves, water, people) can move independently of camera motion. Reliable alignment and merging in the presence of such complex, sometimes non-rigid, motion is difficult. The algorithm needs to work even with imperfect motion estimation and incorporate deghosting mechanisms.</li> <li> <strong>Irregular Data Spread:</strong> Due to random hand motion and scene motion, the sampling of the high-resolution grid can be irregular – dense in some areas, sparse in others. This makes the interpolation problem complex.</li> </ul> <p><img src="https://i.imgur.com/0THZIPp.jpeg" alt=""></p> <p>Super Res Zoom addresses these by integrating sophisticated alignment and merging algorithms that are aware of noise and can handle local misalignments to prevent ghosting, similar to the advanced merging techniques developed for HDR+ with Bracketing. The system intelligently selects and weights information from different frames to reconstruct the final zoomed image.</p> <p><img src="https://i.imgur.com/SA8u8cX.jpeg" alt=""></p> <p>Like Live HDR+, the merging and interpolation algorithm of Super Res Zoom comes from a <strong>10+ year</strong> idea of kernel regression for image processing<sup id="fnref:SKR"><a href="#fn:SKR" class="footnote" rel="footnote" role="doc-noteref">19</a></sup>. Its primary contribution is to connect the statistical method of kernel regression to various image processing tasks. Kernel regression is a non-parametric technique used to estimate a function or value at a specific point by calculating a weighted average of its neighbors. The core idea is simple: closer points should have more influence. This influence is defined by a “kernel,” which is a weighting function that decreases with distance.</p> <p><img src="https://i.imgur.com/LcOspTY.jpeg" alt=""></p> <p>The framework’s power is significantly enhanced by the use of adaptive kernels. Instead of using a fixed, symmetric kernel (like a simple circle or square), the method analyzes the local image structure to “steer” the kernel. This means the kernel’s shape, orientation, and size are adapted on-the-fly:</p> <ul> <li>Near an edge, the kernel elongates and orients itself to lie parallel to the edge, thereby averaging pixels along the edge but not across it.</li> <li>In flat, textureless regions, the kernel remains more uniform and circular.</li> </ul> <p>This data-adaptive approach allows for superior preservation of sharp details and textures compared to methods using fixed kernels.</p> <p>In my works, <a href="/publications/#learning%20steerable%20resampling">LeRF</a> and <a href="/publications/#lerf:">LeRF++</a>, we push this direction a step foward by introducing a <strong>learning-based parametric CNN</strong> for the prediction of kernel shapes, which is further accelerated by <a href="/streaming-&amp;-display/"><strong>LUTs</strong></a> to achieve adaptive and efficient interpolation.</p> <h2 id="v-depth-portraits-and-semantic-understanding"><strong>V. Depth, Portraits, and Semantic Understanding</strong></h2> <p>Google Camera’s Portrait Mode, which simulates the shallow depth-of-field effect (bokeh) typically associated with DSLR cameras using wide-aperture lenses, has undergone significant algorithmic evolution, heavily relying on advancements in depth estimation and machine learning for semantic understanding.</p> <h3 id="depth-estimation-segmentation-and-portrait-mode">Depth Estimation, Segmentation, and Portrait Mode</h3> <p><img src="https://i.imgur.com/C4Yfzp2.jpeg" alt=""></p> <ul> <li> <strong>Pixel 2 (2017): Single Camera Depth via Dual-Pixel Auto-Focus (PDAF) and ML Segmentation:</strong> The Pixel 2, despite having a single rear camera, introduced Portrait Mode by ingeniously using its dual-pixel auto-focus (PDAF) sensor. Each pixel on a PDAF sensor is split into two photodiodes, capturing two slightly different perspectives of the scene (a very short baseline stereo pair). The parallax (apparent shift) between these two sub-pixel views can be used to compute a depth map. This depth map, combined with a machine learning model trained to segment people from the background, allowed for the initial bokeh effect. For the front-facing camera, which initially lacked PDAF-based depth, segmentation was achieved purely through an ML model<sup id="fnref:Portrait"><a href="#fn:Portrait" class="footnote" rel="footnote" role="doc-noteref">20</a></sup>.</li> <li> <strong>Pixel 3 (2018): ML-Enhanced Depth from PDAF:</strong> The Pixel 3 improved upon this by using machine learning to directly predict depth from the PDAF pixel data<sup id="fnref:Portrait2"><a href="#fn:Portrait2" class="footnote" rel="footnote" role="doc-noteref">21</a></sup>. Instead of a traditional stereo algorithm, a Convolutional Neural Network (CNN) trained in TensorFlow took the two PDAF views as input and learned to predict a higher quality depth map<sup id="fnref:SynDepth"><a href="#fn:SynDepth" class="footnote" rel="footnote" role="doc-noteref">22</a></sup>. This ML approach was better at handling errors common with traditional stereo, such as those around repeating patterns or textureless surfaces, and could leverage semantic cues (e.g., recognizing a face to infer its distance).</li> </ul> <h3 id="more-applications-alpha-matting-relighting">More applications: Alpha Matting, Relighting</h3> <p><img src="https://i.imgur.com/Q2w9LuP.jpeg" alt=""></p> <p>While depth maps are crucial for determining the <em>amount</em> of blur, accurately separating the subject from the background, especially around fine details like hair, requires more than just depth. This is where semantic segmentation and alpha matting come into play.</p> <ul> <li>Early Semantic Segmentation: From its inception, Portrait Mode used ML-based semantic segmentation to identify people in the scene, creating a mask to distinguish foreground from background. This mask was then refined by the depth map.</li> <li>Pixel 6 (2021): High-Resolution ML Alpha Matting<sup id="fnref:Matting"><a href="#fn:Matting" class="footnote" rel="footnote" role="doc-noteref">23</a></sup>: A major leap in subject separation for selfies came with the Pixel 6, which introduced a new ML-based approach for Portrait Matting to estimate a high-resolution and accurate alpha matte. An alpha matte specifies the opacity of each pixel, allowing for very fine-grained foreground-background compositing.<br> The Portrait Matting model is a fully convolutional neural network with a MobileNetV3 backbone and encoder-decoder blocks. It takes the color image and an initial coarse alpha matte (from a low-resolution person segmenter) as input. It first predicts a refined low-resolution matte, then a shallow encoder-decoder refines this to a high-resolution matte, focusing on structural features and fine details like individual hair strands. This model was trained using a sophisticated dataset: <ol> <li> <strong>Light Stage Data:</strong> High-quality ground truth alpha mattes were generated using Google’s Light Stage, a volumetric capture system with 331 LED lights and high-resolution cameras/depth sensors. This allowed for “ratio matting” (silhouetting against an illuminated background) to get precise mattes. These subjects were then relit and composited onto various backgrounds.</li> <li> <strong>In-the-Wild Portraits:</strong> To improve generalization, pseudo-ground truth mattes were generated for a large dataset of in-the-wild Pixel photos using an ensemble of existing matting models and test-time augmentation. This high-quality alpha matte allows for much more accurate bokeh rendering around complex boundaries like hair, significantly reducing artifacts where the background might have remained sharp or the foreground was incorrectly blurred.</li> </ol> </li> </ul> <p>Further, they design a novel system<sup id="fnref:Relight"><a href="#fn:Relight" class="footnote" rel="footnote" role="doc-noteref">24</a></sup> for portrait relighting and background replacement, which maintains high-frequency boundary details and accurately synthesizes the subject’s appearance as lit by novel illumination, thereby producing realistic composite images for any desired scene. The key componenets include foreground estimation via alpha matting, relighting, and compositing.</p> <p><img src="https://i.imgur.com/GFENBBl.jpeg" alt=""></p> <p>The relighting module is divided into three sequential steps. A first Geometry Network estimates per-pixel surface normals from the input foreground. The surface normals and foreground 𝐹 are used to generate the albedo. The target HDR lighting environment is prefiltered using diffuse and specular convolution operations, and then these prefiltered maps are sampled using surface normals or reflection vectors, producing a per-pixel representation of diffuse and specular reflectance for the target illumination (light maps). Finally, a Shading Network produces the final relit foreground.</p> <p><img src="https://i.imgur.com/fbP6oqQ.jpeg" alt=""></p> <h2 id="vi-other-topics"><strong>VI. Other Topics</strong></h2> <h3 id="video-stabilization">Video Stabilization</h3> <p><img src="https://i.imgur.com/4ZoCoCj.jpeg" alt=""></p> <p>Fused Video Stabilization is a hybrid approach that combines Optical Image Stabilization (OIS) with Electronic Image Stabilization (EIS) to produce smooth, professional-looking videos.</p> <p>The core idea is to address common problems like camera shake and motion blur. The system uses the phone’s gyroscope and accelerometer to precisely measure and predict the user’s intended motion. The OIS hardware compensates for small, high-frequency jitters, while the EIS software handles larger motions and corrects for other distortions like rolling shutter (“jello” effect). By intelligently fusing these two methods, the technology delivers exceptionally stable video that mimics the look of footage shot with professional camera equipment.</p> <h3 id="denoise-and-deblur">Denoise and Deblur</h3> <p>The denosing takes advantage of self-similarity of patches across the image to denoise with high fidelity. The general principle behind the seminal “non-local” denoising is that noisy pixels can be denoised by averaging pixels with similar local structure. However, these approaches typically incur high computational costs because they require a brute force search for pixels with similar local structure, making them impractical for on-device use. In the “pull-push” approach, the algorithmic complexity is decoupled from the size of filter footprints thanks to effective information propagation across spatial scales.</p> <p>Instead of tackling severe motion blur, the deblur<sup id="fnref:Deblur"><a href="#fn:Deblur" class="footnote" rel="footnote" role="doc-noteref">25</a></sup> function focuses on “mild” blur—the subtle loss of sharpness caused by minor camera shake, small focus errors, or lens optics. The proposed method, Polyblur<sup id="fnref:Polyblur"><a href="#fn:Polyblur" class="footnote" rel="footnote" role="doc-noteref">26</a></sup>, is a two-stage process designed to be fast enough to run in a fraction of a second on mobile devices.</p> <p><img src="https://i.imgur.com/he7qCci.jpeg" alt=""></p> <h2 id="vii-final-remarks"><strong>VII. Final Remarks</strong></h2> <p>The Google Camera team, led by Prof. Marc Levoy<sup id="fnref:Marc"><a href="#fn:Marc" class="footnote" rel="footnote" role="doc-noteref">27</a></sup> and Prof. Peyman Milanfar<sup id="fnref:Peymann"><a href="#fn:Peymann" class="footnote" rel="footnote" role="doc-noteref">28</a></sup> later, contributed a lot to the advancement in both academia and massive application in industry of computational photography technology, and had a deep influence to myself in terms of both research taste and ideas. Personally, I want to appreciate their openness.</p> <p>Look back to the evolution of Google Camera, a clear trend is the <strong>iterative enhancement of core techniques</strong>, including HDR+, Night Sight, and Super Res Zoom. Another key is takeaway the synergy between <strong>software, hardware, and data-driven approach</strong>.</p> <p><img src="https://i.imgur.com/o3S4PcU.jpeg" alt=""></p> <p>The ultimate goal of computatiobal photography goes beyond match the human vision in terms of spatial, temporal, sectral resolution. From my point of view, future trends include intergration of other sensor modality, e.g., <a href="/publications/#multi-spectral">multi-spectral</a>, take advantage of on-device generative AI for creative post-processing, e.g., applying personalized photographic style, and adaptation to novel capture devices, e.g., AR Glasses &amp; Wearables.</p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:HDR"> <p><a href="https://research.google/blog/hdr-low-light-and-high-dynamic-range-photography-in-the-google-camera-app/" rel="external nofollow noopener" target="_blank">HDR+: Low Light and High Dynamic Range photography in the Google Camera App</a> <a href="#fnref:HDR" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:HDRPaper"> <p><a href="https://dl.acm.org/doi/10.1145/2980179.2980254" rel="external nofollow noopener" target="_blank">Burst photography for high dynamic range and low-light imaging on mobile cameras</a>, in SIGGRAPH 2016 <a href="#fnref:HDRPaper" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:HDRDataset"> <p><a href="https://research.google/blog/introducing-the-hdr-burst-photography-dataset/" rel="external nofollow noopener" target="_blank">Introducing the HDR+ Burst Photography Dataset</a> <a href="#fnref:HDRDataset" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:VisualCore"> <p><a href="https://blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/" rel="external nofollow noopener" target="_blank">Pixel Visual Core: image processing and machine learning on Pixel 2</a> <a href="#fnref:VisualCore" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:HDRBracket"> <p><a href="https://research.google/blog/hdr-with-bracketing-on-pixel-phones/" rel="external nofollow noopener" target="_blank">HDR+ with Bracketing on Pixel Phones</a> <a href="#fnref:HDRBracket" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:LiveHDR"> <p><a href="https://research.google/blog/live-hdr-and-dual-exposure-controls-on-pixel-4-and-4a/" rel="external nofollow noopener" target="_blank">Live HDR+ and Dual Exposure Controls on Pixel 4 and 4a</a> <a href="#fnref:LiveHDR" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:HDRNet"> <p><a href="https://groups.csail.mit.edu/graphics/hdrnet/" rel="external nofollow noopener" target="_blank">Deep bilateral learning for real-time image enhancement</a>, in SIGGRPAH 2017 <a href="#fnref:HDRNet" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:BiGrid2"> <p><a href="https://dl.acm.org/doi/10.1145/1276377.1276506" rel="external nofollow noopener" target="_blank">Real-time Edge-Aware Image Processing with the Bilateral Grid</a>, in SIGGRAPH 2007 <a href="#fnref:BiGrid2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:BiGrid"> <p><a href="https://link.springer.com/chapter/10.1007/11744085_44" rel="external nofollow noopener" target="_blank">A Fast Approximation of the Bilateral Filter using a Signal Processing Approach</a>, in ECCV 2006 <a href="#fnref:BiGrid" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Guided"> <p><a href="https://ieeexplore.ieee.org/document/6319316/" rel="external nofollow noopener" target="_blank">Guided Image Filtering</a>, in T-PAMI 2013 <a href="#fnref:Guided" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:BiGuided"> <p><a href="https://dl.acm.org/doi/10.1145/2980179.2982423" rel="external nofollow noopener" target="_blank">Bilateral guided upsampling</a>, in SIGGRAPH Asia 2016 <a href="#fnref:BiGuided" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:NightSight"> <p><a href="https://research.google/blog/night-sight-seeing-in-the-dark-on-pixel-phones/" rel="external nofollow noopener" target="_blank">Night Sight: Seeing in the Dark on Pixel Phones</a> <a href="#fnref:NightSight" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:NightSightPaper"> <p><a href="https://dl.acm.org/doi/10.1145/3355089.3356508" rel="external nofollow noopener" target="_blank">Handheld Mobile Photography in Very Low Light</a>, in TOG 2019 <a href="#fnref:NightSightPaper" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:FCCC"> <p><a href="https://research.google/pubs/fast-fourier-color-constancy/" rel="external nofollow noopener" target="_blank">Fast Fourier Color Constancy</a>, in CVPR 2017 <a href="#fnref:FCCC" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Astrophotography"> <p><a href="https://research.google/blog/astrophotography-with-night-sight-on-pixel-phones/" rel="external nofollow noopener" target="_blank">Astrophotography with Night Sight on Pixel Phones</a> <a href="#fnref:Astrophotography" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Sky"> <p><a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Liba_Sky_Optimization_Semantically_Aware_Image_Processing_of_Skies_in_Low-Light_CVPRW_2020_paper.pdf" rel="external nofollow noopener" target="_blank">Sky Optimization: Semantically aware image processing of skies in low-light photography</a>, in CVPRW 2020 <a href="#fnref:Sky" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:SuperRes"> <p><a href="https://research.google/blog/see-better-and-further-with-super-res-zoom-on-the-pixel-3/" rel="external nofollow noopener" target="_blank">See Better and Further with Super Res Zoom on the Pixel 3</a> <a href="#fnref:SuperRes" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:SuperRes:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:RAISR"> <p><a href="https://ieeexplore.ieee.org/iel7/6745852/6960042/07744595.pdf" rel="external nofollow noopener" target="_blank">Rapid and Accurate Image Super-Resolution</a>, in TCI 2017 <a href="#fnref:RAISR" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:SKR"> <p><a href="http://ieeexplore.ieee.org/document/4060955/" rel="external nofollow noopener" target="_blank">Kernel Regression for Image Processing and Reconstruction</a>, in T-IP 2007 <a href="#fnref:SKR" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Portrait"> <p><a href="https://research.google/blog/portrait-mode-on-the-pixel-2-and-pixel-2-xl-smartphones/" rel="external nofollow noopener" target="_blank">Portrait mode on the Pixel 2 and Pixel 2 XL smartphones</a> <a href="#fnref:Portrait" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Portrait2"> <p><a href="https://research.google/blog/learning-to-predict-depth-on-the-pixel-3-phones/" rel="external nofollow noopener" target="_blank">Learning to Predict Depth on the Pixel 3 Phones</a> <a href="#fnref:Portrait2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:SynDepth"> <p><a href="https://dl.acm.org/doi/10.1145/3197517.3201329" rel="external nofollow noopener" target="_blank">Synthetic depth-of-field with a single-camera mobile phone</a>, in TOG 2018 <a href="#fnref:SynDepth" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Matting"> <p><a href="https://research.google/blog/accurate-alpha-matting-for-portrait-mode-selfies-on-pixel-6/" rel="external nofollow noopener" target="_blank">Accurate Alpha Matting for Portrait Mode Selfies on Pixel 6</a> <a href="#fnref:Matting" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Relight"> <p><a href="https://augmentedperception.github.io/total_relighting/" rel="external nofollow noopener" target="_blank">Learning to Relight Portraits for Background Replacement</a>, in SIGGRPAH 2021 <a href="#fnref:Relight" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Deblur"> <p><a href="https://research.google/blog/take-all-your-pictures-to-the-cleaners-with-google-photos-noise-and-blur-reduction/" rel="external nofollow noopener" target="_blank">Take All Your Pictures to the Cleaners, with Google Photos Noise and Blur Reduction</a> <a href="#fnref:Deblur" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Polyblur"> <p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502555" rel="external nofollow noopener" target="_blank">Polyblur: Removing Mild Blur by Polynomial Reblurring</a>, in T-CI 2017 <a href="#fnref:Polyblur" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Marc"> <p><a href="https://graphics.stanford.edu/~levoy/" rel="external nofollow noopener" target="_blank">Marc Levoy</a> <a href="#fnref:Marc" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Peymann"> <p><a href="https://sites.google.com/view/milanfarhome/" rel="external nofollow noopener" target="_blank">Peyman Milanfar</a> <a href="#fnref:Peymann" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/DLSS4/">NVIDIA DLSS 4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/AMD-FSR/">AMD FidelityFX Super Resolution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/CG-Pipeline/">The Real-Time Rendering Pipeline</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jiacheng LI. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>