<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Anchor-Free Object Detection | LI, Jiacheng (李 家丞) </title> <meta name="author" content="Jiacheng LI"> <meta name="description" content="An overview and summary of anchor-free series detection works."> <meta name="keywords" content="Jiacheng Li, Sony Research, Sony AI, Sony, USTC"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ddlee-cn.github.io/blog/2020/Anchor-Free-Object-Detection/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> LI, Jiacheng (李 家丞) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/computational-photography/">Computational Photography</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rendering-&amp;-generative-ai/">Rendering &amp; Genenerative AI</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/streaming-&amp;-display/">Streaming &amp; Display</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/intelligent-sensing/">Intelligent Sensing</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Anchor-Free Object Detection</h1> <p class="post-meta"> Created on April 12, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/all"> <i class="fa-solid fa-hashtag fa-sm"></i> all</a>   <a href="/blog/tag/intelligent-sensing"> <i class="fa-solid fa-hashtag fa-sm"></i> intelligent-sensing</a>   <a href="/blog/tag/object-detection"> <i class="fa-solid fa-hashtag fa-sm"></i> object-detection</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#cornernet-detecting-objects-as-paired-keypoints---eccv-2018">CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018</a></li> <li class="toc-entry toc-h2"><a href="#cornernet-lite-efficient-keypoint-based-object-detection">CornerNet-Lite: Efficient Keypoint Based Object Detection</a></li> <li class="toc-entry toc-h2"><a href="#extremenetbottom-up-object-detection-by-grouping-extreme-and-center-points---cvpr-2019">(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019</a></li> <li class="toc-entry toc-h2"><a href="#centernet-dcenternet-keypoint-triplets-for-object-detection---iccv-2019">(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019</a></li> <li class="toc-entry toc-h2"><a href="#centernet-zobjects-as-points">(CenterNet-Z)Objects as Points</a></li> <li class="toc-entry toc-h2"><a href="#fsaffeature-selective-anchor-free-module-for-single-shot-object-detection---cvpr-2019">(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019</a></li> <li class="toc-entry toc-h2"><a href="#foveabox-beyond-anchor-based-object-detector">FoveaBox: Beyond Anchor-based Object Detector</a></li> <li class="toc-entry toc-h2"><a href="#fcos-fully-convolutional-one-stage-object-detection---iccv-2019">FCOS: Fully Convolutional One-Stage Object Detection - ICCV 2019</a></li> <li class="toc-entry toc-h2"><a href="#reppoints-point-set-representation-for-object-detection---iccv-2019">RepPoints: Point Set Representation for Object Detection - ICCV 2019</a></li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="cornernet-detecting-objects-as-paired-keypoints---eccv-2018"><strong>CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018</strong></h2> <p><img src="https://i.imgur.com/npESC4v.jpg" alt="CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018"></p> <p>The model detect an object as a pair of bounding box corners grouped together. A convolutional network outputs a heatmap for all top-left corners, a heatmap for all bottom-right corners, and an embedding vector for each detected corner. The network is trained to predict similar embeddings for corners that belong to the same object.</p> <p><img src="https://i.imgur.com/wHrOzEW.jpg" alt="CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018"></p> <p>The backbone network is followed by two prediction modules, one for the top-left corners and the other for the bottom-right corners. Using the predictions from both modules, we locate and group the corners.</p> <p><img src="https://i.imgur.com/YyHEujb.jpg" alt="CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/princeton-vl/CornerNet" rel="external nofollow noopener" target="_blank">PyTorch</a></p> <h2 id="cornernet-lite-efficient-keypoint-based-object-detection"><strong>CornerNet-Lite: Efficient Keypoint Based Object Detection</strong></h2> <p>CornerNet-Saccade speeds up inference by reducing the number of pixels to process. It uses an attention mechanism similar to saccades in human vision. It starts with a downsized full image and generates an attention map, which is then zoomed in on and processed further by the model. This differs from the original CornerNet in that it is applied fully convolutionally across multiple scales.</p> <p><img src="https://i.imgur.com/Zzu1wRl.jpg" alt="CornerNet-Lite: Efficient Keypoint Based Object Detection"></p> <p>We predict a set of possible object locations from the attention maps and bounding boxes generated on a downsized full image. We zoom into each location and crop a small region around that location. Then we detect objects in each region. We control the efficiency by ranking the object locations and choosing top k locations to process. Finally, we merge the detections by NMS.</p> <p><img src="https://i.imgur.com/DTkcDKz.png" alt="CornerNet-Lite: Efficient Keypoint Based Object Detection"></p> <p>CornerNet-Squeeze speeds up inference by reducing the amount of processing per pixel. It incorporates ideas from SqueezeNet and MobileNets, and introduces a new, compact hourglass backbone that makes extensive use of 1×1 convolution, bottleneck layer, and depth-wise separable convolution.</p> <p><img src="https://i.imgur.com/iXqDI10.png" alt="CornerNet-Lite: Efficient Keypoint Based Object Detection"></p> <h2 id="extremenetbottom-up-object-detection-by-grouping-extreme-and-center-points---cvpr-2019"><strong>(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019</strong></h2> <p><img src="https://i.imgur.com/AsbOUJw.png" alt="(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019"></p> <p>In this paper, we propose ExtremeNet, a bottom-up object detection framework that detects four extreme points (top-most, left-most, bottom-most, right-most) of an object. We use a state-of-the-art keypoint estimation framework to find extreme points, by predicting four multi-peak heatmaps for each object category. In addition, we use one heatmap per category predicting the object center, as the average of two bounding box edges in both the x and y dimension. We group extreme points into objects with a purely geometry-based approach. We group four extreme points, one from each map, if and only if their geometric center is predicted in the center heatmap with a score higher than a pre-defined threshold. We enumerate all $O(n^4)$combinations of extreme point prediction, and select the valid ones.</p> <p><img src="https://i.imgur.com/Ek7WhEw.png" alt="(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019"></p> <p>Given four extreme points t, b, r, l extracted from heatmaps Ŷ (t) , Ŷ (l) , Ŷ (b) , Ŷ (r), we compute their geometric center $c=\left(\frac{l_{x}+t_{x}}{2}, \frac{t_{y}+b_{y}}{2}\right)$. If this center is predicted2 2with a high response in the center map Ŷ (c), we commit the extreme points as a valid detection: Ŷcx ,cy ≥ τc for a threshold τc. We then enumerate over all quadruples of keypoints t, b, r, l in a brute force manner. We extract detections foreach class independently.</p> <p><img src="https://i.imgur.com/TYhe4Al.png" alt="(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/xingyizhou/ExtremeNet" rel="external nofollow noopener" target="_blank">PyTorch</a></p> <h2 id="centernet-dcenternet-keypoint-triplets-for-object-detection---iccv-2019"><strong>(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019</strong></h2> <p><img src="https://i.imgur.com/6pFsCoQ.png" alt="(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019"></p> <p>In this paper, we present a low-cost yet effective solution named CenterNet, which explores the central part of a proposal, i.e., the region that is close to the geometric center of a box, with one extra keypoint. We intuit that if a predicted bounding box has a high IoU with the ground-truth box, then the probability that the center keypoint in the central region of the bounding box will be predicted as the same class is high, and vice versa. Thus, during inference, after a proposal is generated as a pair of corner keypoints, we determine if the proposal is indeed an object by checking if there is a center keypoint of the same class falling within its central region.</p> <p><img src="https://i.imgur.com/MFkqXVY.jpg" alt="(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019"></p> <p>A convolutional backboneoutput two corner heatmaps and a center keypoint heatmap, respectively.the similar embeddings are used to detect a potential boundingthe final bounding boxes. network applies cascade corner pooling and center pooling to Similar to CornerNet, a pair of detected corners and box. Then the detected center keypoints are used to determine the final bounding boxes.</p> <p><img src="https://i.imgur.com/ozuZHNW.jpg" alt="(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/Duankaiwen/CenterNet" rel="external nofollow noopener" target="_blank">PyTorch</a></p> <h2 id="centernet-zobjects-as-points"><strong>(CenterNet-Z)Objects as Points</strong></h2> <p><img src="https://i.imgur.com/OCXfp70.png" alt="(CenterNet-Z)Objects as Points"></p> <p>We represent objects by a single point at their bounding box center. Other properties, such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard keypoint estimation problem. We simply feed the input image to a fully convolutional network that generates a heatmap. Peaks in this heatmap correspond to object centers. Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised learning. Inference is a single network forward-pass, without non-maximal suppression for post-processing.</p> <p><img src="https://i.imgur.com/qwbBPGi.png" alt="(CenterNet-Z)Objects as Points"></p> <p>The numbers in the boxes represent the stride to the image. (a): Hourglass Network, We use it as is in CornerNet.. (b): ResNet with transpose convolutions. We add one 3 × 3 deformable convolutional layer before each up-sampling layer. Specifically, we first use deformable convolution to change the channels and then use transposed convolution to upsample the feature map (such two steps are shown separately in 32 → 16. We show these two steps together as a dashed arrow for 16 → 8 and 8 → 4). (c): The original DLA-34 for semantic segmentation. (d): Our modified DLA-34. We add more skip connections from the bottom layers and upgrade every convolutional layer in upsampling stages to deformable convolutional layer.</p> <p><img src="https://i.imgur.com/GA6wNG7.png" alt="(CenterNet-Z)Objects as Points"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/xingyizhou/CenterNet" rel="external nofollow noopener" target="_blank">PyTorch</a></p> <h2 id="fsaffeature-selective-anchor-free-module-for-single-shot-object-detection---cvpr-2019"><strong>(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019</strong></h2> <p><img src="https://i.imgur.com/ObEG4iU.png" alt="(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019"></p> <p>The motivation is to let each instance select the best level of feature freely to optimize the network, so there should be no anchor boxes to constrain the feature selection in our module. Instead, we encode the instances in an anchor-free manner to learn the parameters for classification and regression. An anchor-free branch is built per level of feature pyramid, independent to the anchor-based branch. Similar to the anchor-based branch, it consists of a classification subnet and a regression subnet. An instance can be assigned to arbitrary level of the anchor-free branch. During training, we dynamically select the most suitable level of feature for each instance based on the instance content instead of just the size of instance box. The selected level of feature then learns to detect the assigned instances. At inference, the FSAF module can run independently or jointly with anchor-based branches.</p> <p><img src="https://i.imgur.com/rspa42e.png" alt="(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019"></p> <p>The FSAF module only introduces two additional conv layers (dashed feature maps) per pyramid level, keeping the architecture fully convolutional.</p> <p><img src="https://i.imgur.com/3tsiCZG.png" alt="(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019"></p> <p>Online feature selection mechanism. Each instance is passing through all levels of anchor-free branches to compute the averaged classification (focal) loss and regression (IoU) loss over effective regions. Then the level with minimal summation of two losses is selected to set up the supervision signals for that instance.</p> <p><img src="https://i.imgur.com/GmH06Q0.png" alt=""></p> <h2 id="foveabox-beyond-anchor-based-object-detector"><strong>FoveaBox: Beyond Anchor-based Object Detector</strong></h2> <p><img src="https://i.imgur.com/Y5K4Uan.png" alt="FoveaBox: Beyond Anchor-based Object Detector"></p> <p>FoveaBox is motivated from the fovea of human eyes: the center of the vision field (object) is with the highest visual acuity. FoveaBox jointly predicts the locations where the object’s center area is likely to exist as well as the bounding box at each valid location. Thanks to the feature pyramidal representations, different scales of objects are naturally detected from multiple levels of features.</p> <p><img src="https://i.imgur.com/nTougKV.png" alt="FoveaBox: Beyond Anchor-based Object Detector"></p> <p>The design of the architecture follows RetinaNet to make a fair comparison. FoveaBox uses a Feature Pyramid Network backbone on top of a feedforward ResNet architecture. To this backbone, FoveaBox attaches two subnetworks, one for classifying the corresponding cells and one for predict the (x1 , y1 , x2 , y2 ) of ground-truth object boxes. For each spatial output location, the FoveaBox predicts one score output for each class and the corresponding 4-dimensional box, which is different from previous works attaching A anchors in each position (usually A = 9).</p> <p><img src="https://i.imgur.com/ea6VsgH.png" alt="FoveaBox: Beyond Anchor-based Object Detector"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/taokong/FoveaBox" rel="external nofollow noopener" target="_blank">PyTorch</a></p> <h2 id="fcos-fully-convolutional-one-stage-object-detection---iccv-2019"><strong>FCOS: Fully Convolutional One-Stage Object Detection - ICCV 2019</strong></h2> <p>In order to suppress these low-quality detections, we introduce a novel “center-ness” branch (only one layer) to predict the deviation of a pixel to the center of its corresponding bounding box. This score is then used to down-weight low-quality detected bounding boxes and merge the detection results in NMS. The simple yet effective center-ness branch allows the FCN-based detector to outperform anchor-based counterparts under exactly the same training and testing settings.</p> <p><img src="https://i.imgur.com/aKH5xYC.png" alt="FCOS: Fully Convolutional One-Stage Object Detection - ICCV 2019"></p> <p>The network architecture of FCOS, where C3, C4, and C5 denote the feature maps of the backbone network and P3 to P7 are the feature levels used for the final prediction. H × W is the height and width of feature maps. ‘/s’ (s = 8, 16, …, 128) is the downsampling ratio of the feature maps at the level to the input image. As an example, all the numbers are computed with an 800 × 1024 input.</p> <p><img src="https://i.imgur.com/JqIcW0A.png" alt="FCOS: Fully Convolutional One-Stage Object Detection - ICCV 2019"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/tianzhi0549/FCOS" rel="external nofollow noopener" target="_blank">PyTorch</a></p> <h2 id="reppoints-point-set-representation-for-object-detection---iccv-2019"><strong>RepPoints: Point Set Representation for Object Detection - ICCV 2019</strong></h2> <p>RepPoints is a set of points that learns to adaptively position themselves over an object in a manner that circumscribes the object’s spatial extent and indicates semantically significant local areas. The training of RepPoints is driven jointly by object localization and recognition targets, such that the RepPoints are tightly bound by the ground-truth bounding box and guide the detector toward correct object classification. This adaptive and differentiable representation can be coherently used across the different stages of a modern object detector, and does not require the use of anchors to sample over a space of bounding boxes.</p> <p><img src="https://i.imgur.com/5fhfyee.jpg" alt="RepPoints: Point Set Representation for Object Detection - ICCV 2019"></p> <p>Overview of the proposed RPDet (RepPoints detector). Whilebone, we only draw the afterwards pipeline of one scale of FPN featureshare the same afterwards network architecture and the same model weights. feature pyramidal networks (FPN) are adopted as the backmaps for clear illustration. Note all scales of FPN feature maps share the same afterwards network architecture and the same model weights.</p> <p><img src="https://i.imgur.com/UCX7qCq.png" alt="RepPoints: Point Set Representation for Object Detection - ICCV 2019"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/microsoft/RepPoints" rel="external nofollow noopener" target="_blank">PyTorch</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/DLSS4/">NVIDIA DLSS 4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/AMD-FSR/">AMD FidelityFX Super Resolution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/CG-Pipeline/">The Real-Time Rendering Pipeline</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jiacheng LI. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>