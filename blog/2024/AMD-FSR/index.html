<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AMD FidelityFX Super Resolution | LI, Jiacheng (李 家丞) </title> <meta name="author" content="Jiacheng LI"> <meta name="description" content="An overview of the latest advancements in neural rendering with AMD FSR 3.x."> <meta name="keywords" content="Jiacheng Li, Sony Research, Sony AI, Sony, USTC"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ddlee-cn.github.io/blog/2024/AMD-FSR/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> LI, Jiacheng (李 家丞) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/computational-photography/">Computational Photography</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rendering-&amp;-generative-ai/">Rendering &amp; Genenerative AI</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/streaming-&amp;-display/">Streaming &amp; Display</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/intelligent-sensing/">Intelligent Sensing</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">AMD FidelityFX Super Resolution</h1> <p class="post-meta"> Created on August 21, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/all"> <i class="fa-solid fa-hashtag fa-sm"></i> all</a>   <a href="/blog/tag/rendering-generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> rendering-generative-ai</a>   <a href="/blog/tag/super-resolution"> <i class="fa-solid fa-hashtag fa-sm"></i> super-resolution</a>   <a href="/blog/tag/super-sampling"> <i class="fa-solid fa-hashtag fa-sm"></i> super-sampling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="i-introduction"><strong>I. Introduction</strong></h2> <h3 id="overview">Overview</h3> <p>AMD FidelityFX Super Resolution (FSR) encompasses a suite of open-source image upscaling technologies engineered to enhance application framerates. This is achieved by rendering scenes at a lower internal resolution and subsequently employing intelligent algorithms to upscale the image to the desired, typically native, output resolution. The primary benefit is a significant boost in performance, which can be utilized to enable more demanding graphical settings, achieve higher resolutions, or ensure smoother gameplay, particularly on hardware with limited computational resources.</p> <p>A defining characteristic of FSR is its open-source nature, distributed under the permissive MIT license, and its broad cross-platform compatibility. FSR supports a wide array of graphics processing units (GPUs), including those from competitors, and integrates with multiple graphics APIs such as DirectX 11, DirectX 12, and Vulkan. This open approach fosters wider adoption within the game development community, as a single FSR implementation can benefit a diverse user base across various hardware configurations. The strategic decision to make FSR open and cross-platform, contrasting with some proprietary hardware-locked solutions, allows developers to reach a broader audience without fragmenting their engineering efforts. This approach aims to establish FSR as a widely available upscaling standard, indirectly enhancing the value of AMD’s ecosystem by promoting a technology accessible to all.</p> <h3 id="the-evolution-of-fsr">The Evolution of FSR</h3> <p>The FSR technology suite has undergone significant evolution since its inception, with each iteration addressing limitations of its predecessor and incorporating more advanced rendering techniques.</p> <ul> <li> <strong>FSR 1.0 (Released 2021)<sup id="fnref:FSR1"><a href="#fn:FSR1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</strong> This initial version introduced spatial upscaling. Its core components were an Edge-Adaptive Spatial Upsampling (EASU) algorithm and a Robust Contrast-Adaptive Sharpening (RCAS) pass. FSR 1.0 was designed for ease of integration and targeted broad hardware compatibility, operating on a single input frame without reliance on temporal data.</li> <li> <strong>FSR 2.x (Released 2022)<sup id="fnref:FSR1:1"><a href="#fn:FSR1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</strong> Representing a fundamental shift, FSR 2.x transitioned to temporal upscaling. This version was developed from the ground up and is not an incremental update to FSR 1.0. It leverages data from previous frames, such as motion vectors and depth information, to reconstruct images with significantly higher quality and more effective anti-aliasing compared to FSR 1.0.1 Subsequent minor versions (e.g., FSR 2.1, 2.2) introduced refinements to improve image quality further, such as reducing ghosting artifacts and enhancing the stability of motion vector processing.</li> <li> <strong>FSR 3.x (Released 2023<sup id="fnref:FSR3"><a href="#fn:FSR3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, 2024<sup id="fnref:FSR31"><a href="#fn:FSR31" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>):</strong> This iteration builds upon the temporal upscaling foundation of FSR 2.x and introduces a significant new feature: frame generation. Technologies like Optical Flow and Frame Interpolation are employed to synthesize new frames, aiming to further increase the displayed frames per second (FPS) beyond what upscaling alone can achieve. FSR 3 also introduced a “Native AA” (Anti-Aliasing) mode, which applies the FSR processing pipeline at native resolution for improved anti-aliasing without upscaling.</li> </ul> <p>This evolutionary trajectory highlights a pattern of iterative problem-solving. FSR 1.0, while effective for boosting performance, exhibited limitations in image fidelity, particularly concerning the reconstruction of fine details and maintaining temporal stability, sometimes resulting in blurriness or shimmering. FSR 2.x directly targeted these shortcomings by incorporating temporal data, a well-established method for enhancing reconstruction quality by accumulating more information per output pixel over successive frames. This advancement, however, necessitated more complex input data from the game engine and a more intricate integration process. FSR 3.x addresses a different performance aspect: scenarios where even temporally upscaled rendering may not achieve the very high frame rates desired for modern high-refresh-rate displays. By introducing frame generation, FSR 3.x decouples the displayed FPS from the rendered FPS. This, however, introduces new engineering challenges, primarily related to increased input latency and the potential for artifacts in the synthetically generated frames. Each generation thus reflects a drive to overcome prior limitations by employing more sophisticated graphics techniques, leading to a richer feature set but also potentially increasing integration complexity and imposing stricter demands on the quality of input data.</p> <h2 id="ii-fundamental-computer-graphics-concepts"><strong>II. Fundamental Computer Graphics Concepts</strong></h2> <h3 id="rendering-resolution-vs-display-resolution">Rendering Resolution vs. Display Resolution</h3> <p>In the context of real-time 3D graphics, it is crucial to distinguish between rendering resolution and display resolution:</p> <ul> <li> <strong>Rendering Resolution:</strong> This refers to the internal resolution at which the 3D scene geometry is rasterized, and fragments are shaded by the GPU.8 A lower rendering resolution means fewer pixels are processed for complex shading calculations, lighting, and other effects. This reduction in pixel load directly translates to decreased computational work for the GPU, typically resulting in higher frame rates.</li> <li> <strong>Display Resolution (Output Resolution):</strong> This is the resolution at which the final image is presented to the user, often corresponding to the native pixel grid of the display device (e.g., 1920x1080, 2560x1440, 3840x2160).</li> </ul> <p>Upscaling technologies like FSR serve as a bridge between these two resolutions. They take an input frame rendered at a lower resolution and algorithmically reconstruct it to match a higher target display resolution.1 A common characteristic of this process is that while the 3D rendered scene might be processed at a lower fidelity, user interface (UI) elements can be rendered directly at the display resolution and composited over the upscaled scene, ensuring their sharpness and legibility.</p> <h3 id="aliasing-and-anti-aliasing-aa">Aliasing and Anti-Aliasing (AA)</h3> <p><img src="https://i.imgur.com/WXdCmeC.jpeg" alt=""></p> <p><strong>Aliasing</strong> is a pervasive artifact in computer graphics, manifesting as jagged, “stair-step” patterns on the edges of rendered objects, particularly noticeable on diagonal or curved lines. It arises from the process of representing continuous geometric shapes on a discrete grid of pixels, effectively an undersampling of the original signal.</p> <p><strong>Anti-Aliasing (AA)</strong> techniques are employed to mitigate these aliasing artifacts. The general goal is to create smoother, more visually appealing edges by, for example, sampling the scene at a higher frequency than the final display resolution (e.g., Supersampling Anti-Aliasing - SSAA) or by employing analytical methods to blend pixel colors along detected edges (e.g., Morphological Anti-Aliasing - MLAA).</p> <p><strong>Temporal Anti-Aliasing (TAA)</strong> is a widely adopted AA technique that leverages information from previously rendered frames to improve edge smoothing and reduce temporal artifacts like shimmering or crawling of edges on moving objects. TAA achieves this by strategically jittering the sample positions within pixels across successive frames and accumulating these samples over time. This temporal accumulation effectively increases the sampling density per pixel, leading to better anti-aliasing quality, often with a more favorable performance-to-quality ratio than purely spatial methods like SSAA. FSR 2.x and subsequent versions are designed to replace a game engine’s existing TAA solution, as their temporal upscaling process inherently includes a high-quality anti-aliasing component.</p> <h3 id="motion-vectors-depth-buffers-and-other-temporal-data">Motion Vectors, Depth Buffers, and Other Temporal Data</h3> <ul> <li> <strong>Motion Vectors:</strong> These are 2D vectors, typically stored in a dedicated buffer, that describe the screen-space displacement of pixels or surfaces from their position in the previous frame to their position in the current frame. They are indispensable for temporal upscalers to correctly reproject data from the history buffer onto the current frame’s geometry. Motion vectors are also fundamental for frame generation techniques, as they inform how pixels are likely to move into the synthesized intermediate frame. The accuracy, precision (e.g., 16-bit float format is often recommended over 8-bit), and coverage (ensuring all dynamic scene elements, including particles and animated textures, contribute to the motion vector buffer) are paramount for high-quality temporal processing. Engine-generated motion vectors often require careful handling of camera jitter to ensure static objects truly have zero motion.</li> <li> <strong>Depth Buffer:</strong> This buffer stores per-pixel depth information, usually representing the distance of the visible surface from the camera. The depth buffer is critical for several aspects of temporal upscaling: <ul> <li> <strong>Reprojection:</strong> Assisting in accurately mapping historical samples to their correct 3D locations in the current frame.</li> <li> <strong>Disocclusion Detection:</strong> Identifying regions of the scene that have become newly visible (e.g., when an object moves away, revealing what was behind it) or newly occluded. This allows the upscaler to invalidate or appropriately blend historical data that is no longer relevant, thereby mitigating ghosting artifacts.</li> <li> <strong>Edge and Feature Analysis:</strong> Depth discontinuities can aid in edge detection and help differentiate between distinct geometric surfaces. For optimal precision, especially with distant objects, rendering pipelines often use reverse depth (where the far plane is at z=0 and the near plane at z=1) and floating-point depth formats (e.g., R32_FLOAT).</li> </ul> </li> <li> <strong>Exposure and Color Space:</strong> The color data fed into FSR, and how it’s processed internally, is sensitive to color space and exposure levels. FSR 1.0, for example, is documented to work best in a perceptual color space and should be integrated after tone mapping. FSR 2.x and FSR 3.x, when handling High Dynamic Range (HDR) content, typically require input color to be in linear RGB space. The engine’s exposure value might be required by FSR to correctly normalize or tonemap HDR data internally, or to consistently combine frames that may have been rendered with varying pre-exposure factors. If the engine applies a dynamic pre-exposure factor that changes frame-to-frame, this value must be communicated to FSR to ensure temporal consistency.</li> <li> <strong>Reactivity, Transparency, and Composition Masks:</strong> These are optional but often crucial input textures that provide per-pixel guidance to the temporal upscaler on how to treat specific areas of the image. <ul> <li>A <strong>Reactivity Mask</strong> (often derived from alpha values of transparent objects) informs the upscaler to increase the influence of current frame samples over historical samples for pixels corresponding to highly reactive elements like particles, smoke, or other alpha-blended effects. This helps prevent stale historical data from “smearing” or “ghosting” through such effects.</li> <li>A <strong>Transparency and Composition Mask</strong> can be used to denote areas with other types of special rendering, such as raytraced reflections or animated textures, where standard history accumulation and protection mechanisms might need adjustment. These masks allow developers to fine-tune the upscaler’s behavior, improving quality in challenging rendering scenarios where default heuristics might fall short.</li> </ul> </li> </ul> <h2 id="iii-deep-dive-amd-fidelityfx-super-resolution-31"><strong>III. Deep Dive: AMD FidelityFX Super Resolution 3.1</strong></h2> <h3 id="overview-combining-temporal-upscaling-and-frame-generation">Overview: Combining Temporal Upscaling and Frame Generation</h3> <p>FSR 3.1 is architecturally designed as a two-stage process to maximize framerates and visual quality.</p> <ol> <li> <strong>Temporal Upscaling:</strong> The first stage employs a sophisticated temporal upscaling algorithm, which builds directly upon the principles and advancements established in FSR 2.x. This component takes a lower-resolution, jittered input frame along with auxiliary data (depth, motion vectors, etc.) and reconstructs a high-resolution, anti-aliased output frame by leveraging historical information from previous frames.</li> <li> <strong>Frame Generation:</strong> The second stage, a key differentiator of FSR 3.x, utilizes the upscaled frames produced by the first stage to generate and insert additional frames. This process aims to significantly increase the final displayed frame rate, enhancing perceived motion smoothness.</li> </ol> <p>It is important to note that the frame generation component can operate in conjunction with the upscaling component or, through the “Native AA” mode, it can be used with frames rendered at native resolution if only frame rate multiplication is desired without upscaling.</p> <h3 id="core-upscaling-algorithm">Core Upscaling Algorithm</h3> <p>The upscaling algorithm within FSR 3.1 inherits its core mechanics from FSR 2.0, which was a ground-up redesign focused on high-quality temporal reconstruction.</p> <ul> <li> <strong>Temporal Data Utilization and History Reconstruction:</strong> The algorithm maintains a history buffer that stores color information and other relevant data from previously processed frames. This historical data is reprojected into the coordinate space of the current frame using game engine-provided motion vectors. The process relies on jittered input samples (sub-pixel offsets applied to the projection matrix each frame) to allow the accumulation of sub-pixel detail over time, effectively increasing the information density available for reconstructing each output pixel at the higher resolution. Lanczos resampling is identified as a component in this process, featuring mechanisms to mitigate ringing artifacts and dynamically adjust resampling sharpness based on local temporal luminance stability to enhance detail in stable regions.</li> <li> <strong>Handling of Disocclusions and Thin Features:</strong> Robustly managing dynamic scenes requires specialized handling for areas where historical data becomes invalid or insufficient: <ul> <li> <strong>Disocclusion Detection:</strong> The system identifies regions of the scene that were occluded in previous frames but are now visible in the current frame (or vice-versa). This is primarily achieved by comparing depth values from the current frame with reprojected depth values from the reconstructed previous frame.</li> <li> <strong>History Rectification (Depth and Color Based):</strong> When a disocclusion is detected for a pixel, its reliance on accumulated historical color is significantly reduced or clamped to a range derived from neighboring pixels in the current frame. This prevents “ghosting” artifacts, where outdated information from occluded surfaces might persist. For smoother transitions in disoccluded areas, FSR 2.0’s design includes pushing a new, potentially more blurred, sample during the upsampling stage for these regions.</li> <li> <strong>Thin Feature Locking:</strong> To preserve the stability and integrity of very fine geometric details (e.g., wires, railings, distant foliage) that might only be sparsely sampled in any single low-resolution frame, FSR 2.0 introduced a “locking” mechanism. This logic detects pixel-wide ridges and emphasizes their historical color during rectification, preventing them from flickering or breaking up over time. These locks are designed to persist for the length of the jitter sequence or until invalidated by significant scene changes like disocclusion or substantial shading modifications.</li> </ul> </li> </ul> <p><img src="https://i.imgur.com/75Sgm1s.jpeg" alt=""></p> <p>The upscaling techniques employed in FSR 2.x and, by extension, FSR 3.1, are distinct from machine learning-based approaches. They rely on meticulously engineered, hand-coded algorithms. The detailed descriptions of mechanisms like biased Lanczos resampling, depth-based disocclusion handling with adaptive sample pushing, color-based history clamping, and the intricate logic for thin feature locking and unlocking underscore a complex array of heuristics and carefully designed solutions. This suggests a substantial investment in empirical tuning and expert graphics engineering to create algorithms capable of addressing a wide spectrum of challenging rendering scenarios, including fast motion, transparencies, and the preservation of fine details. This can offer advantages in terms of predictable performance characteristics and potentially more straightforward debugging, but may also necessitate more explicit guidance from developers (e.g., through reactivity and composition masks) to optimally handle specific edge cases or unique rendering styles.</p> <h3 id="frame-generation">Frame Generation</h3> <p>The frame generation component of FSR 3.1 is designed to further elevate the perceived frame rate by synthesizing new frames based on the temporally upscaled output. This involves two key technologies: Optical Flow and Frame Interpolation.</p> <p><img src="https://i.imgur.com/edeKtqA.jpeg" alt=""></p> <ul> <li> <strong>Optical Flow (enhanced from AMD Fluid Motion Frames - AFMF):</strong> The FSR 3 Optical Flow workload, derived and improved from AMD’s Fluid Motion Frames (AFMF) technology, plays a critical role in frame generation. Optical flow algorithms perform a dense analysis of consecutive upscaled frames to estimate per-pixel motion between them. This process generates a set of optical flow vectors, which are typically more detailed and can capture more nuanced screen-space motion (e.g., from shadows, reflections, volumetric effects, and transparencies) than the often object-based or sparser motion vectors provided by the game engine for the upscaling pass.6 The FidelityFX SDK includes a dedicated FfxOpticalFlow component for this task, which takes the current and previous (upscaled) frames as primary inputs and outputs these detailed motion vectors.</li> <li> <strong>Frame Interpolation Techniques:</strong> Using the dense motion vectors generated by the Optical Flow pass, along with the two surrounding “real” (rendered and upscaled) frames, the frame interpolation stage synthesizes one or more entirely new frames to be inserted between them. The objective is to create visually plausible intermediate frames that significantly enhance the illusion of smooth motion, effectively doubling (or more, depending on the implementation) the displayed frame rate. The quality of these interpolated frames is heavily dependent on the accuracy of the optical flow field and the complexity of the on-screen motion. Fast, erratic, or unpredictable motion, as well as significant disocclusions or complex interacting transparent surfaces, can pose challenges for optical flow estimation and may lead to visual artifacts in the generated frames (such as warping, morphing, or object discontinuities). To mitigate the visibility of such artifacts, AMD recommends that the game should be running at a minimum of 60 FPS <em>before</em> frame generation is applied (i.e., after the upscaling pass), as interpolation artifacts tend to be more noticeable and disruptive at lower base framerates.</li> </ul> <p>The quality and robustness of the Optical Flow algorithm are paramount for the success of FSR 3.1’s frame generation. Frame interpolation is fundamentally an “inventive” process, relying entirely on the information from surrounding real frames and the guidance of motion vectors to construct what happened “in between.” If the optical flow accurately captures the true motion of all on-screen elements, the resulting interpolated frame is likely to be of high quality and free from significant visual disturbances. Conversely, errors or inaccuracies in the optical flow—such as incorrect motion vectors or misinterpretations of complex occluding edges—will directly translate into visual artifacts in the generated frames. This underscores the importance of the FfxOpticalFlow component 20 as a specialized, and potentially computationally intensive, cornerstone of the FSR 3.1 frame generation pipeline.</p> <h3 id="graphics-pipeline-integration">Graphics Pipeline Integration</h3> <p>Proper integration of FSR 3.1 into the graphics pipeline is crucial for achieving optimal image quality and performance. This involves careful placement of both the upscaling and frame generation stages, as well as managing dependencies with other rendering passes.</p> <p><img src="https://i.imgur.com/w0Hq9GW.jpeg" alt=""></p> <ul> <li> <strong>Upscaler Placement:</strong> <ul> <li>The FSR 3.1 upscaler, much like FSR 2.x, is designed to replace the game engine’s conventional Temporal Anti-Aliasing (TAA) pass. It should be integrated into the pipeline <em>after</em> all main scene opaque geometry has been rendered and essential G-buffer data (such as depth and motion vectors for the current frame) have been generated, but <em>before</em> any TAA would typically run</li> <li>Post-processing effects that rely on or benefit from an anti-aliased input (e.g., certain types of bloom, lens flares, depth of field that assumes clean edges) should be applied <em>after</em> the FSR upscaling pass, operating on the high-resolution upscaled output.</li> <li>Conversely, effects that require the raw, lower-resolution depth buffer or other non-anti-aliased G-buffer information (e.g., Screen Space Ambient Occlusion (SSAO), screen-space reflections if they are not re-rendered or adapted for the output resolution) should generally be executed <em>before</em> FSR upscaling.</li> </ul> </li> <li> <strong>Frame Generation Placement &amp; Swapchain Management:</strong> <ul> <li>The frame generation process (Optical Flow and Frame Interpolation) occurs <em>after</em> the FSR upscaling pass has produced a high-resolution frame. It also typically happens <em>after</em> the User Interface (UI) has been rendered, or the UI is composited separately onto the final displayed frames to avoid interpolation artifacts on UI elements.</li> <li>A significant integration requirement for FSR 3.1 with frame generation is the management of the swap chain. Developers must either replace their existing swap chain or create a new one using the AMD FidelityFX API’s DX12 or Vulkan Frame Generation Swapchain interfaces. This custom swapchain is essential for correctly pacing and presenting the interleaved sequence of rendered (upscaled) frames and synthetically generated frames.</li> <li>The Optical Flow and Frame Generation workloads are computationally intensive but can often be scheduled to run on an asynchronous compute queue if the hardware supports it. This helps to hide their latency and reduce their impact on the main graphics rendering pipeline.</li> </ul> </li> </ul> <p><strong>Official Presentations</strong></p> <p><a href="https://gpuopen.com/gdc-presentations/2024/GDC2024_High_Performance_Rendering_in_Snowdrop_Using_AMD_FidelityFX_Super_Resolution_3.pdf" rel="external nofollow noopener" target="_blank">HIGH PERFORMANCE RENDERING IN SNOWDROP USING AMD FSR 3 - GDC 2024</a></p> <p><a href="https://gpuopen.com/gdc-presentations/2023/GDC-2023-Temporal-Upscaling.pdf" rel="external nofollow noopener" target="_blank">TEMPORAL UPSCALING: PAST, PRESENT, AND FUTURE - GDC 2023</a></p> <p><a href="https://gpuopen.com/gdc-presentations/2022/GDC_FidelityFX_Super_Resolution_2_0.pdf" rel="external nofollow noopener" target="_blank">FIDELITYFX SUPER RESOLUTION 2.0</a></p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:FSR1"> <p><a href="https://gpuopen.com/fidelityfx-superresolution/" rel="external nofollow noopener" target="_blank">AMD FidelityFX™ Super Resolution 1 (FSR 1) - AMD GPUOpen</a> <a href="#fnref:FSR1" class="reversefootnote" role="doc-backlink">↩</a> <a href="#fnref:FSR1:1" class="reversefootnote" role="doc-backlink">↩<sup>2</sup></a></p> </li> <li id="fn:FSR3"> <p><a href="https://gpuopen.com/fidelityfx-super-resolution-3/" rel="external nofollow noopener" target="_blank">AMD FidelityFX™ Super Resolution 3 (FSR 1) - AMD GPUOpen</a> <a href="#fnref:FSR3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:FSR31"> <p><a href="https://community.amd.com/t5/gaming/amd-fsr-3-1-now-available-fsr-3-available-and-upcoming-in-60/ba-p/692000" rel="external nofollow noopener" target="_blank">AMD FSR 3.1 Now Available, FSR 3 Available and Upcoming in 60 Games</a> <a href="#fnref:FSR31" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/DLSS4/">NVIDIA DLSS 4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/CG-Pipeline/">The Real-Time Rendering Pipeline</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/Google-Camera/">The Evolution of Google Camera</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jiacheng LI. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>