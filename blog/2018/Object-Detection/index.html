<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deep Learning for Object Detection: Fundamentals | LI, Jiacheng (李 家丞) </title> <meta name="author" content="Jiacheng LI"> <meta name="description" content="A comprehensive guide to deep learning for object detection, covering the fundamentals of two-stage and one-stage methods."> <meta name="keywords" content="Jiacheng Li, Sony Research, Sony AI, Sony, USTC"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ddlee-cn.github.io/blog/2018/Object-Detection/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> LI, Jiacheng (李 家丞) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/computational-photography/">Computational Photography</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rendering-&amp;-generative-ai/">Rendering &amp; Genenerative AI</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/streaming-&amp;-display/">Streaming &amp; Display</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/intelligent-sensing/">Intelligent Sensing</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep Learning for Object Detection: Fundamentals</h1> <p class="post-meta"> Created on March 02, 2018 </p> <p class="post-tags"> <a href="/blog/2018"> <i class="fa-solid fa-calendar fa-sm"></i> 2018 </a>   ·   <a href="/blog/tag/all"> <i class="fa-solid fa-hashtag fa-sm"></i> all</a>   <a href="/blog/tag/intelligent-sensing"> <i class="fa-solid fa-hashtag fa-sm"></i> intelligent-sensing</a>   <a href="/blog/tag/object-detection"> <i class="fa-solid fa-hashtag fa-sm"></i> object-detection</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#fast-rcnn---grishick---iccv-2015">Fast RCNN - Grishick - ICCV 2015</a></li> <li class="toc-entry toc-h2"><a href="#faster-r-cnn-towards-real-time-object-detection-with-region-proposal---ren---nips-2015">Faster R-CNN: Towards Real Time Object Detection with Region Proposal - Ren - NIPS 2015</a></li> <li class="toc-entry toc-h2"><a href="#r-fcn-object-detection-via-region-based-fully-convolutional-networks---dai---nips-2016">R-FCN: Object Detection via Region-based Fully Convolutional Networks - Dai - NIPS 2016</a></li> <li class="toc-entry toc-h2"><a href="#fpnfeature-pyramid-networks-for-object-detection---lin---cvpr-2017">(FPN)Feature Pyramid Networks for Object Detection - Lin - CVPR 2017</a></li> <li class="toc-entry toc-h2"><a href="#yoloyou-only-look-once-unified-real-time-object-detection---redmon-et-al---cvpr-2016">(YOLO)You Only Look Once: Unified, Real Time Object Detection - Redmon et al. - CVPR 2016</a></li> <li class="toc-entry toc-h2"><a href="#yolo9000-better-faster-stronger---redmon-et-al---2016">YOLO9000: Better, Faster, Stronger - Redmon et al. - 2016</a></li> <li class="toc-entry toc-h2"><a href="#retinanetfocal-loss-for-dense-object-detection---lin----iccv-2017">(RetinaNet)Focal loss for dense object detection - Lin - ICCV 2017</a></li> <li class="toc-entry toc-h2"><a href="#an-analysis-of-scale-invariance-in-object-detection---singh---cvpr-2018">An analysis of scale invariance in object detection - Singh - CVPR 2018</a></li> <li class="toc-entry toc-h2"><a href="#sniper-efficient-multi-scale-training---singh---nips-2018">SNIPER: efficient multi-scale training - Singh - NIPS 2018</a></li> <li class="toc-entry toc-h2"><a href="#ohemtraining-region-based-object-detectors-with-online-hard-example-mining---shrivastava-et-al---cvpr-2016">(OHEM)Training Region-based Object Detectors with Online Hard Example Mining - Shrivastava et al. - CVPR 2016</a></li> <li class="toc-entry toc-h2"><a href="#dsod-learning-deeply-supervised-object-detectors-from-scratch---shen---iccv-2017">DSOD: learning deeply supervised object detectors from scratch - Shen - ICCV 2017</a></li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="fast-rcnn---grishick---iccv-2015"><strong>Fast RCNN - Grishick - ICCV 2015</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>Fast RCNN</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: Ross Girshick</li> <li>Arxiv: <a href="https://arxiv.org/abs/1504.08083" rel="external nofollow noopener" target="_blank">1504.08083</a> </li> <li>Date: April 2015</li> <li>Published: ICCV 2015</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>An improvement to [R-CNN] (https://blog.ddlee.cn/posts/415f4992/), ROI Pooling Design</li> <li>Article structure is clear</li> </ul> <p><strong>R-CNN’s Drawbacks</strong></p> <ul> <li>Training is a multi-stage process (Proposal, Classification, Regression)</li> <li>Training takes time and effort</li> <li>Infer time-consuming</li> </ul> <p>The reason of time-consuming is that CNN is performed separately on each Proposal, with no shared calculations.</p> <p><strong>Architecture</strong> <img src="https://i.imgur.com/Hy6fRGr.png" alt="Fast RCNN"></p> <p>The picture above shows the architecture of Fast R-CNN. The image is generated by the feature extractor, and the Selective Search algorithm is used to map the RoI (Region of Interest) to the feature map. Then, the RoI Pooling operation is performed for each RoI to obtain the feature vector of the same length. Classification and BBox Regression.</p> <p>This structure of Fast R-CNN is the prototype of the meta-structure used in the main 2-stage method of the detection task. The entire system consists of several components: Proposal, Feature Extractor, Object Recognition &amp; Localization. The Proposal part is replaced by RPN (Faster R-CNN), the Feature Extractor part uses SOTA’s classified CNN network (ResNet, etc.), and the last part is often a parallel multitasking structure (Mask R-CNN, etc.).</p> <p><strong>Performance &amp; Ablation Study</strong> <img src="https://i.imgur.com/LcgsD10.png" alt="Fast RCNN"></p> <p><strong>Code</strong> <a href="https://github.com/rbgirshick/fast-rcnn" rel="external nofollow noopener" target="_blank">Caffe(Official)</a></p> <h2 id="faster-r-cnn-towards-real-time-object-detection-with-region-proposal---ren---nips-2015"><strong>Faster R-CNN: Towards Real Time Object Detection with Region Proposal - Ren - NIPS 2015</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>Faster R-CNN: Towards Real Time Object Detection with Region Proposal</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun</li> <li>Date: June 2015</li> <li>Arxiv: <a href="https://arxiv.org/abs/1506.01497" rel="external nofollow noopener" target="_blank">1506.01497</a> </li> <li>Published: NIPS 2015</li> </ul> <p><strong>Highlights</strong> Faster R-CNN is the mainstream method of 2-stage method. The proposed RPN network replaces the Selective Search algorithm so that the detection task can be completed end-to-end by the neural network. Roughly speaking, Faster R-CNN = RPN + Fast R-CNN, the nature of the convolution calculation shared with RCNN makes the calculations introduced by RPN very small, allowing Faster R-CNN to run at 5fps on a single GPU. Reach SOTA in terms of accuracy.</p> <p><strong>Regional Proposal Networks</strong></p> <p><img src="https://i.imgur.com/Fjlw3aF.png" alt="Faster R-CNN: Towards Real Time Object Detection with Region Proposal"></p> <p>The RPN network models the Proposal task as a two-category problem.</p> <p>The first step is to generate an anchor box of different size and aspect ratio on a sliding window, determine the threshold of the IOU, and calibrate the positive and negative of the anchor box according to Ground Truth. Thus, the sample that is passed into the RPN network is the anchor box and whether there is an object in each anchor box. The RPN network maps each sample to a probability value and four coordinate values. The probability value reflects the probability that the anchor box has an object, and the four coordinate values ​​are used to regress the position of the defined object. Finally, the two classifications and the coordinates of the Loss are unified to be the target training of the RPN network.</p> <p>The RPN network has a large number of super-parameters, the size and length-to-width ratio of the anchor box, the threshold of IoU, and the ratio of Proposal positive and negative samples on each image.</p> <p><strong>Performance</strong> <img src="https://i.imgur.com/6bjIITD.png" alt="Faster R-CNN: Towards Real Time Object Detection with Region Proposal"></p> <h2 id="r-fcn-object-detection-via-region-based-fully-convolutional-networks---dai---nips-2016"><strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks - Dai - NIPS 2016</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: Jifeng Dai, Yi Li, Kaiming He, and Jian Sun</li> <li>Arxiv: <a href="https://arxiv.org/abs/1605.06409" rel="external nofollow noopener" target="_blank">1605.06409</a> </li> <li>Published: NIPS 2016</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>Full convolutional network, sharing weights across ROIs</li> </ul> <p><strong>Design</strong></p> <p><img src="https://i.imgur.com/Nq92vNC.png" alt="R-FCN: Object Detection via Region-based Fully Convolutional Networks"></p> <p>The article points out that there is an unnatural design of the framework before the detection task, that is, the feature extraction part of the full convolution + the fully connected classifier, and the best performing image classifier is a full convolution structure (ResNet, etc.). One point is caused by the contradiction between the translation invariance of the classification task and the translation sensitivity of the detection task. In other words, the detection model uses the feature extractor of the classification model, and the position information is lost. This article proposes to solve this problem by using a “location-sensitive score map” approach.</p> <p><strong>Performance &amp; Ablation Study</strong></p> <p>The comparison with Faster R-CNN shows that R-FCN achieves better accuracy while maintaining shorter inference time. <img src="https://i.imgur.com/6DYmf4m.png" alt="R-FCN: Object Detection via Region-based Fully Convolutional Networks"></p> <p><strong>Code</strong> <a href="https://github.com/daijifeng001/R-FCN" rel="external nofollow noopener" target="_blank">MXNet</a></p> <h2 id="fpnfeature-pyramid-networks-for-object-detection---lin---cvpr-2017"><strong>(FPN)Feature Pyramid Networks for Object Detection - Lin - CVPR 2017</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>Feature Pyramid Networks for Object Detection</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie</li> <li>Date: March 2016</li> <li>Arxiv: <a href="https://arxiv.org/abs/1612.03144" rel="external nofollow noopener" target="_blank">1612.03144</a> </li> <li>Published: CVPR 2017</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>Image pyramid to feature pyramid</li> </ul> <p><strong>Feature Pyramid Networks</strong></p> <p><img src="https://i.imgur.com/S7aVS9v.png" alt="(FPN)Feature Pyramid Networks for Object Detection"></p> <p>Starting from the picture, the cascading feature extraction is performed as usual, and a return path is added: starting from the highest feature map, the nearest neighbor is sampled down to get the return feature map of the same size as the low-level feature map. A lateral connection at the element position is then made to form features in this depth.</p> <p>The belief in this operation is that the low-level feature map contains more location information, and the high-level feature map contains better classification information, combining the two to try to achieve the location classification dual requirements of the detection task.</p> <p><strong>Performance &amp; Ablation Study</strong></p> <p>The main experimental results of the article are as follows:</p> <p><img src="https://i.imgur.com/krnPtFI.png" alt="(FPN)Feature Pyramid Networks for Object Detection"></p> <p>Comparing the different head parts, the input feature changes do improve the detection accuracy, and the lateral and top-down operations are also indispensable.</p> <p><strong>Code</strong> <a href="https://github.com/facebookresearch/Detectron" rel="external nofollow noopener" target="_blank">Caffe2(FAIR’s Detectron)</a></p> <h2 id="yoloyou-only-look-once-unified-real-time-object-detection---redmon-et-al---cvpr-2016"><strong>(YOLO)You Only Look Once: Unified, Real Time Object Detection - Redmon et al. - CVPR 2016</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>You Only Look Once: Unified, Real Time Object Detection</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: J. Redmon, S. Divvala, R. Girshick, and A. Farhadi</li> <li>Arxiv: https://arxiv.org/abs/1506.02640</li> <li>Date: June. 2015</li> <li>Published: CVPR 2016</li> </ul> <p><strong>Highlights &amp; Drawbacks</strong></p> <ul> <li>Fast.</li> <li>Global processing makes background errors relatively small compared to local (regional) based methods such as Fast RCNN.</li> <li>Generalization performance is good, YOLO performs well when testing on art works.</li> <li>The idea of YOLO meshing is still relatively rough, and the number of boxes generated by each mesh also limits its detection of small objects and similar objects.</li> </ul> <p><strong>Design</strong></p> <p><img src="https://i.imgur.com/ZO5EiVs.png" alt="You Only Look Once: Unified, Real Time Object Detection"></p> <p><img src="https://i.imgur.com/dBqrPc5.png" alt="You Only Look Once: Unified, Real Time Object Detection"></p> <p>The loss function is divided into three parts: coordinate error, object error, and class error. In order to balance the effects of category imbalance and large and small objects, weights are added to the loss and the root length is taken.</p> <p><strong>Performance &amp; Ablation Study</strong></p> <p><img src="https://i.imgur.com/RJQH4lU.png" alt="You Only Look Once: Unified, Real Time Object Detection"></p> <p><img src="https://i.imgur.com/Bx7fLGT.png" alt="You Only Look Once: Unified, Real Time Object Detection"></p> <p>Compared to Fast-RCNN, YOLO’s background false detections account for a small proportion of errors, while position errors account for a large proportion (no log coding).</p> <p><strong>Code</strong></p> <ul> <li><a href="https://pjreddie.com/darknet/yolo/" rel="external nofollow noopener" target="_blank">Project Site(Contains newest v3)</a></li> <li><a href="https://github.com/pjreddie/darknet" rel="external nofollow noopener" target="_blank">Darknet</a></li> </ul> <h2 id="yolo9000-better-faster-stronger---redmon-et-al---2016"><strong>YOLO9000: Better, Faster, Stronger - Redmon et al. - 2016</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>YOLO9000: Better, Faster, Stronger</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: J. Redmon and A. Farhadi</li> <li>Arxiv: <a href="https://arxiv.org/abs/1612.08242" rel="external nofollow noopener" target="_blank">1612.08242</a> </li> <li>Date: Dec. 2016</li> </ul> <p><strong>Highlights &amp; Drawbacks</strong></p> <ul> <li>A significant improvement for YOLO.</li> <li>Extends to more categories.</li> </ul> <p><strong>Design</strong></p> <ol> <li>Add BN to the convolutional layer and discard Dropout</li> <li>Higher size input</li> <li>Use Anchor Boxes and replace the fully connected layer with convolution in the head</li> <li>Use the clustering method to get a better a priori for generating Anchor Boxes</li> <li>Refer to the Fast R-CNN method for log/exp transformation of position coordinates to keep the loss of coordinate regression at the appropriate order of magnitude.</li> <li>Passthrough layer: Similar to ResNet’s skip-connection, stitching feature maps of different sizes together</li> <li>Multi-scale training</li> <li>More efficient network Darknet-19, a VGG-like network, achieves the same accuracy as the current best on ImageNet with fewer parameters.</li> </ol> <p>After this improvement, YOLOv2 absorbs the advantages of a lot of work, achieving the accuracy and faster inference speed of SSD.</p> <p><strong>Performance &amp; Ablation Study</strong> <img src="https://i.imgur.com/1G7OXeq.jpg" alt="YOLO9000: Better, Faster, Stronger"></p> <p><strong>Code</strong></p> <ul> <li><a href="https://pjreddie.com/darknet/yolo/" rel="external nofollow noopener" target="_blank">Project Site(Contains newest v3)</a></li> <li><a href="https://github.com/pjreddie/darknet" rel="external nofollow noopener" target="_blank">Darknet</a></li> </ul> <h2 id="retinanetfocal-loss-for-dense-object-detection---lin----iccv-2017"><strong>(RetinaNet)Focal loss for dense object detection - Lin - ICCV 2017</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>Focal loss for dense object detection</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dollár</li> <li>Date: Aug. 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1708.02002" rel="external nofollow noopener" target="_blank">1708.02002</a> </li> <li>Published: ICCV 2017(Best Student Paper)</li> </ul> <p><strong>Highlights &amp; Drawbacks</strong></p> <ul> <li>Loss function improvement</li> <li>For <em>Dense</em> samples from single-stage models like SSD</li> </ul> <p><strong>Design</strong></p> <p>In single-stage models, a massive number of training samples are calculated in the loss function at the same time, because of the lack of proposing candidate regions. Based on findings that the loss of a single-stage model is dominated by easy samples(usually backgrounds), Focal Loss introduces a suppression factor on losses from these easy samples, in order to let hard cases play a bigger role in the training process.</p> <p><img src="https://i.imgur.com/C6uuJrQ.png" alt="(RetinaNet)Focal loss for dense object detection"></p> <p>Utilizing focal loss term, a dense detector called RetinaNet is designed based on ResNet and FPN:</p> <p><img src="https://i.imgur.com/62SFpNT.png" alt="(RetinaNet)Focal loss for dense object detection"></p> <p><strong>Performance &amp; Ablation Study</strong></p> <p>The author’s experiments show that a single-stage detector can achieve comparable accuracy like two-stage models thanks to the proposed loss function. However, Focal Loss function brings in two additional hyper-parameters. The authors use a grid search to optimize these two hyper-parameters, which is not inspiring at all since it provides little experience when using the proposed loss function on other datasets or scenarios. Focal Loss optimizes the weight between easy and hard training samples in the loss function from the perspective of sample imbalance.</p> <p><img src="https://i.imgur.com/IObdCS1.png" alt="(RetinaNet)Focal loss for dense object detection"></p> <p><strong>Code</strong> <a href="https://github.com/facebookresearch/Detectron" rel="external nofollow noopener" target="_blank">Caffe2(FAIR’s Detectron)</a></p> <h2 id="an-analysis-of-scale-invariance-in-object-detection---singh---cvpr-2018"><strong>An analysis of scale invariance in object detection - Singh - CVPR 2018</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>An analysis of scale invariance in object detection - SNIP</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: B. Singh and L. S. Davis</li> <li>Date: Nov. 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1711.08189" rel="external nofollow noopener" target="_blank">1711.08189</a> </li> <li>Published: CVPR 2018</li> </ul> <p><strong>Highlights &amp; Drawbacks</strong></p> <ul> <li>Training strategy optimization, ready to integrate with other tricks</li> <li>Informing experiments for multi-scale training trick</li> </ul> <p><strong>Design</strong></p> <p>The process of SNIP:</p> <ol> <li> <p>Select 3 image resolutions: (480, 800) to train [120, ∞) proposals, (800, 1200) to train [40, 160] proposals, (1400, 2000) to train [0, 80] for proposals</p> </li> <li> <p>For each resolution image, BP only returns the gradient of the proposal within the corresponding scale.</p> </li> <li> <p>This ensures that only one network is used, but the size of each training object is the same, and the size of the object of ImageNet is consistent to solve the problem of domain shift, and it is consistent with the experience of the backbone, and the training and test dimensions are consistent, satisfying “ ImageNet pre-trained size, an object size, a network, a receptive field, these four match each other, and the train and test dimensions are the same.</p> </li> <li> <p>A network, but using all the object training, compared to the scale specific detector, SNIP is fully conducive to the data</p> </li> <li> <p>During the test, the same detector is measured once on each of the three resolution images, and only the detected boxes of the corresponding scale are retained for each resolution image, and then merged to execute SoftNMS.</p> </li> </ol> <p><strong>Performance &amp; Ablation Study</strong></p> <p>The authors conducted experiments for RFCN and Faster-RCNN and SNIP improves performance for both meta architectures.</p> <p><img src="https://i.imgur.com/6gKN6kw.png" alt="An analysis of scale invariance in object detection"></p> <p>Check full introduction at <a href="https://cvnote.ddlee.cc/An-analysis-of-scale-invariance-in-object-detection-SNIP-Singh-CVPR-2018.html" rel="external nofollow noopener" target="_blank">An analysis of scale invariance in object detection - SNIP - Singh - CVPR 2018</a>.</p> <h2 id="sniper-efficient-multi-scale-training---singh---nips-2018"><strong>SNIPER: efficient multi-scale training - Singh - NIPS 2018</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>SNIPER: efficient multi-scale training</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: B. Singh, M. Najibi, and L. S. Davis</li> <li>Date: May 2018</li> <li>Arxiv: <a href="https://arxiv.org/abs/1805.09300" rel="external nofollow noopener" target="_blank">1805.09300</a> </li> <li>Published: NIPS 2018</li> </ul> <p><strong>Highlights &amp; Drawbacks</strong></p> <ul> <li>Efficient version of SNIP training strategy for object detection</li> <li>Select ROIs with proper size only inside a batch</li> </ul> <p><strong>Design</strong> <img src="https://i.imgur.com/fcqlVp8.gif" alt="SNIPER: efficient multi-scale training"></p> <p>Following SNIP, the authors put crops of an image which contain objects to be detected(called chips) into training instead of the entire image. This design also makes large-batch training possible, which accelerates the training process. This training method utilizes the context of the object, which can save unnecessary calculations for simple background(such as the sky) so that the utilization rate of training data is improved.</p> <p><img src="https://i.imgur.com/WUTZeG1.png" alt="SNIPER: efficient multi-scale training"></p> <p>The core design of SNIPER is the selection strategy for ROIs from a chip(a crop of entire image). The authors use several hyper-params to filter boxes with proper size in a batch, hopping that the detector network only learns features beyond object size.</p> <p>Due to its memory efficient design, SNIPER can benefit from Batch Normalization during training and it makes larger batch-sizes possible for instance-level recognition tasks on a single GPU. Hence, there is no need to synchronize batch-normalization statistics across GPUs.</p> <p>Performance &amp; Ablation Study An improvement of the accuracy of small-size objects was reported according to the author’s experiments.</p> <p><img src="https://i.imgur.com/vQ3Qan1.png" alt="SNIPER: efficient multi-scale training"></p> <p><strong>Code</strong> <a href="https://github.com/mahyarnajibi/SNIPER" rel="external nofollow noopener" target="_blank">MXNet</a></p> <h2 id="ohemtraining-region-based-object-detectors-with-online-hard-example-mining---shrivastava-et-al---cvpr-2016"><strong>(OHEM)Training Region-based Object Detectors with Online Hard Example Mining - Shrivastava et al. - CVPR 2016</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>Training Region-based Object Detectors with Online Hard Example Mining</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: A. Shrivastava, A. Gupta, and R. Girshick</li> <li>Date: Apr. 2016</li> <li>Arxiv: <a href="https://arxiv.org/abs/1604.03540" rel="external nofollow noopener" target="_blank">1604.03540</a> </li> <li>Published: CVPR 2016</li> </ul> <p><strong>Highlights &amp; Drawbacks</strong></p> <ul> <li>Learning-based design for balancing examples for ROI in 2-stage detection network</li> <li>Plug-in ready trick, easy to be integrated</li> <li>Additional Parameters for Training</li> </ul> <p><strong>Motivation &amp; Design</strong></p> <p>There is a 1:3 strategy in Faster-RCNN network, which samples negative ROIs(backgrounds) to balance the ratio for positive and negative data in a batch. It’s empirical and hand-designed(need additional effort when setting hyper-params).</p> <p><img src="https://i.imgur.com/6aFz3zx.png" alt=" (OHEM)Training Region-based Object Detectors with Online Hard Example Mining"></p> <p>The authors designed an additional sub-network to “learn” the sampling process for negative ROIs, forcing the network focus on ones which are similar to objects(the hard ones), such as backgrounds contain part of objects.</p> <p>The ‘hard’ examples are defined using probability from detection head, which means that the sample network is exactly the classification network. In practice, the selecting range is set to [0.1, 0.5].</p> <p><strong>Performance &amp; Ablation Study</strong></p> <p><img src="https://i.imgur.com/UyenHVl.png" alt=" (OHEM)Training Region-based Object Detectors with Online Hard Example Mining"></p> <p><img src="https://i.imgur.com/ETa4rjl.png" alt=" (OHEM)Training Region-based Object Detectors with Online Hard Example Mining"></p> <p>OHEM can improve performance even after adding bells and whistles like Multi-scale training and Iterative bbox regression.</p> <p><strong>Code</strong> <a href="https://github.com/abhi2610/ohem" rel="external nofollow noopener" target="_blank">caffe</a></p> <h2 id="dsod-learning-deeply-supervised-object-detectors-from-scratch---shen---iccv-2017"><strong>DSOD: learning deeply supervised object detectors from scratch - Shen - ICCV 2017</strong></h2> <p><strong>Info</strong></p> <ul> <li>Title: <strong>DSOD: learning deeply supervised object detectors from scratch</strong> </li> <li>Task: <strong>Object Detection</strong> </li> <li>Author: Z. Shen, Z. Liu, J. Li, Y. Jiang, Y. Chen, and X. Xue</li> <li>Date: Aug. 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1708.01241" rel="external nofollow noopener" target="_blank">1708.01241</a> </li> <li>Published: ICCV 2017</li> </ul> <p><strong>Highlights &amp; Drawbacks</strong></p> <ul> <li>Object Detection without pre-training</li> <li>DenseNet-like network</li> </ul> <p><strong>Design</strong></p> <p>A common practice that used in earlier works such as R-CNN is to pre-train a backbone network on a categorical dataset like ImageNet, and then use these pre-trained weights as initialization of detection model. Although I have once successfully trained a small detection network from random initialization on a large dataset, there are few models trained from scratch when the number of instances in a dataset is limited like Pascal VOC and COCO. Actually, using better pre-trained weights is one of the tricks in detection challenges. DSOD attempts to train the detection network from scratch with the help of “Deep Supervision” from DenseNet.</p> <p>The 4 principles authors argued for object detection networks:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Proposal-free
2. Deep supervision
3. Stem Block
4. Dense Prediction Structure
</code></pre></div></div> <p><img src="https://i.imgur.com/amvcbcK.png" alt="DSOD: learning deeply supervised object detectors from scratch"></p> <p><strong>Performance &amp; Ablation Study</strong></p> <p>DSOD outperforms detectors with pre-trained weights. <img src="https://i.imgur.com/1dt4lad.png" alt="DSOD: learning deeply supervised object detectors from scratch"></p> <p>Ablation Study on parts: <img src="https://i.imgur.com/vKRUrAf.png" alt="DSOD: learning deeply supervised object detectors from scratch"></p> <p><strong>Code</strong></p> <p><a href="https://github.com/szq0214/DSOD" rel="external nofollow noopener" target="_blank">Caffe</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/DLSS4/">NVIDIA DLSS 4</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/AMD-FSR/">AMD FidelityFX Super Resolution</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/CG-Pipeline/">The Real-Time Rendering Pipeline</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jiacheng LI. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>