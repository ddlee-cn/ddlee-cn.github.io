<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Streaming &amp; Display | LI, Jiacheng (李 家丞) </title> <meta name="author" content="Jiacheng LI"> <meta name="description" content="A research scientist with Sony Research, focusing on multimedia technology. "> <meta name="keywords" content="Jiacheng Li, Sony Research, Sony AI, Sony, USTC"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ddlee-cn.github.io/streaming-&amp;-display/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> LI, Jiacheng (李 家丞) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/computational-photography/">Computational Photography</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rendering-&amp;-generative-ai/">Rendering &amp; Genenerative AI</a> <div class="dropdown-divider"></div> <a class="dropdown-item active" href="/streaming-&amp;-display/">Streaming &amp; Display</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/intelligent-sensing/">Intelligent Sensing</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Streaming &amp; Display</h1> <p class="post-description"></p> </header> <article> <div class="post"> <article> <hr> <div class="profile float-right"> </div> <div class="clearfix"> <h2 id="toward-efficiency-learned-look-up-tables-luts">Toward Efficiency: Learned Look-Up Tables (LUTs)</h2> <p>My deep dive into Learned Look-Up Tables (LUTs)—a fundamental data structure in the <a href="/blog/2022/ISP/">image signal pipeline</a>—began with an appreciation for the pioneering SR-LUT<sup id="fnref:SRLUT"><a href="#fn:SRLUT" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> paper. It demonstrated the remarkable potential of achieving interpolation-level efficiency for super-resolution by converting a compact neural network into a LUT. However, its limitation to fixed upsampling factors presented a clear challenge for the field: how to imbue learned LUTs with the critical property of continuity to truly rival classical interpolation.</p> <p>Our initial attempts, inspired by MetaSR<sup id="fnref:MetaSR"><a href="#fn:MetaSR" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, revealed that directly predicting interpolation weights was infeasible with the limited capacity of a small receptive field. This led to our first key contribution: a method for expanding model capacity by orchestrating multiple LUTs in concert, much like layers in a deep network. This approach effectively overcame the exponential size growth of a single LUT, culminating in our publications on <a href="/publications/#cooperating%20multiple">MuLUT</a> and its more advanced successor, <a href="/publications/#dnn%20of%20luts">DNN-of-LUTs</a>.</p> <p>A subsequent breakthrough was born from a moment of insight while re-examining classic operators. While diving deeper to the origin of the coefficients of Bicubic interpolation, I realized they are derived from underlying assumptions of smoothness and continuity under a cubic formulation (i.e., the resampling function is a cubic polynomial). This sparked a new idea: instead of predicting interpolation weights from scratch, why not predict the hyperparameters that define these constraints? This concept formed the basis for our research on <a href="/publications/#learning%20steerable%20resampling">Learning Resampling Function (LeRF)</a>. Drawing inspiration from the seminal work on Steering Kernel Regression<sup id="fnref:SKR"><a href="#fn:SKR" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, which notably powers Google Camera’s Super Res Zoom feature<sup id="fnref:SuperRes"><a href="#fn:SuperRes" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, we proposed a learning-based method, built upon MuLUT, to adaptively learn the hyperparameters that shape the resampling functions for different spatial locations. LeRF delivered on the original promise: a learned, continuous alternative to classical interpolation that runs <strong>as fast as interpolation</strong>, <strong>generalizes well</strong> to arbitrary transformations, and <strong>outperforms it significantly</strong>, achieving up to a 3dB PSNR gain over Bicubic. We later refined this framework in <a href="/publications/#lerf:">LeRF++</a>.</p> <p>Another line of our research, <a href="/publications/#look-up%20table%20compression">Diagonal First Compression (DFC)</a>, was inspired by a unexpected discovery. I discovered an <a href="https://github.com/ddlee-cn/MuLUT/blob/cfbf43b4ec8212e50c2476f03b29cf01b8c4014b/sr/4_test_lut.py#L178" rel="external nofollow noopener" target="_blank">implementation bug</a> in previous works while applying MuLUT to image demoisaicing. This bug, which never appears in super-resolution, highlighted a <strong>diagonal dominance property</strong> in the activation patterns during LUT lookups. This observation aligns with the known low-manifold distribution of natural image data<sup id="fnref:LocalBehavior"><a href="#fn:LocalBehavior" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>, indicating inherent redundancy in the learned LUTs. Consequently, we developed a diagonal-first compression technique that significantly reduces LUT size (achieving up to 10x compression) while preserving performance.</p> <p>The interplay between the LUT trilogy (MuLUT, LeRF, and DFC), particularly concerning LUT size, can be understood through the following formulation:</p> <p><img src="https://i.imgur.com/njDYybL.png" alt="LUT-Summary"></p> <p>Beyond image processing, our work on learned LUTs has also extended into the domain of <a href="/blog/2022/Image-Video-Codec/">video coding</a> with the development of <a href="/publications/#in-loop">ILF-LUT</a>. This method integrates learned LUTs into the video codec pipeline, specifically as an in-loop filter. ILF-LUT has demonstrated substantial improvements over existing in-loop filtering parts in the latest Versatile Video Coding (VVC) standard, offering a way to integrate learned components into the video codec pipeline. In the field of image and video codecs, beyond learned LUTs, I also mentored a work for <a href="/publications/#plug-and-play%20versatile">Versatile Compressed Video Enhancement</a> that takes the advantages of codec priors like motion vectors. Finally, I also contributed to a robust framework for <a href="/publications/#all-in-one">All-in-One Image Compression and Restoration</a>.</p> <p>The concept of learned LUTs is gaining increasing traction within the research community. We are witnessing more advanced developments, exploration of new application scenarios (such as video quality enhancement<sup id="fnref:VQE-LUT"><a href="#fn:VQE-LUT" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>), and deployment on diverse hardware platforms (including FPGAs<sup id="fnref:FPGA-LUT"><a href="#fn:FPGA-LUT" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>). Looking ahead, I am dedicated to further expanding the applications of learned LUTs and developing more generalizable and powerful LUT-based components🚀.</p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:SRLUT"> <p><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Jo_Practical_Single-Image_Super-Resolution_Using_Look-Up_Table_CVPR_2021_paper.html" rel="external nofollow noopener" target="_blank">Practical Single-Image Super-Resolution Using Look-Up Table</a>, in CVPR 2021 <a href="#fnref:SRLUT" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:MetaSR"> <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Meta-SR_A_Magnification-Arbitrary_Network_for_Super-Resolution_CVPR_2019_paper.html" rel="external nofollow noopener" target="_blank">Meta-SR: A Magnification-Arbitrary Network for Super-Resolution</a>, in CVPR 2019 <a href="#fnref:MetaSR" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:SKR"> <p><a href="http://ieeexplore.ieee.org/document/4060955/" rel="external nofollow noopener" target="_blank">Kernel Regression for Image Processing and Reconstruction</a>, in T-IP 2007 <a href="#fnref:SKR" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:SuperRes"> <p><a href="https://research.google/blog/see-better-and-further-with-super-res-zoom-on-the-pixel-3/" rel="external nofollow noopener" target="_blank">See Better and Further with Super Res Zoom on the Pixel 3</a>, Google Research Blog <a href="#fnref:SuperRes" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:LocalBehavior"> <p><a href="https://link.springer.com/article/10.1007/s11263-007-0056-x" rel="external nofollow noopener" target="_blank">On the local behavior of spaces of natural images</a>, in IJCV 2008 <a href="#fnref:LocalBehavior" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:VQE-LUT"> <p><a href="http://arxiv.org/abs/2311.13616" rel="external nofollow noopener" target="_blank">Online Video Quality Enhancement with Spatial-Temporal Look-up Tables</a>, in ECCV 2024 <a href="#fnref:VQE-LUT" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:FPGA-LUT"> <p><a href="https://ieeexplore.ieee.org/abstract/document/10595880" rel="external nofollow noopener" target="_blank">An Energy-Efficient Look-up Table Framework for Super Resolution on FPGA</a>, in AICAS 2024 <a href="#fnref:FPGA-LUT" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> <hr> <div class="profile float-left"> </div> <div class="clearfix"> <h2 id="toward-faithfulness-hdr-display">Toward Faithfulness: HDR Display</h2> <p>High Dynamic Range (HDR) imaging and precise <a href="/blog/2023/Color-Science/">color reproduction</a> are fundamental to creating visually faithful and realistic media. My own deep dive into this field was sparked by the impressive “Live HDR+”<sup id="fnref:live-hdr"><a href="#fn:live-hdr" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> feature on <a href="/blog/2022/Google-Camera/">Google Pixel Phones</a>, a powerful application of the seminal HDRNet framework<sup id="fnref:HDRNet"><a href="#fn:HDRNet" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. This experience ignited a deeper interest in the entire <a href="/blog/2023/HDR-Imaging-Display/">HDR tech</a> ecosystem, from capture techniques like bracketing exposure and staggered pixels to advanced display standards such as PQ (Perceptual Quantizer), HLG (Hybrid Log-Gamma), and Dolby Vision. <del>This interest also led to an upgrade💰 of my home cinema setup</del>.</p> <p>The comprehensive adoption of HDR across both content capture and display is an inevitable technological progression. Since late 2023, the industry has converged on a powerful solution for distributing HDR images with backward compatibility: the use of HDR gain maps. This supplementary metadata allows a single file to be rendered correctly on both Standard Dynamic Range (SDR) and HDR displays, an approach now championed by industry leaders like Apple<sup id="fnref:Apple"><a href="#fn:Apple" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, Google<sup id="fnref:Google"><a href="#fn:Google" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>, and Adobe<sup id="fnref:Adobe"><a href="#fn:Adobe" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>. [2025 Update: This is now part of the emerging ISO 21496 standard<sup id="fnref:ISO21496"><a href="#fn:ISO21496" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>.]</p> <p>My research directly tackles this transition by developing learning-based tools to enable seamless HDR adoption. I have mentored two key projects in this domain:</p> <ul> <li> <a href="/publications/#mlp%20embedded">MLP Embedded Inverse Tone Mapping (ITM)</a>: We developed a framework that embeds a lightweight, per-image MLP network as “neural metadata” within a standard SDR file. This allows for high-fidelity Inverse Tone Mapping on HDR screens, effectively restoring the content’s original dynamic range.</li> <li> <a href="/publications/#learning%20gain%20map">Learning Gain Maps for ITM</a>: To bring the benefits of HDR to legacy content, this project developed a neural network capable of predicting gain maps for existing SDR images. This approach expands their dynamic range for compelling HDR presentation. As a key contribution, we also curated a new real-world dataset to drive further research and development.</li> </ul> <p>Looking forward, my goal is to push the boundaries of visual faithfulness further. I plan to extend HDR principles into new dimensions, exploring temporal consistency for video and view-dependent effects for truly immersive and realistic visual experiences.</p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:live-hdr"> <p><a href="https://research.google/blog/live-hdr-and-dual-exposure-controls-on-pixel-4-and-4a/" rel="external nofollow noopener" target="_blank">Live HDR+ and Dual Exposure Controls on Pixel 4 and 4a</a>, Google Research Blog <a href="#fnref:live-hdr" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:HDRNet"> <p><a href="https://groups.csail.mit.edu/graphics/hdrnet/" rel="external nofollow noopener" target="_blank">Deep bilateral learning for real-time image enhancement</a>, in SIGGRPAH 2017 <a href="#fnref:HDRNet" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Apple"> <p><a href="https://developer.apple.com/documentation/appkit/applying-apple-hdr-effect-to-your-photos" rel="external nofollow noopener" target="_blank">Applying Apple HDR effect to your photos</a> <a href="#fnref:Apple" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Google"> <p><a href="https://developer.android.com/media/platform/hdr-image-format" rel="external nofollow noopener" target="_blank">Ultra HDR Image Format</a> <a href="#fnref:Google" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:Adobe"> <p><a href="https://helpx.adobe.com/camera-raw/using/gain-map.html" rel="external nofollow noopener" target="_blank">Gain Map in Adobe Camera Raw</a> <a href="#fnref:Adobe" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:ISO21496"> <p><a href="https://www.iso.org/standard/86775.html" rel="external nofollow noopener" target="_blank">ISO 21496: Gain map metadata for image conversion</a> <a href="#fnref:ISO21496" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Jiacheng LI. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>