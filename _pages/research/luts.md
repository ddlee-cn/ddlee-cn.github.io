## Toward Efficiency: Learned Look-Up Tables (LUTs)

My deep dive into Learned Look-Up Tables (LUTs)â€”a fundamental data structure in the [image signal pipeline]({{ site.baseurl }}/blog/2022/ISP/)â€”began with an appreciation for the pioneering SR-LUT[^SRLUT] paper. It demonstrated the remarkable potential of achieving interpolation-level efficiency for super-resolution by converting a compact neural network into a LUT. However, its limitation to fixed upsampling factors presented a clear challenge for the field: how to imbue learned LUTs with the critical property of continuity to truly rival classical interpolation.

Our initial attempts, inspired by MetaSR[^MetaSR], revealed that directly predicting interpolation weights was infeasible with the limited capacity of a small receptive field. This led to our first key contribution: a method for expanding model capacity by orchestrating multiple LUTs in concert, much like layers in a deep network. This approach effectively overcame the exponential size growth of a single LUT, culminating in our publications on [MuLUT]({{ site.baseurl }}/publications/#cooperating%20multiple) and its more advanced successor, [DNN-of-LUTs]({{ site.baseurl }}/publications/#dnn%20of%20luts).

A subsequent breakthrough was born from a moment of insight while re-examining classic operators. While diving deeper to the origin of the coefficients of Bicubic interpolation, I realized they are derived from underlying assumptions of smoothness and continuity under a cubic formulation (i.e., the resampling function is a cubic polynomial). This sparked a new idea: instead of predicting interpolation weights from scratch, why not predict the hyperparameters that define these constraints? This concept formed the basis for our research on [Learning Resampling Function (LeRF)]({{ site.baseurl }}/publications/#learning steerable resampling). Drawing inspiration from the seminal work on Steering Kernel Regression[^SKR], which notably powers Google Camera's Super Res Zoom feature[^SuperRes], we proposed a learning-based method, built upon MuLUT, to adaptively learn the hyperparameters that shape the resampling functions for different spatial locations. LeRF delivered on the original promise: a learned, continuous alternative to classical interpolation that runs **as fast as interpolation**, **generalizes well** to arbitrary transformations, and **outperforms it significantly**, achieving up to a 3dB PSNR gain over Bicubic. We later refined this framework in [LeRF++]({{ site.baseurl }}/publications/#lerf:).

Another line of our research, [Diagonal First Compression (DFC)]({{ site.baseurl }}/publications/#look-up%20table%20compression), was inspired by a unexpected discovery. I discovered an [implementation bug](https://github.com/ddlee-cn/MuLUT/blob/cfbf43b4ec8212e50c2476f03b29cf01b8c4014b/sr/4_test_lut.py#L178) in previous works while applying MuLUT to image demoisaicing. This bug, which never appears in super-resolution, highlighted a **diagonal dominance property** in the activation patterns during LUT lookups. This observation aligns with the known low-manifold distribution of natural image data[^LocalBehavior], indicating inherent redundancy in the learned LUTs. Consequently, we developed a diagonal-first compression technique that significantly reduces LUT size (achieving up to 10x compression) while preserving performance.

The interplay between the LUT trilogy (MuLUT, LeRF, and DFC), particularly concerning LUT size, can be understood through the following formulation:

![LUT-Summary](https://i.imgur.com/njDYybL.png)

Beyond image processing, our work on learned LUTs has also extended into the domain of [video coding]({{ site.baseurl }}/blog/2022/Image-Video-Codec/) with the development of [ILF-LUT]({{ site.baseurl }}/publications/#in-loop). This method integrates learned LUTs into the video codec pipeline, specifically as an in-loop filter. ILF-LUT has demonstrated substantial improvements over existing in-loop filtering parts in the latest Versatile Video Coding (VVC) standard, offering a way to integrate learned components into the video codec pipeline. In the field of image and video codecs, beyond learned LUTs, I also mentored a work for [Versatile Compressed Video Enhancement]({{ site.baseurl }}/publications/#plug-and-play versatile) that takes the advantages of codec priors like motion vectors. Finally, I also contributed to a robust framework for [All-in-One Image Compression and Restoration]({{ site.baseurl }}/publications/#all-in-one).

The concept of learned LUTs is gaining increasing traction within the research community. We are witnessing more advanced developments, exploration of new application scenarios (such as video quality enhancement[^VQE-LUT]), and deployment on diverse hardware platforms (including FPGAs[^FPGA-LUT]). Looking ahead, I am dedicated to further expanding the applications of learned LUTs and developing more generalizable and powerful LUT-based componentsðŸš€.


**References**


[^SRLUT]: [Practical Single-Image Super-Resolution Using Look-Up Table](https://openaccess.thecvf.com/content/CVPR2021/html/Jo_Practical_Single-Image_Super-Resolution_Using_Look-Up_Table_CVPR_2021_paper.html), in CVPR 2021
[^MetaSR]: [Meta-SR: A Magnification-Arbitrary Network for Super-Resolution](https://openaccess.thecvf.com/content_CVPR_2019/html/Hu_Meta-SR_A_Magnification-Arbitrary_Network_for_Super-Resolution_CVPR_2019_paper.html), in CVPR 2019
[^SuperRes]: [See Better and Further with Super Res Zoom on the Pixel 3](https://research.google/blog/see-better-and-further-with-super-res-zoom-on-the-pixel-3/), Google Research Blog
[^SKR]: [Kernel Regression for Image Processing and Reconstruction](http://ieeexplore.ieee.org/document/4060955/), in T-IP 2007
[^VQE-LUT]: [Online Video Quality Enhancement with Spatial-Temporal Look-up Tables](http://arxiv.org/abs/2311.13616), in ECCV 2024
[^FPGA-LUT]: [An Energy-Efficient Look-up Table Framework for Super Resolution on FPGA](https://ieeexplore.ieee.org/abstract/document/10595880), in AICAS 2024
[^LocalBehavior]: [On the local behavior of spaces of natural images](https://link.springer.com/article/10.1007/s11263-007-0056-x), in IJCV 2008