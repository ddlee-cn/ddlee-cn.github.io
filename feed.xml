<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ddlee-cn.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ddlee-cn.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-10T08:25:58+00:00</updated><id>https://ddlee-cn.github.io/feed.xml</id><title type="html">LI, Jiacheng (李 家丞)</title><subtitle>A research scientist with Sony Research, focusing on multimedia technology. </subtitle><entry><title type="html">NVIDIA DLSS 4</title><link href="https://ddlee-cn.github.io/blog/2025/DLSS4/" rel="alternate" type="text/html" title="NVIDIA DLSS 4"/><published>2025-04-04T17:39:00+00:00</published><updated>2025-04-04T17:39:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2025/DLSS4</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2025/DLSS4/"><![CDATA[<h2 id="i-introduction"><strong>I. Introduction</strong></h2> <p>The foundational principle of DLSS is to decouple the internal rendering resolution from the final display resolution. It allows the graphics pipeline to render the primary 3D scene at a lower internal resolution, thereby significantly reducing the computational workload on the GPU. Subsequently, sophisticated AI algorithms, executed on specialized hardware units known as Tensor Cores within NVIDIA’s RTX series of GPUs, are employed to intelligently reconstruct a high-quality image at the target display resolution. Over its iterations, DLSS has expanded its capabilities significantly beyond mere super-resolution upscaling.</p> <p>The advent and evolution of DLSS signify a paradigm shift in how rendering efficiency is approached. Rather than solely relying on brute-force computation for every pixel at the native display resolution, AI facilitates an intelligent reconstruction from a reduced workload. This strategic reallocation of rendering budget frees up GPU resources, which can then be utilized to achieve higher frame rates, enable more demanding graphical settings (such as higher levels of ray tracing), or drive higher display resolutions, all within the same performance envelope.</p> <p>Furthermore, the trajectory of DLSS highlights a critical co-evolutionary relationship between AI algorithms, GPU architectures, and game engine integration. The capabilities and performance of each DLSS iteration are not merely a product of more sophisticated AI models but are intrinsically linked to advancements in NVIDIA’s GPU hardware—particularly the evolution of Tensor Core capabilities and the introduction of specialized hardware units like the Optical Flow Accelerator in the Ada Lovelace architecture for DLSS 3.0 Frame Generation, and new features in the Blackwell architecture for DLSS 4.</p> <h2 id="ii-evolution-of-nvidia-dlss"><strong>II. Evolution of NVIDIA DLSS</strong></h2> <p>NVIDIA’s Deep Learning Super Sampling has undergone a significant evolutionary journey since its inception, with each major iteration addressing limitations of its predecessor and introducing new AI-driven capabilities. This progression reflects a continuous refinement of AI models, a deeper integration with the graphics pipeline, and an increasing synergy with NVIDIA’s GPU hardware advancements.</p> <ul> <li><strong>DLSS 1.x (2019)<sup id="fnref:DLSS1"><a href="#fn:DLSS1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>: The Spatial Pioneer</strong> DLSS 1.0 was primarily a <strong>spatial image upscaler</strong>. It employed convolutional auto-encoder neural networks to achieve its upscaling. A defining characteristic of this initial version was its <strong>per-game training requirement</strong>. NVIDIA had to train a unique AI model for each supported game. This training process involved feeding the neural network vast numbers of aliased low-resolution frames from the specific game, alongside corresponding “ground truth” high-resolution reference images. These reference images were typically generated using computationally intensive methods like extreme super-sampling (e.g., 64 samples per pixel) on NVIDIA’s supercomputer infrastructure.11 Some analyses suggest DLSS 1.0 operated via a two-stage process: an initial image enhancement network that utilized the current frame and motion vectors for tasks like edge enhancement and rudimentary spatial anti-aliasing, followed by a separate upscaling network that operated primarily on the single raw low-resolution frame to produce the final output resolution.</li> </ul> <p><img src="https://i.imgur.com/lRRYH6f.jpeg" alt=""/></p> <ul> <li><strong>DLSS 2.x (2020)<sup id="fnref:DLSS2"><a href="#fn:DLSS2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>: The Temporal Revolution and Generalization</strong> DLSS 2.0 represented a major architectural overhaul and a paradigm shift from its predecessor. It introduced a <strong>generalized AI model</strong> and transitioned to being a <strong>temporal anti-aliasing upsampling (TAAU)</strong> technique. This meant it extensively utilized data from previously rendered frames, in addition to the current frame, to inform the reconstruction process. The AI model was a single, generalized convolutional autoencoder network. Crucially, this network was trained on a diverse dataset of non-game-specific content (e.g., thousands of high-resolution images rendered offline with high sample counts), allowing it to work effectively across a wide variety of games and visual styles without the need for per-game retraining. DLSS 2.0 employed <strong>temporal feedback</strong>, where the AI network takes the low-resolution current frame and the high-resolution output from the previous frame (reprojected using motion vectors) to determine, on a pixel-by-pixel basis, how to generate a higher quality current frame.</li> </ul> <p><img src="https://i.imgur.com/XsXiRuD.jpeg" alt=""/></p> <ul> <li><strong>DLSS 3.0 (2022)<sup id="fnref:DLSS3"><a href="#fn:DLSS3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>: Optical Multi Frame Generation (AI Frame Generation)</strong> DLSS 3.0 builds upon the Super Resolution capabilities of DLSS 2.x and introduces a novel AI-powered <strong>Frame Generation</strong> technique. This technology synthesizes entirely new frames, inserting them between traditionally rendered (and super-resolved) frames, rather than just upscaling pixels within existing frames.12 This approach can lead to dramatic increases in displayed FPS, particularly beneficial in CPU-bound scenarios where traditional rendering is limited by the game engine’s processing speed. The <strong>frame generation</strong> component of DLSS 3.0 employs a convolutional autoencoder. This neural network takes current and prior game frames, an optical flow field, and game engine data (such as motion vectors and depth) as inputs to predict and generate the intermediate frame. <strong>Optical Flow Accelerator (OFA)</strong> is another crucial hardware innovation introduced with the NVIDIA Ada Lovelace architecture (found in GeForce RTX 40 Series GPUs). The OFA is a dedicated unit designed to analyze two sequential in-game frames and calculate a dense optical flow field. This field captures pixel-level motion for elements that traditional game engine motion vectors might not accurately model, such as particles, reflections, shadows, and lighting effects.12 The OFA was a hardware-exclusive feature making DLSS 3.0 Frame Generation initially available only on RTX 40 Series GPUs.</li> </ul> <p><img src="https://i.imgur.com/Fw1IDUo.jpeg" alt=""/></p> <ul> <li><strong>DLSS 3.5 (2023)<sup id="fnref:DLSS35"><a href="#fn:DLSS35" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>: Ray Reconstruction (RR)</strong> DLSS 3.5 introduced <strong>Ray Reconstruction</strong>, an AI-powered neural rendering technique specifically designed to improve the image quality of ray-traced effects. It achieves this by replacing the multiple, often hand-tuned, denoisers traditionally used in ray tracing pipelines with a single, more advanced and unified AI network. The Ray Reconstruction AI model was trained on significantly more data—reportedly 5x more than the models used in DLSS 3.0. This extensive training enables the AI to recognize various ray-traced effects (such as global illumination, ambient occlusion, reflections, and shadows) with greater accuracy. It makes more intelligent decisions about how to utilize temporal and spatial data from the noisy ray-traced input, and is better at retaining high-frequency information, which is crucial for subsequent upscaling stages.</li> </ul> <p>The evolution from DLSS 1.0 to 3.5 showcases a clear pattern of iterative problem-solving and increasing specialization. Each version targeted specific limitations of its predecessor—DLSS 1.0’s spatial-only upscaling and per-game training were addressed by DLSS 2.0’s temporal accumulation and generalized model. DLSS 2.0’s residual temporal artifacts and the desire for higher raw FPS paved the way for DLSS 3.0’s Frame Generation, which, in turn, introduced new considerations around latency and UI handling. DLSS 3.5 then focused on a very specific and challenging aspect of modern graphics: the quality of ray tracing denoising. This progression has resulted in a suite of specialized AI models (Super Resolution, Frame Generation, Ray Reconstruction) rather than a single, monolithic AI attempting to solve all rendering challenges. This modularity appears to be a key strategy in tackling the immense complexity of real-time graphics.</p> <p>Concurrently, each major DLSS iteration has become more deeply intertwined with specific NVIDIA GPU hardware capabilities, moving beyond general compute. While Tensor Cores have been foundational to all DLSS versions, DLSS 3.0’s Frame Generation was made practical and performant on the RTX 40 Series due to the dedicated Optical Flow Accelerator. This hardware unit provided crucial optical flow data at high speed, a task that might have been too slow or of lower quality if attempted purely in software on previous-generation hardware. This trend indicates that NVIDIA is not just developing AI algorithms in isolation but is co-designing them with hardware accelerators to achieve performance targets that would otherwise be unattainable. This creates a powerful hardware-software ecosystem but also tends to tie the most advanced features to the latest hardware generations.</p> <p>As DLSS has evolved to boost GPU rendering throughput (via Super Resolution) and apparent frame rates (via Frame Generation), other system bottlenecks, such as CPU performance, memory bandwidth, and input latency, have become more prominent. NVIDIA’s introduction of Reflex as an integral part of DLSS 3.0, and its continued emphasis in the DLSS 4 framework, demonstrates an awareness that true perceived performance is a holistic characteristic of the entire system. Super Resolution eases the load on the GPU’s shading units. Frame Generation can even provide benefits in CPU-bound scenarios by synthesizing frames largely independently of the main game simulation loop.14 However, merely increasing FPS numbers is insufficient if system latency increases to a detrimental degree or if critical elements like the UI become visually compromised. The mandatory inclusion of Reflex with DLSS 3.0 Frame Generation was a direct acknowledgment that the latency introduced by frame interpolation required active mitigation. This holistic view, considering the entire end-to-end pipeline from player input to final display, is crucial for delivering a genuinely enhanced user experience.</p> <h2 id="iii-deep-dive-nvidia-dlss-4"><strong>III. Deep Dive: NVIDIA DLSS 4</strong></h2> <svg viewBox="0 0 980 900" xmlns="http://www.w3.org/2000/svg"> <defs> <linearGradient id="cpuGradient" x1="0%" y1="0%" x2="100%" y2="100%"> <stop offset="0%" style="stop-color:#4A90E2;stop-opacity:1"/> <stop offset="100%" style="stop-color:#357ABD;stop-opacity:1"/> </linearGradient> <linearGradient id="gpuGradient" x1="0%" y1="0%" x2="100%" y2="100%"> <stop offset="0%" style="stop-color:#76B900;stop-opacity:1"/> <stop offset="100%" style="stop-color:#5A8A00;stop-opacity:1"/> </linearGradient> <linearGradient id="dlssGradient" x1="0%" y1="0%" x2="100%" y2="100%"> <stop offset="0%" style="stop-color:#FF6B35;stop-opacity:1"/> <stop offset="100%" style="stop-color:#E55A2B;stop-opacity:1"/> </linearGradient> <linearGradient id="displayGradient" x1="0%" y1="0%" x2="100%" y2="100%"> <stop offset="0%" style="stop-color:#9B59B6;stop-opacity:1"/> <stop offset="100%" style="stop-color:#8E44AD;stop-opacity:1"/> </linearGradient> <linearGradient id="inputGradient" x1="0%" y1="0%" x2="100%" y2="100%"> <stop offset="0%" style="stop-color:#A0A0A0;stop-opacity:1"/> <stop offset="100%" style="stop-color:#808080;stop-opacity:1"/> </linearGradient> <filter id="shadow" x="-50%" y="-50%" width="200%" height="200%"> <feDropShadow dx="2" dy="3" stdDeviation="3"/> </filter> <marker id="arrowhead" markerWidth="9" markerHeight="6" refX="8" refY="3" orient="auto"> <polygon points="0 0, 9 3, 0 6" class="arrowhead-main-fill"/> </marker> <marker id="inputArrowhead" markerWidth="9" markerHeight="6" refX="8" refY="3" orient="auto"> <polygon points="0 0, 9 3, 0 6" class="arrowhead-input-fill"/> </marker> <style>.svg-background{fill:transparent}.main-title-text{fill:#4d4d4d}.annotation-text{fill:#737373}.legend-text{fill:#4d4d4d}.legend-background{fill:transparent;stroke:transparent}.flow-arrow-general-stroke{stroke:#737373}.arrowhead-main-fill{fill:#737373}.flow-arrow-input-stroke{stroke:#6482b9}.arrowhead-input-fill{fill:#6482b9}#shadow feDropShadow{flood-color:#000;flood-opacity:.2}</style> </defs> <rect width="980" height="900" class="svg-background"/> <text x="490" y="40" text-anchor="middle" font-family="Arial, sans-serif" font-size="22" font-weight="bold" class="main-title-text"> Graphics Pipeline with DLSS 4 Integration </text> <g id="pipeline-row1"> <g id="application-stage"> <rect x="90" y="80" width="240" height="85" rx="8" fill="url(#cpuGradient)" filter="url(#shadow)"/> <text x="210" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Application Stage</text> <text x="210" y="128" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(CPU)</text> <text x="210" y="148" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Game Logic, Physics, Input</text> </g> <g id="geometry-processing"> <rect x="370" y="80" width="240" height="85" rx="8" fill="url(#gpuGradient)" filter="url(#shadow)"/> <text x="490" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Geometry Processing</text> <text x="490" y="128" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(GPU)</text> <text x="490" y="148" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Vertex Shading, Projection</text> </g> <g id="rasterization"> <rect x="650" y="80" width="240" height="85" rx="8" fill="url(#gpuGradient)" filter="url(#shadow)"/> <text x="770" y="108" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Rasterization</text> <text x="770" y="128" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(GPU)</text> <text x="770" y="148" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Primitives to Fragments</text> </g> </g> <g id="pipeline-row2"> <g id="pixel-shading-rt"> <rect x="50" y="205" width="240" height="85" rx="8" fill="url(#gpuGradient)" filter="url(#shadow)"/> <text x="170" y="233" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Pixel/RT Shading</text> <text x="170" y="253" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(GPU Render Stage)</text> <text x="170" y="273" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Lower res if DLSS active</text> </g> <g id="dlss-ray-reconstruction"> <rect x="330" y="205" width="280" height="85" rx="8" fill="url(#dlssGradient)" filter="url(#shadow)"/> <text x="470" y="233" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">DLSS Ray Reconstruction</text> <text x="470" y="253" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(AI Denoiser)</text> <text x="470" y="273" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Cleans noisy ray tracing output</text> </g> <g id="dlss-super-resolution"> <rect x="650" y="205" width="280" height="85" rx="8" fill="url(#dlssGradient)" filter="url(#shadow)"/> <text x="790" y="233" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">DLSS Super Resolution</text> <text x="790" y="253" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(AI Upscaler)</text> <text x="790" y="273" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Upscales frames with motion vectors</text> </g> </g> <g id="annotations-row2" font-family="Arial, sans-serif" font-size="11"> <text x="170" y="300" class="annotation-text"> <tspan x="170" text-anchor="middle" dy="0em" font-weight="bold">Pixel/RT Shading Note:</tspan> <tspan x="170" text-anchor="middle" dy="1.3em">Lower internal resolution when</tspan> <tspan x="170" text-anchor="middle" dy="1.3em">DLSS SR/RR features are active.</tspan> </text> <text x="790" y="300" class="annotation-text"> <tspan x="790" text-anchor="middle" dy="0em" font-weight="bold">DLSS Super Resolution:</tspan> <tspan x="790" text-anchor="middle" dy="1.3em">AI upscales frames using temporal</tspan> <tspan x="790" text-anchor="middle" dy="1.3em">data and motion vectors.</tspan> </text> </g> <g id="pipeline-row3"> <g id="post-processing"> <rect x="70" y="370" width="240" height="85" rx="8" fill="url(#gpuGradient)" filter="url(#shadow)"/> <text x="190" y="398" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Post-Processing</text> <text x="190" y="418" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(GPU)</text> <text x="190" y="438" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Full-screen effects, Tone Mapping</text> </g> <g id="frame-buffer"> <rect x="350" y="370" width="240" height="85" rx="8" fill="url(#gpuGradient)" filter="url(#shadow)"/> <text x="470" y="398" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Frame Buffer</text> <text x="470" y="418" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(GPU Memory)</text> <text x="470" y="438" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Stores final/intermediate frames</text> </g> <g id="dlss-frame-generation"> <rect x="630" y="370" width="280" height="85" rx="8" fill="url(#dlssGradient)" filter="url(#shadow)"/> <text x="770" y="398" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">DLSS Frame Generation</text> <text x="770" y="418" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(AI Frame Interpolation)</text> <text x="770" y="438" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Generates additional frames</text> </g> </g> <g id="annotations-row3" font-family="Arial, sans-serif" font-size="11"> <text x="770" y="465" class="annotation-text"> <tspan x="770" text-anchor="middle" dy="0em" font-weight="bold">DLSS Frame Generation:</tspan> <tspan x="770" text-anchor="middle" dy="1.3em">AI generates new frames between</tspan> <tspan x="770" text-anchor="middle" dy="1.3em">rendered frames for higher FPS.</tspan> </text> </g> <g id="pipeline-row4"> <g id="player-input"> <rect x="80" y="535" width="220" height="85" rx="8" fill="url(#inputGradient)" filter="url(#shadow)"/> <text x="190" y="563" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Latest Player Input</text> <text x="190" y="583" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(Mouse, Keyboard, etc.)</text> </g> <g id="dlss-reflex-frame-warp"> <rect x="340" y="535" width="280" height="85" rx="8" fill="url(#dlssGradient)" filter="url(#shadow)"/> <text x="480" y="563" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">DLSS Reflex Frame Warp</text> <text x="480" y="583" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">(Low Latency Sync)</text> <text x="480" y="603" text-anchor="middle" font-family="Arial, sans-serif" font-size="11" fill="white">Last-moment reprojection</text> </g> <g id="display-output"> <rect x="660" y="535" width="240" height="85" rx="8" fill="url(#displayGradient)" filter="url(#shadow)"/> <text x="780" y="563" text-anchor="middle" font-family="Arial, sans-serif" font-size="14" font-weight="bold" fill="white">Display</text> <text x="780" y="587" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="white">Final Image Output</text> </g> </g> <g id="annotations-row4" font-family="Arial, sans-serif" font-size="11"> <text x="480" y="630" class="annotation-text"> <tspan x="480" text-anchor="middle" dy="0em" font-weight="bold">DLSS Reflex Frame Warp:</tspan> <tspan x="480" text-anchor="middle" dy="1.3em">Syncs rendering with latest input,</tspan> <tspan x="480" text-anchor="middle" dy="1.3em">reprojects to reduce latency.</tspan> </text> </g> <g id="flow-arrows" stroke-width="2.5" fill="none" class="flow-arrow-general-stroke"> <line x1="330" y1="122.5" x2="360" y2="122.5" marker-end="url(#arrowhead)"/> <line x1="610" y1="122.5" x2="640" y2="122.5" marker-end="url(#arrowhead)"/> <path d="M 770 165 V 185 H 170 V 205" marker-end="url(#arrowhead)"/> <line x1="290" y1="247.5" x2="320" y2="247.5" marker-end="url(#arrowhead)"/> <line x1="610" y1="247.5" x2="640" y2="247.5" marker-end="url(#arrowhead)"/> <path d="M 790 290 V 350 H 190 V 370" marker-end="url(#arrowhead)"/> <line x1="310" y1="412.5" x2="340" y2="412.5" marker-end="url(#arrowhead)"/> <line x1="590" y1="412.5" x2="620" y2="412.5" marker-end="url(#arrowhead)"/> <path d="M 770 455 V 515 H 480 V 535" marker-end="url(#arrowhead)"/> <line x1="300" y1="577.5" x2="330" y2="577.5" class="flow-arrow-input-stroke" stroke-dasharray="5,5" marker-end="url(#inputArrowhead)"/> <line x1="620" y1="577.5" x2="650" y2="577.5" marker-end="url(#arrowhead)"/> </g> <g id="legend"> <rect x="50" y="720" width="880" height="130" rx="10" class="legend-background" stroke-width="1.5"/> <text x="80" y="745" font-family="Arial, sans-serif" font-size="15" font-weight="bold" class="legend-text">Legend</text> <rect x="100" y="765" width="22" height="18" fill="url(#cpuGradient)" rx="3"/> <text x="132" y="779" font-family="Arial, sans-serif" font-size="12" class="legend-text">CPU Processing Stage</text> <rect x="100" y="790" width="22" height="18" fill="url(#gpuGradient)" rx="3"/> <text x="132" y="804" font-family="Arial, sans-serif" font-size="12" class="legend-text">Standard GPU Processing Stage</text> <rect x="400" y="765" width="22" height="18" fill="url(#dlssGradient)" rx="3"/> <text x="432" y="779" font-family="Arial, sans-serif" font-size="12" class="legend-text">DLSS Technology Component (GPU)</text> <rect x="400" y="790" width="22" height="18" fill="url(#displayGradient)" rx="3"/> <text x="432" y="804" font-family="Arial, sans-serif" font-size="12" class="legend-text">Display Output Stage</text> <rect x="700" y="765" width="22" height="18" fill="url(#inputGradient)" rx="3"/> <text x="732" y="779" font-family="Arial, sans-serif" font-size="12" class="legend-text">User Input Source</text> <line x1="100" y1="825" x2="122" y2="825" class="flow-arrow-general-stroke" stroke-width="2.5" marker-end="url(#arrowhead)"/> <text x="132" y="829" font-family="Arial, sans-serif" font-size="12" class="legend-text">Main Data Flow</text> <line x1="400" y1="825" x2="422" y2="825" class="flow-arrow-input-stroke" stroke-width="2.5" stroke-dasharray="4,4" marker-end="url(#inputArrowhead)"/> <text x="432" y="829" font-family="Arial, sans-serif" font-size="12" class="legend-text">Player Input Data Flow</text> </g> </svg> <p>NVIDIA DLSS 4<sup id="fnref:DLSS4"><a href="#fn:DLSS4" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> represents the latest iteration in the company’s suite of AI-driven rendering technologies, introducing significant architectural advancements and new features aimed at further enhancing image quality, frame rates, and responsiveness in real-time graphics applications. This version builds upon the foundations laid by its predecessors, particularly DLSS 3.x, while incorporating novel AI models and deeper hardware integration with the NVIDIA Blackwell GPU architecture.</p> <p>A hallmark of DLSS 4 is the strategic shift in its AI model architecture for key components and the introduction of more sophisticated frame generation capabilities.</p> <ul> <li><strong>Transformer-based AI Models for Super Resolution (SR) and Ray Reconstruction (RR):</strong> DLSS 4 marks a pivotal transition from predominantly Convolutional Neural Network (CNN) based architectures, used in previous DLSS versions for Super Resolution and Ray Reconstruction, to <strong>transformer-based models</strong>. Transformers, which have demonstrated remarkable success in fields like natural language processing and offline image generation, employ <strong>attention mechanisms</strong>. These mechanisms allow the model to dynamically weigh the importance of different parts of the input data, enabling them to capture global dependencies and long-range relationships within an image or across frames more effectively than traditional CNNs, which are inherently more focused on local features. This architectural change is anticipated to lead to improved image stability, more effective reduction of ghosting artifacts, better preservation of detail in motion, and smoother edges. The new transformer model for SR is reported to involve four times the number of compute operations compared to its predecessor but has been co-designed with the enhanced Tensor Cores of the Blackwell architecture to maintain high efficiency. The adoption of transformers also suggests a path towards models that can scale more effectively with larger and more diverse training datasets, potentially leading to continuous improvements in generalization and fidelity.</li> </ul> <p><img src="https://i.imgur.com/LzTiQFF.jpeg" alt=""/></p> <ul> <li><strong>Multi Frame Generation (MFG)</strong>: DLSS 4 evolves the Frame Generation capabilities of DLSS 3.0 with a new technique termed Multi Frame Generation (MFG). While DLSS 3.0 generated one additional AI frame for every rendered frame, DLSS 4’s MFG aims to generate up to three additional frames for every one traditionally rendered frame. When combined with DLSS Super Resolution (which itself might be reconstructing a significant portion of the rendered frame’s pixels from a lower internal resolution), MFG has the potential to achieve up to an 8x increase in rendering efficiency compared to brute-force native resolution rendering. The neural architecture for MFG has been redesigned for efficiency and improved quality. It splits the neural component of DLSS 3.0’s Frame Generation into two parts: a larger, more computationally intensive part that runs once per pair of input (rendered) frames, with its output being reusable for the generation of multiple intermediate frames; and a much smaller, lighter part that runs once for every generated output frame. This split architecture is a key optimization that allows for reduced latency and improved efficiency, making the generation of multiple frames feasible within tight real-time budgets.</li> <li><strong>AI-based Optical Flow:</strong> A significant architectural change in DLSS 4’s Frame Generation pipeline is the replacement of the hardware-based Optical Flow Accelerator (OFA)—a feature of the Ada Lovelace architecture used by DLSS 3.0—with a <strong>highly efficient AI model dedicated to calculating optical flow</strong>. This new AI-driven approach to optical flow is reported to be 40% faster and use 30% less VRAM for the frame generation model compared to the previous OFA-dependent model. Furthermore, it is designed to offer improved image quality, particularly in challenging scenarios like the rendering of particle effects, due to more refined flow estimation capabilities. This AI optical flow model only needs to execute once per rendered frame to support the generation of multiple intermediate frames. This shift from a fixed-function hardware unit to a continuously improvable AI model for optical flow represents a notable trend in AI graphics.</li> </ul> <p><img src="https://i.imgur.com/tHoGMAp.jpeg" alt=""/></p> <p>These architectural innovations—the adoption of transformers, the introduction of multi-stage frame generation, and the use of AI for optical flow—collectively aim to deliver higher frame rates and superior image quality by leveraging more sophisticated AI paradigms and co-designing these algorithms with new GPU hardware capabilities.</p> <p><strong>Reflex Integration and Frame Warp</strong> Minimizing latency remains a critical concern, especially when employing frame generation techniques. NVIDIA Reflex, which optimizes the rendering pipeline to reduce system latency, continues to be an integral part of the DLSS framework. DLSS 4 introduces a new enhancement to this system: Reflex Frame Warp.</p> <p>Reflex Frame Warp is described as a late-stage reprojection technique. Its core function is to update the most recently rendered frame based on the very latest player input (e.g., mouse movement, camera adjustments) <em>immediately before</em> that frame is sent to the display. The process involves the CPU calculating the new camera position based on the latest input, and then Frame Warp samples this new position and “warps” the frame just rendered by the GPU to align with this updated perspective. This ensures that the image viewed by the player reflects their most recent actions as closely as possible, aiming to counteract the inherent latency that can be introduced by multi-stage rendering and frame generation pipelines.</p> <p>A key challenge with such warping techniques is handling <strong>disocclusions</strong>—areas of the scene that become newly visible due to the shift in camera perspective and for which no pixel data exists in the original rendered frame. Reflex Frame Warp addresses this through a combination of strategies: minimizing their occurrence by rendering a guard band around the screen border and using layered rendering, and employing <strong>predictive rendering</strong>. In predictive rendering, camera movement is extrapolated from user input, and the frame is initially rendered at this predicted future position. This predicted frame is then warped to the true, most current viewpoint before display, correcting any deviations and significantly reducing the average size of disocclusions with minimal performance impact.3 For any remaining holes created by the reprojection, Frame Warp utilizes a latency-optimized <strong>AI inpainting</strong> approach. This inpainting algorithm incorporates historical frame data, G-buffer information from the predictive rendering pass, and information about the upcoming camera position to reconstruct the missing areas, striving for visual consistency while dynamically adjusting algorithm fidelity to maximize latency savings.</p> <p>Reflex Frame Warp is thus positioned as a critical technology for maintaining a responsive gaming experience, especially when used in conjunction with Multi Frame Generation. It aims to make the significantly increased frame rates delivered by MFG feel immediate and connected to player input.</p> <p><strong>Pipeline Interface: Inputs, Processing, and Outputs</strong> The effective operation of DLSS 4 relies on a sophisticated interplay of data inputs from the game engine and graphics pipeline, specific processing injection points, and carefully constructed outputs.</p> <ul> <li><strong>Inputs:</strong> <ul> <li><strong>Super Resolution (Transformer-based):</strong> Likely requires similar inputs to previous DLSS SR versions: low-resolution current frames, motion vectors, depth information, exposure data, and a history buffer containing previous high-resolution outputs for temporal feedback. The transformer model is noted to be better at handling scenarios like animated textures, where it can intelligently decide to ignore motion vectors if they are deemed unreliable for that content.</li> <li><strong>Ray Reconstruction (Transformer-based):</strong> Takes the noisy, un-denoised output from ray tracing passes as a primary input. It also likely utilizes other G-buffer data (normals, roughness, etc.) and temporal information (motion vectors, history frames) to provide context for the AI to accurately denoise and reconstruct ray-traced effects like global illumination, reflections, and shadows.</li> <li><strong>Multi Frame Generation (AI Optical Flow):</strong> Requires two sequential rendered (and typically super-resolved) frames as a basis. Instead of OFA output, it uses an internally generated optical flow field from its AI optical flow model. It also utilizes game engine motion vectors and depth data, similar to DLSS 3.0 Frame Generation. The split architecture allows for the output of the first, larger network part (run once per input pair) to be reused for generating multiple intermediate frames.</li> <li><strong>Reflex Frame Warp:</strong> Operates on the final rendered frame from the GPU. It also takes the latest mouse/controller input from the system, the new camera position calculated by the CPU based on this input, and leverages historical frame data and G-buffers from its predictive rendering stage for inpainting.</li> </ul> </li> <li><strong>Processing Injection Points:</strong> <ul> <li><strong>Super Resolution and Ray Reconstruction:</strong> These processes likely occur after the main geometry processing and initial lighting/shading passes (which produce the low-resolution frame and noisy ray-traced data) but before final screen-space post-processing effects (like tone mapping or film grain) and UI rendering. The DLSS 4 research mentions that ray-traced shading (reflections, GI, shadows) is processed at a reduced resolution and then passed through a super resolution model.</li> <li><strong>Multi Frame Generation:</strong> This is fundamentally a post-processing step. It takes fully rendered (and super-resolved) frames as input and generates additional frames that are then inserted into the display sequence.</li> <li><strong>Reflex Frame Warp:</strong> This is designed to be a very late-stage process, occurring “just before the rendered frame is sent to the display”. This timing is critical for it to incorporate the absolute latest user input.</li> </ul> </li> <li><strong>Outputs:</strong> <ul> <li>The Super Resolution and Ray Reconstruction components output high-resolution, high-quality, and temporally stable frames with enhanced ray-traced effects.</li> <li>The Multi Frame Generation component outputs multiple AI-synthesized intermediate frames.</li> <li>The final frames presented to the display are those that have potentially been modified by Reflex Frame Warp to reflect the latest player input.</li> </ul> </li> </ul> <p>Understanding this data flow and the specific points at which DLSS 4 integrates its various processing stages is crucial for game developers seeking to implement the technology effectively and for engineers analyzing its performance characteristics and potential sources of visual artifacts. The shift to an AI-based optical flow model for MFG simplifies one aspect of the input pipeline by removing the dependency on a specific hardware OFA signal, potentially offering more flexibility.</p> <p>The replacement of a specialized hardware unit (the Optical Flow Accelerator in DLSS 3.0) with an AI-based model for optical flow calculation in DLSS 4’s Multi Frame Generation is another noteworthy trend. This indicates that AI models, when executed on sufficiently powerful and generalized AI hardware like modern Tensor Cores, are becoming capable and efficient enough to supplant fixed-function or more specialized hardware blocks for certain complex tasks. While the OFA in the Ada Lovelace architecture was a dedicated silicon block optimized for its specific purpose, its functionality was inherently fixed by its hardware design. In contrast, an AI model for optical flow, as described for DLSS 4, offers the potential for continuous improvement through new training data, architectural refinements, and algorithmic advancements, without necessitating new hardware revisions for that specific function. This approach offers greater flexibility and adaptability. It also potentially frees up die space or power that would have been consumed by a dedicated OFA, which could then be allocated to other GPU resources. This is a strong indicator of AI’s expanding capability to take over tasks that were traditionally the domain of hardware acceleration or complex, hand-tuned algorithms.</p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:DLSS1"> <p><a href="https://developer.nvidia.com/blog/dlss-what-does-it-mean-for-game-developers/">DLSS: What Does It Mean for Game Developers?</a> <a href="#fnref:DLSS1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:DLSS2"> <p><a href="https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/">NVIDIA DLSS 2.0: A Big Leap In AI Rendering</a> <a href="#fnref:DLSS2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:DLSS3"> <p><a href="https://www.nvidia.com/en-us/geforce/news/dlss3-ai-powered-neural-graphics-innovations/">NVIDIA DLSS 3: AI-Powered Performance Multiplier Boosts Frame Rates By Up To 4X</a> <a href="#fnref:DLSS3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:DLSS35"> <p><a href="https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-3-5-ray-reconstruction/">NVIDIA DLSS 3.5: Enhancing Ray Tracing With AI</a> <a href="#fnref:DLSS35" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:DLSS4"> <p><a href="https://developer.nvidia.com/blog/dlss-what-does-it-mean-for-game-developers/">NVIDIA DLSS 4 Introduces Multi Frame Generation &amp; Enhancements For All DLSS Technologies</a> <a href="#fnref:DLSS4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="all"/><category term="rendering-generative-ai"/><category term="super-resolution"/><category term="denoising"/><category term="super-sampling"/><summary type="html"><![CDATA[An overview of the latest advancements in neural rendering with DLSS 4, featuring multi-frame generation, transformer-based ray reconstruction and super-resolution, and relex frame warp.]]></summary></entry><entry><title type="html">AMD FidelityFX Super Resolution</title><link href="https://ddlee-cn.github.io/blog/2024/AMD-FSR/" rel="alternate" type="text/html" title="AMD FidelityFX Super Resolution"/><published>2024-08-21T17:39:00+00:00</published><updated>2024-08-21T17:39:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2024/AMD-FSR</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2024/AMD-FSR/"><![CDATA[<h2 id="i-introduction"><strong>I. Introduction</strong></h2> <h3 id="overview">Overview</h3> <p>AMD FidelityFX Super Resolution (FSR) encompasses a suite of open-source image upscaling technologies engineered to enhance application framerates. This is achieved by rendering scenes at a lower internal resolution and subsequently employing intelligent algorithms to upscale the image to the desired, typically native, output resolution. The primary benefit is a significant boost in performance, which can be utilized to enable more demanding graphical settings, achieve higher resolutions, or ensure smoother gameplay, particularly on hardware with limited computational resources.</p> <p>A defining characteristic of FSR is its open-source nature, distributed under the permissive MIT license, and its broad cross-platform compatibility. FSR supports a wide array of graphics processing units (GPUs), including those from competitors, and integrates with multiple graphics APIs such as DirectX 11, DirectX 12, and Vulkan. This open approach fosters wider adoption within the game development community, as a single FSR implementation can benefit a diverse user base across various hardware configurations. The strategic decision to make FSR open and cross-platform, contrasting with some proprietary hardware-locked solutions, allows developers to reach a broader audience without fragmenting their engineering efforts. This approach aims to establish FSR as a widely available upscaling standard, indirectly enhancing the value of AMD’s ecosystem by promoting a technology accessible to all.</p> <h3 id="the-evolution-of-fsr">The Evolution of FSR</h3> <p>The FSR technology suite has undergone significant evolution since its inception, with each iteration addressing limitations of its predecessor and incorporating more advanced rendering techniques.</p> <ul> <li><strong>FSR 1.0 (Released 2021)<sup id="fnref:FSR1"><a href="#fn:FSR1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</strong> This initial version introduced spatial upscaling. Its core components were an Edge-Adaptive Spatial Upsampling (EASU) algorithm and a Robust Contrast-Adaptive Sharpening (RCAS) pass. FSR 1.0 was designed for ease of integration and targeted broad hardware compatibility, operating on a single input frame without reliance on temporal data.</li> <li><strong>FSR 2.x (Released 2022)<sup id="fnref:FSR1:1"><a href="#fn:FSR1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>:</strong> Representing a fundamental shift, FSR 2.x transitioned to temporal upscaling. This version was developed from the ground up and is not an incremental update to FSR 1.0. It leverages data from previous frames, such as motion vectors and depth information, to reconstruct images with significantly higher quality and more effective anti-aliasing compared to FSR 1.0.1 Subsequent minor versions (e.g., FSR 2.1, 2.2) introduced refinements to improve image quality further, such as reducing ghosting artifacts and enhancing the stability of motion vector processing.</li> <li><strong>FSR 3.x (Released 2023<sup id="fnref:FSR3"><a href="#fn:FSR3" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, 2024<sup id="fnref:FSR31"><a href="#fn:FSR31" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>):</strong> This iteration builds upon the temporal upscaling foundation of FSR 2.x and introduces a significant new feature: frame generation. Technologies like Optical Flow and Frame Interpolation are employed to synthesize new frames, aiming to further increase the displayed frames per second (FPS) beyond what upscaling alone can achieve. FSR 3 also introduced a “Native AA” (Anti-Aliasing) mode, which applies the FSR processing pipeline at native resolution for improved anti-aliasing without upscaling.</li> </ul> <p>This evolutionary trajectory highlights a pattern of iterative problem-solving. FSR 1.0, while effective for boosting performance, exhibited limitations in image fidelity, particularly concerning the reconstruction of fine details and maintaining temporal stability, sometimes resulting in blurriness or shimmering. FSR 2.x directly targeted these shortcomings by incorporating temporal data, a well-established method for enhancing reconstruction quality by accumulating more information per output pixel over successive frames. This advancement, however, necessitated more complex input data from the game engine and a more intricate integration process. FSR 3.x addresses a different performance aspect: scenarios where even temporally upscaled rendering may not achieve the very high frame rates desired for modern high-refresh-rate displays. By introducing frame generation, FSR 3.x decouples the displayed FPS from the rendered FPS. This, however, introduces new engineering challenges, primarily related to increased input latency and the potential for artifacts in the synthetically generated frames. Each generation thus reflects a drive to overcome prior limitations by employing more sophisticated graphics techniques, leading to a richer feature set but also potentially increasing integration complexity and imposing stricter demands on the quality of input data.</p> <h2 id="ii-fundamental-computer-graphics-concepts"><strong>II. Fundamental Computer Graphics Concepts</strong></h2> <h3 id="rendering-resolution-vs-display-resolution">Rendering Resolution vs. Display Resolution</h3> <p>In the context of real-time 3D graphics, it is crucial to distinguish between rendering resolution and display resolution:</p> <ul> <li><strong>Rendering Resolution:</strong> This refers to the internal resolution at which the 3D scene geometry is rasterized, and fragments are shaded by the GPU.8 A lower rendering resolution means fewer pixels are processed for complex shading calculations, lighting, and other effects. This reduction in pixel load directly translates to decreased computational work for the GPU, typically resulting in higher frame rates.</li> <li><strong>Display Resolution (Output Resolution):</strong> This is the resolution at which the final image is presented to the user, often corresponding to the native pixel grid of the display device (e.g., 1920x1080, 2560x1440, 3840x2160).</li> </ul> <p>Upscaling technologies like FSR serve as a bridge between these two resolutions. They take an input frame rendered at a lower resolution and algorithmically reconstruct it to match a higher target display resolution.1 A common characteristic of this process is that while the 3D rendered scene might be processed at a lower fidelity, user interface (UI) elements can be rendered directly at the display resolution and composited over the upscaled scene, ensuring their sharpness and legibility.</p> <h3 id="aliasing-and-anti-aliasing-aa">Aliasing and Anti-Aliasing (AA)</h3> <p><img src="https://i.imgur.com/WXdCmeC.jpeg" alt=""/></p> <p><strong>Aliasing</strong> is a pervasive artifact in computer graphics, manifesting as jagged, “stair-step” patterns on the edges of rendered objects, particularly noticeable on diagonal or curved lines. It arises from the process of representing continuous geometric shapes on a discrete grid of pixels, effectively an undersampling of the original signal.</p> <p><strong>Anti-Aliasing (AA)</strong> techniques are employed to mitigate these aliasing artifacts. The general goal is to create smoother, more visually appealing edges by, for example, sampling the scene at a higher frequency than the final display resolution (e.g., Supersampling Anti-Aliasing - SSAA) or by employing analytical methods to blend pixel colors along detected edges (e.g., Morphological Anti-Aliasing - MLAA).</p> <p><strong>Temporal Anti-Aliasing (TAA)</strong> is a widely adopted AA technique that leverages information from previously rendered frames to improve edge smoothing and reduce temporal artifacts like shimmering or crawling of edges on moving objects. TAA achieves this by strategically jittering the sample positions within pixels across successive frames and accumulating these samples over time. This temporal accumulation effectively increases the sampling density per pixel, leading to better anti-aliasing quality, often with a more favorable performance-to-quality ratio than purely spatial methods like SSAA. FSR 2.x and subsequent versions are designed to replace a game engine’s existing TAA solution, as their temporal upscaling process inherently includes a high-quality anti-aliasing component.</p> <h3 id="motion-vectors-depth-buffers-and-other-temporal-data">Motion Vectors, Depth Buffers, and Other Temporal Data</h3> <ul> <li><strong>Motion Vectors:</strong> These are 2D vectors, typically stored in a dedicated buffer, that describe the screen-space displacement of pixels or surfaces from their position in the previous frame to their position in the current frame. They are indispensable for temporal upscalers to correctly reproject data from the history buffer onto the current frame’s geometry. Motion vectors are also fundamental for frame generation techniques, as they inform how pixels are likely to move into the synthesized intermediate frame. The accuracy, precision (e.g., 16-bit float format is often recommended over 8-bit), and coverage (ensuring all dynamic scene elements, including particles and animated textures, contribute to the motion vector buffer) are paramount for high-quality temporal processing. Engine-generated motion vectors often require careful handling of camera jitter to ensure static objects truly have zero motion.</li> <li><strong>Depth Buffer:</strong> This buffer stores per-pixel depth information, usually representing the distance of the visible surface from the camera. The depth buffer is critical for several aspects of temporal upscaling: <ul> <li><strong>Reprojection:</strong> Assisting in accurately mapping historical samples to their correct 3D locations in the current frame.</li> <li><strong>Disocclusion Detection:</strong> Identifying regions of the scene that have become newly visible (e.g., when an object moves away, revealing what was behind it) or newly occluded. This allows the upscaler to invalidate or appropriately blend historical data that is no longer relevant, thereby mitigating ghosting artifacts.</li> <li><strong>Edge and Feature Analysis:</strong> Depth discontinuities can aid in edge detection and help differentiate between distinct geometric surfaces. For optimal precision, especially with distant objects, rendering pipelines often use reverse depth (where the far plane is at z=0 and the near plane at z=1) and floating-point depth formats (e.g., R32_FLOAT).</li> </ul> </li> <li><strong>Exposure and Color Space:</strong> The color data fed into FSR, and how it’s processed internally, is sensitive to color space and exposure levels. FSR 1.0, for example, is documented to work best in a perceptual color space and should be integrated after tone mapping. FSR 2.x and FSR 3.x, when handling High Dynamic Range (HDR) content, typically require input color to be in linear RGB space. The engine’s exposure value might be required by FSR to correctly normalize or tonemap HDR data internally, or to consistently combine frames that may have been rendered with varying pre-exposure factors. If the engine applies a dynamic pre-exposure factor that changes frame-to-frame, this value must be communicated to FSR to ensure temporal consistency.</li> <li><strong>Reactivity, Transparency, and Composition Masks:</strong> These are optional but often crucial input textures that provide per-pixel guidance to the temporal upscaler on how to treat specific areas of the image. <ul> <li>A <strong>Reactivity Mask</strong> (often derived from alpha values of transparent objects) informs the upscaler to increase the influence of current frame samples over historical samples for pixels corresponding to highly reactive elements like particles, smoke, or other alpha-blended effects. This helps prevent stale historical data from “smearing” or “ghosting” through such effects.</li> <li>A <strong>Transparency and Composition Mask</strong> can be used to denote areas with other types of special rendering, such as raytraced reflections or animated textures, where standard history accumulation and protection mechanisms might need adjustment. These masks allow developers to fine-tune the upscaler’s behavior, improving quality in challenging rendering scenarios where default heuristics might fall short.</li> </ul> </li> </ul> <h2 id="iii-deep-dive-amd-fidelityfx-super-resolution-31"><strong>III. Deep Dive: AMD FidelityFX Super Resolution 3.1</strong></h2> <h3 id="overview-combining-temporal-upscaling-and-frame-generation">Overview: Combining Temporal Upscaling and Frame Generation</h3> <p>FSR 3.1 is architecturally designed as a two-stage process to maximize framerates and visual quality.</p> <ol> <li><strong>Temporal Upscaling:</strong> The first stage employs a sophisticated temporal upscaling algorithm, which builds directly upon the principles and advancements established in FSR 2.x. This component takes a lower-resolution, jittered input frame along with auxiliary data (depth, motion vectors, etc.) and reconstructs a high-resolution, anti-aliased output frame by leveraging historical information from previous frames.</li> <li><strong>Frame Generation:</strong> The second stage, a key differentiator of FSR 3.x, utilizes the upscaled frames produced by the first stage to generate and insert additional frames. This process aims to significantly increase the final displayed frame rate, enhancing perceived motion smoothness.</li> </ol> <p>It is important to note that the frame generation component can operate in conjunction with the upscaling component or, through the “Native AA” mode, it can be used with frames rendered at native resolution if only frame rate multiplication is desired without upscaling.</p> <h3 id="core-upscaling-algorithm">Core Upscaling Algorithm</h3> <p>The upscaling algorithm within FSR 3.1 inherits its core mechanics from FSR 2.0, which was a ground-up redesign focused on high-quality temporal reconstruction.</p> <ul> <li><strong>Temporal Data Utilization and History Reconstruction:</strong> The algorithm maintains a history buffer that stores color information and other relevant data from previously processed frames. This historical data is reprojected into the coordinate space of the current frame using game engine-provided motion vectors. The process relies on jittered input samples (sub-pixel offsets applied to the projection matrix each frame) to allow the accumulation of sub-pixel detail over time, effectively increasing the information density available for reconstructing each output pixel at the higher resolution. Lanczos resampling is identified as a component in this process, featuring mechanisms to mitigate ringing artifacts and dynamically adjust resampling sharpness based on local temporal luminance stability to enhance detail in stable regions.</li> <li><strong>Handling of Disocclusions and Thin Features:</strong> Robustly managing dynamic scenes requires specialized handling for areas where historical data becomes invalid or insufficient: <ul> <li><strong>Disocclusion Detection:</strong> The system identifies regions of the scene that were occluded in previous frames but are now visible in the current frame (or vice-versa). This is primarily achieved by comparing depth values from the current frame with reprojected depth values from the reconstructed previous frame.</li> <li><strong>History Rectification (Depth and Color Based):</strong> When a disocclusion is detected for a pixel, its reliance on accumulated historical color is significantly reduced or clamped to a range derived from neighboring pixels in the current frame. This prevents “ghosting” artifacts, where outdated information from occluded surfaces might persist. For smoother transitions in disoccluded areas, FSR 2.0’s design includes pushing a new, potentially more blurred, sample during the upsampling stage for these regions.</li> <li><strong>Thin Feature Locking:</strong> To preserve the stability and integrity of very fine geometric details (e.g., wires, railings, distant foliage) that might only be sparsely sampled in any single low-resolution frame, FSR 2.0 introduced a “locking” mechanism. This logic detects pixel-wide ridges and emphasizes their historical color during rectification, preventing them from flickering or breaking up over time. These locks are designed to persist for the length of the jitter sequence or until invalidated by significant scene changes like disocclusion or substantial shading modifications.</li> </ul> </li> </ul> <p><img src="https://i.imgur.com/75Sgm1s.jpeg" alt=""/></p> <p>The upscaling techniques employed in FSR 2.x and, by extension, FSR 3.1, are distinct from machine learning-based approaches. They rely on meticulously engineered, hand-coded algorithms. The detailed descriptions of mechanisms like biased Lanczos resampling, depth-based disocclusion handling with adaptive sample pushing, color-based history clamping, and the intricate logic for thin feature locking and unlocking underscore a complex array of heuristics and carefully designed solutions. This suggests a substantial investment in empirical tuning and expert graphics engineering to create algorithms capable of addressing a wide spectrum of challenging rendering scenarios, including fast motion, transparencies, and the preservation of fine details. This can offer advantages in terms of predictable performance characteristics and potentially more straightforward debugging, but may also necessitate more explicit guidance from developers (e.g., through reactivity and composition masks) to optimally handle specific edge cases or unique rendering styles.</p> <h3 id="frame-generation">Frame Generation</h3> <p>The frame generation component of FSR 3.1 is designed to further elevate the perceived frame rate by synthesizing new frames based on the temporally upscaled output. This involves two key technologies: Optical Flow and Frame Interpolation.</p> <p><img src="https://i.imgur.com/edeKtqA.jpeg" alt=""/></p> <ul> <li><strong>Optical Flow (enhanced from AMD Fluid Motion Frames - AFMF):</strong> The FSR 3 Optical Flow workload, derived and improved from AMD’s Fluid Motion Frames (AFMF) technology, plays a critical role in frame generation. Optical flow algorithms perform a dense analysis of consecutive upscaled frames to estimate per-pixel motion between them. This process generates a set of optical flow vectors, which are typically more detailed and can capture more nuanced screen-space motion (e.g., from shadows, reflections, volumetric effects, and transparencies) than the often object-based or sparser motion vectors provided by the game engine for the upscaling pass.6 The FidelityFX SDK includes a dedicated FfxOpticalFlow component for this task, which takes the current and previous (upscaled) frames as primary inputs and outputs these detailed motion vectors.</li> <li><strong>Frame Interpolation Techniques:</strong> Using the dense motion vectors generated by the Optical Flow pass, along with the two surrounding “real” (rendered and upscaled) frames, the frame interpolation stage synthesizes one or more entirely new frames to be inserted between them. The objective is to create visually plausible intermediate frames that significantly enhance the illusion of smooth motion, effectively doubling (or more, depending on the implementation) the displayed frame rate. The quality of these interpolated frames is heavily dependent on the accuracy of the optical flow field and the complexity of the on-screen motion. Fast, erratic, or unpredictable motion, as well as significant disocclusions or complex interacting transparent surfaces, can pose challenges for optical flow estimation and may lead to visual artifacts in the generated frames (such as warping, morphing, or object discontinuities). To mitigate the visibility of such artifacts, AMD recommends that the game should be running at a minimum of 60 FPS <em>before</em> frame generation is applied (i.e., after the upscaling pass), as interpolation artifacts tend to be more noticeable and disruptive at lower base framerates.</li> </ul> <p>The quality and robustness of the Optical Flow algorithm are paramount for the success of FSR 3.1’s frame generation. Frame interpolation is fundamentally an “inventive” process, relying entirely on the information from surrounding real frames and the guidance of motion vectors to construct what happened “in between.” If the optical flow accurately captures the true motion of all on-screen elements, the resulting interpolated frame is likely to be of high quality and free from significant visual disturbances. Conversely, errors or inaccuracies in the optical flow—such as incorrect motion vectors or misinterpretations of complex occluding edges—will directly translate into visual artifacts in the generated frames. This underscores the importance of the FfxOpticalFlow component 20 as a specialized, and potentially computationally intensive, cornerstone of the FSR 3.1 frame generation pipeline.</p> <h3 id="graphics-pipeline-integration">Graphics Pipeline Integration</h3> <p>Proper integration of FSR 3.1 into the graphics pipeline is crucial for achieving optimal image quality and performance. This involves careful placement of both the upscaling and frame generation stages, as well as managing dependencies with other rendering passes.</p> <p><img src="https://i.imgur.com/w0Hq9GW.jpeg" alt=""/></p> <ul> <li><strong>Upscaler Placement:</strong> <ul> <li>The FSR 3.1 upscaler, much like FSR 2.x, is designed to replace the game engine’s conventional Temporal Anti-Aliasing (TAA) pass. It should be integrated into the pipeline <em>after</em> all main scene opaque geometry has been rendered and essential G-buffer data (such as depth and motion vectors for the current frame) have been generated, but <em>before</em> any TAA would typically run</li> <li>Post-processing effects that rely on or benefit from an anti-aliased input (e.g., certain types of bloom, lens flares, depth of field that assumes clean edges) should be applied <em>after</em> the FSR upscaling pass, operating on the high-resolution upscaled output.</li> <li>Conversely, effects that require the raw, lower-resolution depth buffer or other non-anti-aliased G-buffer information (e.g., Screen Space Ambient Occlusion (SSAO), screen-space reflections if they are not re-rendered or adapted for the output resolution) should generally be executed <em>before</em> FSR upscaling.</li> </ul> </li> <li><strong>Frame Generation Placement &amp; Swapchain Management:</strong> <ul> <li>The frame generation process (Optical Flow and Frame Interpolation) occurs <em>after</em> the FSR upscaling pass has produced a high-resolution frame. It also typically happens <em>after</em> the User Interface (UI) has been rendered, or the UI is composited separately onto the final displayed frames to avoid interpolation artifacts on UI elements.</li> <li>A significant integration requirement for FSR 3.1 with frame generation is the management of the swap chain. Developers must either replace their existing swap chain or create a new one using the AMD FidelityFX API’s DX12 or Vulkan Frame Generation Swapchain interfaces. This custom swapchain is essential for correctly pacing and presenting the interleaved sequence of rendered (upscaled) frames and synthetically generated frames.</li> <li>The Optical Flow and Frame Generation workloads are computationally intensive but can often be scheduled to run on an asynchronous compute queue if the hardware supports it. This helps to hide their latency and reduce their impact on the main graphics rendering pipeline.</li> </ul> </li> </ul> <p><strong>Official Presentations</strong></p> <p><a href="https://gpuopen.com/gdc-presentations/2024/GDC2024_High_Performance_Rendering_in_Snowdrop_Using_AMD_FidelityFX_Super_Resolution_3.pdf">HIGH PERFORMANCE RENDERING IN SNOWDROP USING AMD FSR 3 - GDC 2024</a></p> <p><a href="https://gpuopen.com/gdc-presentations/2023/GDC-2023-Temporal-Upscaling.pdf">TEMPORAL UPSCALING: PAST, PRESENT, AND FUTURE - GDC 2023</a></p> <p><a href="https://gpuopen.com/gdc-presentations/2022/GDC_FidelityFX_Super_Resolution_2_0.pdf">FIDELITYFX SUPER RESOLUTION 2.0</a></p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:FSR1"> <p><a href="https://gpuopen.com/fidelityfx-superresolution/">AMD FidelityFX™ Super Resolution 1 (FSR 1) - AMD GPUOpen</a> <a href="#fnref:FSR1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:FSR1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:FSR3"> <p><a href="https://gpuopen.com/fidelityfx-super-resolution-3/">AMD FidelityFX™ Super Resolution 3 (FSR 1) - AMD GPUOpen</a> <a href="#fnref:FSR3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:FSR31"> <p><a href="https://community.amd.com/t5/gaming/amd-fsr-3-1-now-available-fsr-3-available-and-upcoming-in-60/ba-p/692000">AMD FSR 3.1 Now Available, FSR 3 Available and Upcoming in 60 Games</a> <a href="#fnref:FSR31" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="all"/><category term="rendering-generative-ai"/><category term="super-resolution"/><category term="super-sampling"/><summary type="html"><![CDATA[An overview of the latest advancements in neural rendering with AMD FSR 3.x.]]></summary></entry><entry><title type="html">The Real-Time Rendering Pipeline</title><link href="https://ddlee-cn.github.io/blog/2024/CG-Pipeline/" rel="alternate" type="text/html" title="The Real-Time Rendering Pipeline"/><published>2024-04-12T17:39:00+00:00</published><updated>2024-04-12T17:39:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2024/CG-Pipeline</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2024/CG-Pipeline/"><![CDATA[<h2 id="overview"><strong>Overview</strong></h2> <p>At its core, the rendering pipeline is a sequence of stages that progressively transform 3D scene data into a final 2D image. Modern graphics engines like Unreal Engine and Unity rely heavily on this pipeline, which is executed primarily by the <strong>Graphics Processing Unit (GPU)</strong> due to its highly parallel architecture, making it incredibly efficient at handling the vast amounts of data and computations involved in rendering.</p> <p>Here’s a conceptual overview of the entire pipeline:</p> <pre><code class="language-mermaid">graph TD
    subgraph CPU
        A[Application Stage]
    end

    subgraph GPU
        B(Geometry Processing / Vertex Shading)
        C(Rasterization)
        D(Pixel Processing / Fragment Shading)
        E(Post-Processing / Composition Stage)
    end

    F[Frame Buffer Final Image]

    A --&gt; B;
    B --&gt; C;
    C --&gt; D;
    D --&gt; E;
    E --&gt; F;
</code></pre> <p>Now, let’s break down each of these stages.</p> <h2 id="i-application-stage"><strong>I. Application Stage</strong></h2> <ul> <li><strong>Primary Goal/Purpose:</strong> This stage is where the “brains” of your application or game reside. It’s responsible for preparing all the data that the GPU needs to render a single frame. It handles game logic, physics simulations, animation updates, user input, and determines what needs to be rendered and how.</li> <li><strong>Key Operations/Tasks:</strong> <ul> <li><strong>Scene Management:</strong> Deciding which objects are visible (a process called <strong>culling</strong>, e.g., frustum culling).</li> <li><strong>Animation:</strong> Updating object positions, rotations, and scales based on animations.</li> <li><strong>Physics Simulation:</strong> Calculating object interactions and movements.</li> <li><strong>Resource Management:</strong> Loading textures, models, and other assets into memory.</li> <li><strong>Sending Rendering Commands:</strong> Preparing and sending data (like object geometry and material properties) and commands (like “draw this object”) to the GPU.</li> </ul> </li> <li><strong>Typical Inputs &amp; Outputs:</strong> <ul> <li><strong>Inputs:</strong> User input, game state, AI decisions, physics calculations, animation data.</li> <li><strong>Outputs:</strong> Rendering commands, geometric data (vertices, indices), transformation matrices (model, view, projection), texture data, shader programs, and lighting information sent to the GPU.</li> </ul> <pre><code class="language-mermaid">      graph TD
          subgraph Application_Stage_CPU
              GL[Game Logic] --&gt; AL;
              PE[Physics Engine] --&gt; AL;
              UI[User Input] --&gt; AL;
              AS[Animation System] --&gt; AL;
              AL[Application Logic] --&gt; SD[Scene Data &amp; Rendering Commands];
          end

          GP[To Geometry Processing]
          SD --&gt; GP;
</code></pre> </li> </ul> <hr/> <h2 id="ii-geometry-processing-vertex-shading"><strong>II. Geometry Processing (Vertex Shading)</strong></h2> <ul> <li><strong>Primary Goal/Purpose:</strong> To transform the 3D models from their local “model space” into the 2D screen space that the viewer will see. It operates on the individual <strong>vertices</strong> of the 3D models. <ul> <li><strong>Vertex:</strong> A point in 3D space that defines a corner of a <strong>polygon</strong>. Vertices usually carry other information like color, texture coordinates, and normals (vectors indicating the direction a surface is facing).</li> <li><strong>Polygon:</strong> A flat, multi-sided shape (typically triangles in computer graphics) that forms the surface of a 3D model.</li> </ul> </li> <li><strong>Key Operations/Tasks:</strong> <ul> <li><strong>Vertex Transformations:</strong> <ul> <li><strong>Model Transformation:</strong> Converts vertices from an object’s local coordinate system (model space) to the global coordinate system (world space).</li> <li><strong>View Transformation:</strong> Converts vertices from world space to the camera’s coordinate system (view space or eye space).</li> <li><strong>Projection Transformation:</strong> Transforms vertices from view space into a canonical view volume (clip space or normalized device coordinates - NDC), and then typically into screen space (2D coordinates). This is where perspective is applied, making objects further away appear smaller.</li> </ul> </li> <li><strong>Vertex Shading (Lighting):</strong> Basic lighting calculations can be performed per vertex. The color of a vertex is computed based on light sources, material properties, and the vertex normal.</li> <li><strong>Tessellation (Optional):</strong> Dynamically subdividing polygons into smaller ones to add more detail to surfaces.</li> <li><strong>Geometry Shaders (Optional):</strong> Can create new geometry or modify existing geometry on the fly.</li> </ul> </li> <li><strong>Typical Inputs &amp; Outputs:</strong> <ul> <li><strong>Inputs:</strong> 3D model data (vertices, indices defining polygons), transformation matrices (model, view, projection), material properties, light information.</li> <li><strong>Outputs:</strong> Transformed vertices in clip space (or screen space), per-vertex attributes like color, texture coordinates, and normals, ready for rasterization.</li> </ul> <pre><code class="language-mermaid">  graph TD
      subgraph Geometry_Processing_GPU
          direction LR
          InputVertices["Input Vertices &amp; Attributes, e.g., position, color, UVs, normals"] --&gt; VS[Vertex Shader];
          Matrices["Transformation Matrices, e.g., Model, View, Projection, Light Info"] --&gt; VS;
          VS -- Performs --&gt; VT[Vertex Transformations];
          VS -- Performs --&gt; VL[Per-Vertex Lighting];
          VS --&gt; OutputVertices["Transformed &amp; Shaded Vertices"];
      end
      OutputVertices --&gt; RAST[To Rasterization];
</code></pre> </li> </ul> <hr/> <h2 id="iii-rasterization"><strong>III. Rasterization</strong></h2> <ul> <li><strong>Primary Goal/Purpose:</strong> To determine which pixels on the screen are covered by each geometric primitive (usually triangles) coming from the Geometry Processing stage. It converts vector graphics (defined by vertices and lines) into raster images (pixels).</li> <li><strong>Key Operations/Tasks:</strong> <ul> <li><strong>Clipping:</strong> Primitives that are outside the view frustum (the visible volume of the scene) are discarded or “clipped” to the boundaries of the frustum.</li> <li><strong>Triangle Setup/Scan Conversion:</strong> Calculates the screen-space representation of triangles and determines which pixels are inside each triangle.</li> <li><strong>Perspective Correct Interpolation:</strong> Attributes calculated at the vertices (like texture coordinates, colors, and normals) are interpolated across the surface of the triangle for each pixel. This interpolation needs to be “perspective correct” to avoid visual artifacts.</li> <li><strong>Fragment Generation:</strong> For each pixel covered by a triangle, a “fragment” is generated. A fragment is a potential pixel with associated data (interpolated color, depth, texture coordinates, etc.).</li> </ul> </li> <li><strong>Typical Inputs &amp; Outputs:</strong> <ul> <li><strong>Inputs:</strong> Transformed and clipped vertices (forming primitives like triangles), per-vertex attributes.</li> <li><strong>Outputs:</strong> A set of <strong>fragments</strong> for each primitive. Each fragment corresponds to a pixel on the screen and carries interpolated data (color, depth, texture coordinates).</li> </ul> <pre><code class="language-mermaid">  graph TD
      subgraph Rasterization_GPU
          TV[Transformed Vertices from Geometry Processing] --&gt; Clip[Clipping];
          Clip --&gt; TS[Triangle Setup / Scan Conversion];
          TS -- Generates --&gt; Frags[Fragments with interpolated attributes: depth, color, UVs];
      end
      Frags --&gt; PP[To Pixel Processing];
</code></pre> </li> </ul> <hr/> <h2 id="iv-pixel-processing--fragment-shading"><strong>IV. Pixel Processing / Fragment Shading</strong></h2> <ul> <li><strong>Primary Goal/Purpose:</strong> To calculate the final color of each fragment generated by the rasterizer. This is where complex lighting, texturing, and other per-pixel effects are applied.</li> <li><strong>Key Operations/Tasks:</strong> <ul> <li><strong>Texturing:</strong> Applying <strong>textures</strong> (2D images or data) to the surfaces of objects. The interpolated texture coordinates (UVs) from the rasterizer are used to look up color values (texels) in the texture map. <ul> <li><strong>Texture:</strong> An image used to add detail to the surface of a 3D model (e.g., wood grain, brick pattern).</li> </ul> </li> <li><strong>Per-Pixel Lighting:</strong> More accurate lighting calculations are performed for each fragment, using interpolated normals, light properties, and material properties. This can include techniques like Phong shading or Blinn-Phong shading.</li> <li><strong>Shader Effects:</strong> Various special effects can be implemented here using <strong>pixel shaders</strong> (also known as fragment shaders).</li> <li><strong>Depth Test (Z-buffering):</strong> This is a crucial step for hidden surface removal. The depth value (Z-value) of the current fragment is compared to the depth value already stored in the <strong>Z-buffer</strong> (or depth buffer) for that pixel. If the current fragment is closer to the camera (has a smaller Z-value), its color and new depth value are written to the <strong>frame buffer</strong> and Z-buffer, respectively. Otherwise, the fragment is discarded. <ul> <li><strong>Z-buffer (Depth Buffer):</strong> A 2D array that stores the depth value (distance from the camera) for each pixel on the screen.</li> <li><strong>Frame Buffer:</strong> A region of memory that holds the pixel data for the image currently being displayed or being prepared for display. It typically contains a color buffer, depth buffer, and stencil buffer.</li> </ul> </li> <li><strong>Stencil Test:</strong> Allows for more complex masking operations, often used for effects like shadows or reflections.</li> <li><strong>Blending:</strong> Combining the color of the current fragment with the color already in the frame buffer, often used for transparency effects.</li> </ul> </li> <li><strong>Typical Inputs &amp; Outputs:</strong> <ul> <li><strong>Inputs:</strong> Fragments with interpolated attributes (depth, color, texture coordinates, normals), textures, material properties, light information, contents of the frame buffer (for blending) and Z-buffer.</li> <li><strong>Outputs:</strong> Final pixel colors written to the color buffer of the frame buffer, and updated depth values written to the Z-buffer.</li> </ul> <pre><code class="language-mermaid">  graph TD
      subgraph Pixel_Processing_GPU
          direction LR
          FragsIn["Fragments (interpolated attributes) from Rasterization"] --&gt; PS[Pixel Shader];
          Tex["Texture Data"] --&gt; PS;
          Mats["Material Properties"] --&gt; PS;
          Lights["Light Information"] --&gt; PS;
          PS -- Computes --&gt; Col[Computed Fragment Color &amp; Depth];
          Col --&gt; DT[Depth Test, i.e., Z-Buffering];
          FB_In["Current Frame Buffer Color &amp; Depth"] --&gt; DT;
          DT --&gt; Blend[Blending, if necessary];
          FB_In2["Current Frame Buffer Color"] --&gt; Blend;
          Blend --&gt; FinalPixel["Final Pixel Color &amp; Depth"];
      end
      FinalPixel --&gt; FB_Out[To Frame Buffer];
</code></pre> </li> </ul> <hr/> <h2 id="v-post-processing--composition-stage"><strong>V. Post-Processing / Composition Stage</strong></h2> <ul> <li><strong>Primary Goal/Purpose:</strong> To apply full-screen effects to the rendered image before it’s displayed. This stage operates on the entire rendered image (or intermediate buffers) rather than individual geometric primitives or pixels.</li> <li><strong>Key Operations/Tasks:</strong> <ul> <li><strong>Tone Mapping:</strong> Adjusting the range of color values in the rendered image to match the display capabilities of the screen (e.g., converting High Dynamic Range (HDR) images to Low Dynamic Range (LDR)).</li> <li><strong>Color Correction/Grading:</strong> Adjusting the overall color balance, brightness, and contrast of the image.</li> <li><strong>Anti-aliasing:</strong> Smoothing out jagged edges (aliasing) that can appear on an image due to the discrete nature of pixels (e.g., FXAA, MSAA, TAA).</li> <li><strong>Bloom:</strong> Creating a glow effect around bright areas of the image.</li> <li><strong>Depth of Field:</strong> Simulating the effect of a camera lens where objects outside a certain range of distances appear blurred.</li> <li><strong>Motion Blur:</strong> Blurring objects based on their motion or the camera’s motion to simulate how our eyes perceive fast movement.</li> <li><strong>Screen Space Ambient Occlusion (SSAO):</strong> Approximating ambient occlusion (how much ambient light a point on a surface receives) based on the depth buffer, adding subtle shadows and enhancing visual depth.</li> <li><strong>User Interface (UI) Overlay:</strong> Drawing UI elements (like health bars, menus) on top of the rendered scene.</li> </ul> </li> <li><strong>Typical Inputs &amp; Outputs:</strong> <ul> <li><strong>Inputs:</strong> The rendered 2D image from the frame buffer (color buffer), potentially the depth buffer, and other intermediate buffers.</li> <li><strong>Outputs:</strong> The final 2D image that will be sent to the display device.</li> </ul> <pre><code class="language-mermaid">  graph TD
      subgraph Post_Processing_GPU
          direction LR
          RenderedImage["Rendered Image from Frame Buffer, i.e., Color Buffer"] --&gt; PPE1[Effect 1, e.g., Tone Mapping];
          DepthBuffer["Depth Buffer, Other G-Buffers"] --&gt; PPE1;
          PPE1 --&gt; PPE2[Effect 2, e.g., Anti-aliasing];
          PPE2 --&gt; PPEn[Effect N, e.g., Bloom, UI Overlay];
          PPEn --&gt; FinalImage[Final Image for Display];
      end
</code></pre> </li> </ul> <hr/> <h2 id="summary"><strong>Summary</strong></h2> <ol> <li>The <strong>Application Stage</strong> (CPU) first decides what needs to be drawn and gathers all the necessary data (models, textures, camera position, lights).</li> <li>This data is sent to the GPU, where <strong>Geometry Processing</strong> transforms all the 3D vertex data into the correct screen positions and performs initial lighting.</li> <li><strong>Rasterization</strong> then figures out which screen pixels are covered by these transformed 3D shapes (triangles) and generates “fragments” for each covered pixel, interpolating data like color and texture coordinates across the triangle’s surface.</li> <li><strong>Pixel Processing</strong> takes these fragments and calculates their final color. This is where detailed texturing, complex lighting, and shadows are applied. The crucial depth test ensures that objects closer to the viewer correctly obscure those further away. The results are written to the frame buffer.</li> <li>Finally, the <strong>Post-Processing/Composition Stage</strong> takes the complete rendered image from the frame buffer and applies full-screen effects like bloom, color correction, or anti-aliasing to enhance the visual quality before sending it to your monitor.</li> </ol>]]></content><author><name></name></author><category term="all"/><category term="rendering"/><category term="computer"/><category term="graphics"/><category term="GPU"/><summary type="html"><![CDATA[An introduction to the fundamental stages of the real-time rendering pipeline, highlighting how graphics engines like Unreal Engine and Unity utilize GPUs for efficient scene visualization.]]></summary></entry><entry><title type="html">The Evolution of Google Camera</title><link href="https://ddlee-cn.github.io/blog/2022/Google-Camera/" rel="alternate" type="text/html" title="The Evolution of Google Camera"/><published>2022-12-24T00:00:00+00:00</published><updated>2022-12-24T00:00:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2022/Google-Camera</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2022/Google-Camera/"><![CDATA[<h2 id="i-introduction"><strong>I. Introduction</strong></h2> <p>Google’s approach to mobile photography has been consistently characterized by a software-first philosophy, leveraging computational photography to transcend the physical limitations of small smartphone sensors and optics. The Google Camera application, particularly on Pixel devices, stands as a testament to this philosophy, repeatedly demonstrating that algorithmic innovation can drive significant advancements in image quality and user-facing features.</p> <h2 id="ii-hdr-and-live-hdr"><strong>II. HDR+ and Live HDR+</strong></h2> <h3 id="hdr-burst-photography">HDR+: Burst Photography</h3> <p>The foundation of Google Camera’s image quality prowess was established with High Dynamic Range Plus (HDR+)<sup id="fnref:HDR"><a href="#fn:HDR" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. Introduced initially on Nexus devices and significantly enhanced on Pixel phones, HDR+ tackled the fundamental challenge of capturing scenes with a wide disparity between the darkest shadows and brightest highlights – a common scenario where small mobile sensors typically struggle.</p> <p><img src="https://i.imgur.com/XEiRljo.jpeg" alt=""/></p> <p>The core algorithmic principle of HDR+ is <strong>burst photography</strong><sup id="fnref:HDRPaper"><a href="#fn:HDRPaper" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Instead of a single exposure, the camera captures a rapid sequence of deliberately underexposed frames. Underexposing protects highlight detail that would otherwise be clipped. These short-exposure frames also minimize motion blur. The captured burst, typically consisting of 2 to 15 raw images depending on conditions, then undergoes a sophisticated alignment and merging process. Alignment algorithms compensate for minor hand shake and subtle scene movements between frames. The aligned frames are then merged into an intermediate, high bit-depth computational raw image. This merging process effectively averages out noise, particularly read noise and shot noise, which are significant in underexposed shots from small sensors. The result is an image with significantly reduced noise and increased dynamic range compared to any single frame. Finally, advanced tone mapping algorithms are applied to render the high dynamic range data into a visually pleasing image that preserves detail in both shadows and highlights for standard displays.</p> <p><img src="https://i.imgur.com/ikoNItM.jpeg" alt=""/></p> <p><img src="https://i.imgur.com/uLCF7m5.jpeg" alt=""/></p> <p>As detailed in the release of the HDR+ Burst Photography Dataset<sup id="fnref:HDRDataset"><a href="#fn:HDRDataset" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, key improvements included transitioning to processing <strong>raw images</strong> directly from the sensor, which provided more data for the pipeline and improved image quality. Another crucial advancement was the elimination of <strong>shutter lag</strong>, ensuring the captured photo corresponded to the exact moment the shutter button was pressed. This was often achieved by utilizing frames already buffered by the camera system (Zero Shutter Lag - ZSL). Processing times and power consumption were also optimized through implementation on specialized hardware accelerators like the Qualcomm Hexagon DSP and, later, Google’s custom-designed Pixel Visual Core<sup id="fnref:VisualCore"><a href="#fn:VisualCore" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>.</p> <p><img src="https://i.imgur.com/mdtRThi.jpeg" alt=""/></p> <p>The introduction of <strong>HDR+ with Bracketing</strong><sup id="fnref:HDRBracket"><a href="#fn:HDRBracket" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> addressed a limitation of the original HDR+ system: noisy shadows in very high dynamic range scenes due to all frames being underexposed.1 HDR+ with Bracketing strategically incorporates one or more longer exposure frames into the burst. For the default camera mode, this typically means capturing an additional long exposure frame <em>after</em> the shutter press (since ZSL uses pre-shutter frames). In Night Sight, multiple long exposures can be captured. The merging algorithm was updated to handle these bracketed exposures, choosing a short frame as the reference to avoid motion blur and clipped highlights from the long exposure. A sophisticated spatial merge algorithm, similar to that used in Super Res Zoom, performs deghosting to prevent artifacts from scene motion between frames of different exposures—a non-trivial task given differing noise characteristics and motion blur. This evolution resulted in improved shadow detail, more natural colors, better texture, and reduced noise, with the merging process also becoming 40% faster. Users of computational RAW also benefited from these enhancements, as the merging happens early in the pipeline using RAW data.</p> <p><img src="https://i.imgur.com/abxgoqR.jpeg" alt=""/></p> <h3 id="live-hdr-from-bilateral-filtering-to-hdrnet">Live HDR+: From Bilateral Filtering to HDRNet</h3> <p><img src="https://i.imgur.com/mDKmCqS.jpeg" alt=""/></p> <p>The Live HDR+<sup id="fnref:LiveHDR"><a href="#fn:LiveHDR" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> provides a <strong>real-time preview</strong> of the final result of HDR+, making HDR imaging more predictable. Thus, it is a fast approximation for multi-frame and bracket exposure process of HDR+ process, producing a HDR+ look at real-time for preview. They divide the input image into “tiles” of size roughly equal to the red patch in the figure below, and approximate HDR+ using a curve for each tile. Since these curves vary gradually, blending between curves is a good way to approximate the optimal curve at any pixel. The behind algorithm is the famous HDRNet<sup id="fnref:HDRNet"><a href="#fn:HDRNet" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>, which originates from the seminal <strong>bilateral filter</strong>.</p> <p><img src="https://i.imgur.com/AadSqTr.jpeg" alt=""/></p> <p>A bilateral filter is an image processing technique that smooths images while preserving sharp edges. Unlike traditional blurring filters that can soften important details, the bilateral filter selectively averages pixels, effectively reducing noise in flat areas without affecting the clarity of boundaries. For each pixel, it considers two factors:</p> <ul> <li>Spatial Distance: How far away the neighboring pixels are.</li> <li>Intensity Difference: How different the brightness or color of the neighboring pixels is.</li> </ul> <p>A neighboring pixel is only given a high weight in the average if it is both physically close and has a similar color. This prevents pixels on one side of an edge from being averaged with pixels on the other side, thus keeping the edge sharp.</p> <p>A naive implementation of the bilateral filter calculates the value of each output pixel by iterating over all of its neighbors within a specified spatial radius. For each neighbor, it computes a weight based on both spatial distance and intensity difference, then calculates a weighted average. This process is repeated for every pixel in the image, leading to a high computational complexity, especially with large filter kernels (i.e., a large spatial sigma).</p> <p><img src="https://i.imgur.com/2bZFkv9.jpeg" alt=""/></p> <p>The <strong>bilateral grid</strong><sup id="fnref:BiGrid2"><a href="#fn:BiGrid2" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> accelerates this process by performing the filtering in a much smaller, lower-dimensional space. Instead of operating directly on the 2D image, it uses a 3D data structure that represents the image’s two spatial dimensions (x, y) and its range dimension (intensity or color).</p> <p>The first step is to “splat” the information from the original, full-resolution image onto the smaller, downsampled bilateral grid. This is analogous to creating a 3D histogram. For each pixel in the input image, its value is distributed among the nearest vertices in the 3D grid.</p> <p><img src="https://i.imgur.com/NkdvU8g.jpeg" alt=""/></p> <p>The second step is to apply a fast, simple 3D Gaussian blur directly on the bilateral grid. This is where the edge-preserving smoothing happens. In the grid, pixels that were close in both space and intensity in the original image are now close to each other in the 3D grid. The blur, therefore, averages their values together. Conversely, pixels that were on opposite sides of an edge (spatially close but with a large intensity difference) are far apart along the grid’s intensity dimension and are not blurred together.</p> <p><img src="https://i.imgur.com/hZq2Qw3.jpeg" alt=""/></p> <p>The final step is to “slice” the blurred grid to produce the final, filtered output image. This step uses the original image as a “guidance map” to read the smoothed values back from the grid.</p> <p>For each pixel in the original image at coordinates (x, y) with intensity z, we find its corresponding position in the now-blurred 3D grid. Since the original pixel’s coordinates and intensity will not align perfectly with the grid’s discrete vertices, we perform a tri-linear interpolation between the surrounding blurred grid cells. This interpolation retrieves the final, smoothed pixel value.</p> <p>The act of looking up and interpolating a value from the grid for every pixel of the input image is what is referred to as “slicing.”<sup id="fnref:BiGrid"><a href="#fn:BiGrid" class="footnote" rel="footnote" role="doc-noteref">9</a></sup> It effectively creates a 2D “slice” of the 3D grid to form the output image. The final value is obtained by dividing the interpolated intensity sum by the interpolated weight sum from the grid.</p> <p>The other origin concept of HDRNet comes from <strong>Joint Bilateral Upsampling</strong>, a combination of principles of both bilateral filtering and guided filtering.</p> <p><img src="https://i.imgur.com/EcFttaO.jpeg" alt=""/></p> <p>Instead of using a non-linear weighting scheme, the <strong>guided filter</strong><sup id="fnref:Guided"><a href="#fn:Guided" class="footnote" rel="footnote" role="doc-noteref">10</a></sup> is based on a local linear model. It assumes that the filtered output image can be expressed as a linear transformation of a guidance image within any local window. This guidance image can be the input image itself or a different image</p> <p><img src="https://i.imgur.com/nIwPAVB.jpeg" alt=""/></p> <p>The primary purpose of Joint Bilateral Upsampling<sup id="fnref:BiGuided"><a href="#fn:BiGuided" class="footnote" rel="footnote" role="doc-noteref">11</a></sup> is to upsample a low-resolution image using a corresponding high-resolution image as a guide. It adapts the bilateral filter by decoupling the two kernels. When filtering the low-resolution input image:</p> <ul> <li>The Spatial Weight is calculated from the pixel coordinates in the high-resolution grid.</li> <li>The Range (Intensity) Weight is calculated using the pixel intensity values from the high-resolution guidance image.</li> </ul> <p><img src="https://i.imgur.com/7F2cDx7.jpeg" alt=""/></p> <p>HDRNet’s architecture is fundamentally a two-stream design that mirrors the bilateral grid’s logic:</p> <ul> <li><strong>A Low-Resolution “Processing” Path (The Grid)</strong>: The input image is first downsampled significantly. This low-resolution preview is fed into a deep but lightweight CNN. This network does the heavy lifting, analyzing the image content and learning the desired enhancement (e.g., tone mapping, color correction, etc.). The output of this network is not an image, but a small 3D grid of affine transformation matrices (e.g., 16x16x8). Each 3x4 matrix in this grid represents the ideal color transformation for a specific spatial location and intensity level. This low-resolution grid of learned transformations is the bilateral grid. It’s where the expensive computation happens efficiently.</li> <li><strong>A Full-Resolution “Guidance” Path (The Input Image)</strong>: The original, full-resolution input image is kept aside and used as the “guidance map.” It provides the crucial high-frequency edge information that must be preserved.</li> </ul> <p><img src="https://i.imgur.com/nZi4kXP.jpeg" alt=""/></p> <p>The magic of HDRNet lies in its custom “slicing” layer, which is a direct implementation of the joint bilateral upsampling principle. This layer’s job is to apply the learned, low-resolution transformations to the full-resolution image without introducing artifacts like halos or blurred edges.</p> <ul> <li>Lookup: For every pixel in the full-resolution input image, the slicing layer performs a lookup into the low-resolution grid of affine matrices.</li> <li>Guidance: The lookup coordinates are determined by the pixel’s properties: <ul> <li>Its spatial (x, y) position determines where to look in the grid’s spatial dimensions.</li> <li>Its intensity (brightness) value determines where to look along the grid’s depth (intensity) dimension.</li> </ul> </li> <li>Interpolation (Upsampling): Since the pixel’s coordinates and intensity won’t perfectly align with the grid’s discrete points, the slicing layer performs a trilinear interpolation between the neighboring affine transformation matrices in the grid. This step effectively “upsamples” the learned transformations, creating a unique, custom affine matrix for every single pixel in the high-resolution image.</li> <li>Application: The newly interpolated, full-resolution affine matrix is then applied to the original pixel’s color value to produce the final, enhanced output pixel.</li> </ul> <p><img src="https://i.imgur.com/aRXWy1F.jpeg" alt=""/></p> <p>In essence, HDRNet’s brilliance is in this combination based on a <strong>10+ year</strong> efforts by the academic community:</p> <ul> <li>It leverages the bilateral grid as a computational framework to perform complex, expensive learning tasks in a small, low-resolution space, which is the key to its real-time speed.</li> <li>It replaces the grid’s simple blur with a powerful CNN that can learn any stylistic enhancement from data.</li> <li>It uses the principle of joint bilateral upsampling in its “slicing” layer to apply these learned, low-resolution enhancements back to the high-resolution image. The original image guides this upsampling process, ensuring that the final result has sharp, clean edges, perfectly preserving the structural integrity of the original while applying a sophisticated new look.</li> </ul> <h2 id="iii-low-light-night-sight-and-astrophotography"><strong>III. Low Light: Night Sight and Astrophotography</strong></h2> <p>Building upon the multi-frame merging principles of HDR+, Google Camera introduced Night Sight<sup id="fnref:NightSight"><a href="#fn:NightSight" class="footnote" rel="footnote" role="doc-noteref">12</a></sup>, revolutionizing low-light mobile photography without requiring flash or a tripod. Night Sight aimed to solve the inherent challenges of low-light imaging: insufficient photons leading to noise, and long exposures leading to motion blur.</p> <h3 id="night-sight">Night Sight</h3> <p><img src="https://i.imgur.com/L1pWIpq.jpeg" alt=""/></p> <p>The core of Night Sight involves capturing significantly more light than a standard shot by using longer effective exposure times, achieved by merging a burst of frames.</p> <p><img src="https://i.imgur.com/FpLvFtY.jpeg" alt=""/></p> <p>Key algorithmic components are:</p> <ul> <li> <p><strong>Motion Metering and Adaptive Exposure Strategy:</strong> Before capture, Night Sight measures natural hand shake and scene motion with the combination of motion estimation based on adjacement frames and angular rate measurements from the gyroscope<sup id="fnref:NightSightPaper"><a href="#fn:NightSightPaper" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>. If the phone is stable and the scene is still, it uses fewer, longer exposures (up to 1 second per frame if on a tripod, or up to 333ms handheld with minimal motion). If motion is detected, it uses more, shorter exposures (e.g., 15 frames of 1/15s or less) to minimize motion blur. This adaptive strategy is crucial for balancing noise reduction (favoring longer exposures) and sharpness (favoring shorter exposures). <img src="https://i.imgur.com/Hq7vZvc.jpeg" alt=""/></p> </li> <li> <p><strong>Multi-Frame Merging and Denoising:</strong> The captured burst of dark but sharp frames is carefully aligned and merged. On Pixel 1 and 2, this utilized a modified HDR+ merging algorithm, retuned for very noisy scenes. Pixel 3 leveraged the Super Res Zoom merging algorithm, which also excels at noise reduction through averaging. This process significantly improves the signal-to-noise ratio (SNR).<br/> <img src="https://i.imgur.com/i3tzKMP.jpeg" alt=""/></p> </li> <li> <p><strong>Learning-Based Auto White Balance (AWB):</strong> Traditional AWB often fails in very low light. Night Sight introduced a learning-based AWB algorithm, based on FCCC<sup id="fnref:FCCC"><a href="#fn:FCCC" class="footnote" rel="footnote" role="doc-noteref">14</a></sup>, trained to recognize and correct color casts, ensuring natural color rendition even in challenging mixed lighting. This model was trained by manually correcting the white balance of numerous low-light scenes, with a newly introduced error metric for more accurate and balanced target.</p> </li> </ul> <h3 id="astrophotography">Astrophotography</h3> <p><img src="https://i.imgur.com/6sfg2VF.jpeg" alt=""/></p> <ul> <li><strong>Extended Multi-Frame Exposures:</strong> To capture enough light from faint celestial objects, total exposure is split into a sequence of frames, each with an exposure time short enough (e.g., up to 16 seconds per frame) to render stars as points rather than trails caused by Earth’s rotation.</li> <li><strong>Advanced Noise Reduction:</strong> <ul> <li><strong>Dark Current and Hot Pixel Correction:</strong> Long exposures exacerbate sensor artifacts like dark current (spurious signal even with no light) and hot/warm pixels (pixels that incorrectly report high values). These are identified by comparing neighboring pixel values within a frame and across the sequence, and outliers are concealed by averaging neighbors<sup id="fnref:Astrophotography"><a href="#fn:Astrophotography" class="footnote" rel="footnote" role="doc-noteref">15</a></sup>.</li> <li><strong>Sky-Specific Denoising:</strong> The algorithm recognizes that noise characteristics can differ between the sky and foreground.</li> </ul> </li> <li><strong>Sky Segmentation and Optimization:</strong> An on-device Convolutional Neural Network (CNN), trained on over 100,000 manually labeled images, identifies sky regions in the photograph. This allows for selective processing, such as targeted contrast enhancement<sup id="fnref:Sky"><a href="#fn:Sky" class="footnote" rel="footnote" role="doc-noteref">16</a></sup> or darkening of the sky to counteract the tendency of low-light amplification to make the night sky appear unnaturally bright. This segmentation is crucial for realistic rendering.</li> </ul> <h2 id="iv-better-detail-super-res-zoom">IV. Better Detail: Super Res Zoom</h2> <p>Smartphones traditionally struggled with zoom, as physical space constraints limit the inclusion of complex optical zoom lens systems found in DSLR cameras. Digital zoom, which typically involves cropping and upscaling a single image, results in significant loss of detail and often introduces artifacts. Google addressed this with Super Res Zoom<sup id="fnref:SuperRes"><a href="#fn:SuperRes" class="footnote" rel="footnote" role="doc-noteref">17</a></sup>, a computational approach to achieve optical-like zoom quality without traditional optical zoom hardware (for modest zoom factors).</p> <p><img src="https://i.imgur.com/iA0yZbQ.jpeg" alt=""/></p> <p>The core algorithmic principle of Super Res Zoom is <strong>multi-frame super-resolution</strong><sup id="fnref:SuperRes:1"><a href="#fn:SuperRes" class="footnote" rel="footnote" role="doc-noteref">17</a></sup>. Instead of relying on a single frame, it leverages the burst of frames captured by HDR+. The key insight is that natural hand tremor, even when imperceptible, causes slight shifts in the camera’s viewpoint between successive frames in a burst. When combined with Optical Image Stabilization (OIS) that can actively introduce tiny, controlled sub-pixel shifts, each frame captures a slightly different sampling of the scene. By aligning these multiple, slightly offset low-resolution frames and merging them onto a higher-resolution grid, Super Res Zoom can reconstruct details that would be lost in any single frame.</p> <p><img src="https://i.imgur.com/8oGyqVz.jpeg" alt=""/></p> <p>This multi-frame approach is fundamentally different from single-frame upscaling techniques like RAISR (Rapid and Accurate Image Super-Resolution)<sup id="fnref:RAISR"><a href="#fn:RAISR" class="footnote" rel="footnote" role="doc-noteref">18</a></sup>, which Google also developed and uses for enhancing visual quality. While RAISR can improve the appearance of an already captured image, the primary resolution gain in Super Res Zoom (especially for modest zoom factors like 2-3x) comes from the multi-frame merging process itself.</p> <p><img src="https://i.imgur.com/TIqaiJr.jpeg" alt=""/></p> <p>Implementing multi-frame super-resolution on a handheld device presents significant challenges:</p> <ul> <li><strong>Noise:</strong> Single frames in a burst, especially if underexposed for HDR+, can be noisy. The algorithm must be robust to this noise and aim to produce a less noisy, higher-resolution result</li> <li><strong>Complex Scene Motion:</strong> Objects in the scene (leaves, water, people) can move independently of camera motion. Reliable alignment and merging in the presence of such complex, sometimes non-rigid, motion is difficult. The algorithm needs to work even with imperfect motion estimation and incorporate deghosting mechanisms.</li> <li><strong>Irregular Data Spread:</strong> Due to random hand motion and scene motion, the sampling of the high-resolution grid can be irregular – dense in some areas, sparse in others. This makes the interpolation problem complex.</li> </ul> <p><img src="https://i.imgur.com/0THZIPp.jpeg" alt=""/></p> <p>Super Res Zoom addresses these by integrating sophisticated alignment and merging algorithms that are aware of noise and can handle local misalignments to prevent ghosting, similar to the advanced merging techniques developed for HDR+ with Bracketing. The system intelligently selects and weights information from different frames to reconstruct the final zoomed image.</p> <p><img src="https://i.imgur.com/SA8u8cX.jpeg" alt=""/></p> <p>Like Live HDR+, the merging and interpolation algorithm of Super Res Zoom comes from a <strong>10+ year</strong> idea of kernel regression for image processing<sup id="fnref:SKR"><a href="#fn:SKR" class="footnote" rel="footnote" role="doc-noteref">19</a></sup>. Its primary contribution is to connect the statistical method of kernel regression to various image processing tasks. Kernel regression is a non-parametric technique used to estimate a function or value at a specific point by calculating a weighted average of its neighbors. The core idea is simple: closer points should have more influence. This influence is defined by a “kernel,” which is a weighting function that decreases with distance.</p> <p><img src="https://i.imgur.com/LcOspTY.jpeg" alt=""/></p> <p>The framework’s power is significantly enhanced by the use of adaptive kernels. Instead of using a fixed, symmetric kernel (like a simple circle or square), the method analyzes the local image structure to “steer” the kernel. This means the kernel’s shape, orientation, and size are adapted on-the-fly:</p> <ul> <li>Near an edge, the kernel elongates and orients itself to lie parallel to the edge, thereby averaging pixels along the edge but not across it.</li> <li>In flat, textureless regions, the kernel remains more uniform and circular.</li> </ul> <p>This data-adaptive approach allows for superior preservation of sharp details and textures compared to methods using fixed kernels.</p> <p>In my works, <a href="/publications/#learning steerable resampling">LeRF</a> and <a href="/publications/#lerf:">LeRF++</a>, we push this direction a step foward by introducing a <strong>learning-based parametric CNN</strong> for the prediction of kernel shapes, which is further accelerated by <a href="/streaming-&amp;-display/"><strong>LUTs</strong></a> to achieve adaptive and efficient interpolation.</p> <h2 id="v-depth-portraits-and-semantic-understanding"><strong>V. Depth, Portraits, and Semantic Understanding</strong></h2> <p>Google Camera’s Portrait Mode, which simulates the shallow depth-of-field effect (bokeh) typically associated with DSLR cameras using wide-aperture lenses, has undergone significant algorithmic evolution, heavily relying on advancements in depth estimation and machine learning for semantic understanding.</p> <h3 id="depth-estimation-segmentation-and-portrait-mode">Depth Estimation, Segmentation, and Portrait Mode</h3> <p><img src="https://i.imgur.com/C4Yfzp2.jpeg" alt=""/></p> <ul> <li><strong>Pixel 2 (2017): Single Camera Depth via Dual-Pixel Auto-Focus (PDAF) and ML Segmentation:</strong> The Pixel 2, despite having a single rear camera, introduced Portrait Mode by ingeniously using its dual-pixel auto-focus (PDAF) sensor. Each pixel on a PDAF sensor is split into two photodiodes, capturing two slightly different perspectives of the scene (a very short baseline stereo pair). The parallax (apparent shift) between these two sub-pixel views can be used to compute a depth map. This depth map, combined with a machine learning model trained to segment people from the background, allowed for the initial bokeh effect. For the front-facing camera, which initially lacked PDAF-based depth, segmentation was achieved purely through an ML model<sup id="fnref:Portrait"><a href="#fn:Portrait" class="footnote" rel="footnote" role="doc-noteref">20</a></sup>.</li> <li><strong>Pixel 3 (2018): ML-Enhanced Depth from PDAF:</strong> The Pixel 3 improved upon this by using machine learning to directly predict depth from the PDAF pixel data<sup id="fnref:Portrait2"><a href="#fn:Portrait2" class="footnote" rel="footnote" role="doc-noteref">21</a></sup>. Instead of a traditional stereo algorithm, a Convolutional Neural Network (CNN) trained in TensorFlow took the two PDAF views as input and learned to predict a higher quality depth map<sup id="fnref:SynDepth"><a href="#fn:SynDepth" class="footnote" rel="footnote" role="doc-noteref">22</a></sup>. This ML approach was better at handling errors common with traditional stereo, such as those around repeating patterns or textureless surfaces, and could leverage semantic cues (e.g., recognizing a face to infer its distance).</li> </ul> <h3 id="more-applications-alpha-matting-relighting">More applications: Alpha Matting, Relighting</h3> <p><img src="https://i.imgur.com/Q2w9LuP.jpeg" alt=""/></p> <p>While depth maps are crucial for determining the <em>amount</em> of blur, accurately separating the subject from the background, especially around fine details like hair, requires more than just depth. This is where semantic segmentation and alpha matting come into play.</p> <ul> <li>Early Semantic Segmentation: From its inception, Portrait Mode used ML-based semantic segmentation to identify people in the scene, creating a mask to distinguish foreground from background. This mask was then refined by the depth map.</li> <li>Pixel 6 (2021): High-Resolution ML Alpha Matting<sup id="fnref:Matting"><a href="#fn:Matting" class="footnote" rel="footnote" role="doc-noteref">23</a></sup>: A major leap in subject separation for selfies came with the Pixel 6, which introduced a new ML-based approach for Portrait Matting to estimate a high-resolution and accurate alpha matte. An alpha matte specifies the opacity of each pixel, allowing for very fine-grained foreground-background compositing.<br/> The Portrait Matting model is a fully convolutional neural network with a MobileNetV3 backbone and encoder-decoder blocks. It takes the color image and an initial coarse alpha matte (from a low-resolution person segmenter) as input. It first predicts a refined low-resolution matte, then a shallow encoder-decoder refines this to a high-resolution matte, focusing on structural features and fine details like individual hair strands. This model was trained using a sophisticated dataset: <ol> <li><strong>Light Stage Data:</strong> High-quality ground truth alpha mattes were generated using Google’s Light Stage, a volumetric capture system with 331 LED lights and high-resolution cameras/depth sensors. This allowed for “ratio matting” (silhouetting against an illuminated background) to get precise mattes. These subjects were then relit and composited onto various backgrounds.</li> <li><strong>In-the-Wild Portraits:</strong> To improve generalization, pseudo-ground truth mattes were generated for a large dataset of in-the-wild Pixel photos using an ensemble of existing matting models and test-time augmentation. This high-quality alpha matte allows for much more accurate bokeh rendering around complex boundaries like hair, significantly reducing artifacts where the background might have remained sharp or the foreground was incorrectly blurred.</li> </ol> </li> </ul> <p>Further, they design a novel system<sup id="fnref:Relight"><a href="#fn:Relight" class="footnote" rel="footnote" role="doc-noteref">24</a></sup> for portrait relighting and background replacement, which maintains high-frequency boundary details and accurately synthesizes the subject’s appearance as lit by novel illumination, thereby producing realistic composite images for any desired scene. The key componenets include foreground estimation via alpha matting, relighting, and compositing.</p> <p><img src="https://i.imgur.com/GFENBBl.jpeg" alt=""/></p> <p>The relighting module is divided into three sequential steps. A first Geometry Network estimates per-pixel surface normals from the input foreground. The surface normals and foreground 𝐹 are used to generate the albedo. The target HDR lighting environment is prefiltered using diffuse and specular convolution operations, and then these prefiltered maps are sampled using surface normals or reflection vectors, producing a per-pixel representation of diffuse and specular reflectance for the target illumination (light maps). Finally, a Shading Network produces the final relit foreground.</p> <p><img src="https://i.imgur.com/fbP6oqQ.jpeg" alt=""/></p> <h2 id="vi-other-topics"><strong>VI. Other Topics</strong></h2> <h3 id="video-stabilization">Video Stabilization</h3> <p><img src="https://i.imgur.com/4ZoCoCj.jpeg" alt=""/></p> <p>Fused Video Stabilization is a hybrid approach that combines Optical Image Stabilization (OIS) with Electronic Image Stabilization (EIS) to produce smooth, professional-looking videos.</p> <p>The core idea is to address common problems like camera shake and motion blur. The system uses the phone’s gyroscope and accelerometer to precisely measure and predict the user’s intended motion. The OIS hardware compensates for small, high-frequency jitters, while the EIS software handles larger motions and corrects for other distortions like rolling shutter (“jello” effect). By intelligently fusing these two methods, the technology delivers exceptionally stable video that mimics the look of footage shot with professional camera equipment.</p> <h3 id="denoise-and-deblur">Denoise and Deblur</h3> <p>The denosing takes advantage of self-similarity of patches across the image to denoise with high fidelity. The general principle behind the seminal “non-local” denoising is that noisy pixels can be denoised by averaging pixels with similar local structure. However, these approaches typically incur high computational costs because they require a brute force search for pixels with similar local structure, making them impractical for on-device use. In the “pull-push” approach, the algorithmic complexity is decoupled from the size of filter footprints thanks to effective information propagation across spatial scales.</p> <p>Instead of tackling severe motion blur, the deblur<sup id="fnref:Deblur"><a href="#fn:Deblur" class="footnote" rel="footnote" role="doc-noteref">25</a></sup> function focuses on “mild” blur—the subtle loss of sharpness caused by minor camera shake, small focus errors, or lens optics. The proposed method, Polyblur<sup id="fnref:Polyblur"><a href="#fn:Polyblur" class="footnote" rel="footnote" role="doc-noteref">26</a></sup>, is a two-stage process designed to be fast enough to run in a fraction of a second on mobile devices.</p> <p><img src="https://i.imgur.com/he7qCci.jpeg" alt=""/></p> <h2 id="vii-final-remarks"><strong>VII. Final Remarks</strong></h2> <p>The Google Camera team, led by Prof. Marc Levoy<sup id="fnref:Marc"><a href="#fn:Marc" class="footnote" rel="footnote" role="doc-noteref">27</a></sup> and Prof. Peyman Milanfar<sup id="fnref:Peymann"><a href="#fn:Peymann" class="footnote" rel="footnote" role="doc-noteref">28</a></sup> later, contributed a lot to the advancement in both academia and massive application in industry of computational photography technology, and had a deep influence to myself in terms of both research taste and ideas. Personally, I want to appreciate their openness.</p> <p>Look back to the evolution of Google Camera, a clear trend is the <strong>iterative enhancement of core techniques</strong>, including HDR+, Night Sight, and Super Res Zoom. Another key is takeaway the synergy between <strong>software, hardware, and data-driven approach</strong>.</p> <p><img src="https://i.imgur.com/o3S4PcU.jpeg" alt=""/></p> <p>The ultimate goal of computatiobal photography goes beyond match the human vision in terms of spatial, temporal, sectral resolution. From my point of view, future trends include intergration of other sensor modality, e.g., <a href="/publications/#multi-spectral">multi-spectral</a>, take advantage of on-device generative AI for creative post-processing, e.g., applying personalized photographic style, and adaptation to novel capture devices, e.g., AR Glasses &amp; Wearables.</p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:HDR"> <p><a href="https://research.google/blog/hdr-low-light-and-high-dynamic-range-photography-in-the-google-camera-app/">HDR+: Low Light and High Dynamic Range photography in the Google Camera App</a> <a href="#fnref:HDR" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:HDRPaper"> <p><a href="https://dl.acm.org/doi/10.1145/2980179.2980254">Burst photography for high dynamic range and low-light imaging on mobile cameras</a>, in SIGGRAPH 2016 <a href="#fnref:HDRPaper" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:HDRDataset"> <p><a href="https://research.google/blog/introducing-the-hdr-burst-photography-dataset/">Introducing the HDR+ Burst Photography Dataset</a> <a href="#fnref:HDRDataset" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:VisualCore"> <p><a href="https://blog.google/products/pixel/pixel-visual-core-image-processing-and-machine-learning-pixel-2/">Pixel Visual Core: image processing and machine learning on Pixel 2</a> <a href="#fnref:VisualCore" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:HDRBracket"> <p><a href="https://research.google/blog/hdr-with-bracketing-on-pixel-phones/">HDR+ with Bracketing on Pixel Phones</a> <a href="#fnref:HDRBracket" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:LiveHDR"> <p><a href="https://research.google/blog/live-hdr-and-dual-exposure-controls-on-pixel-4-and-4a/">Live HDR+ and Dual Exposure Controls on Pixel 4 and 4a</a> <a href="#fnref:LiveHDR" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:HDRNet"> <p><a href="https://groups.csail.mit.edu/graphics/hdrnet/">Deep bilateral learning for real-time image enhancement</a>, in SIGGRPAH 2017 <a href="#fnref:HDRNet" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:BiGrid2"> <p><a href="https://dl.acm.org/doi/10.1145/1276377.1276506">Real-time Edge-Aware Image Processing with the Bilateral Grid</a>, in SIGGRAPH 2007 <a href="#fnref:BiGrid2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:BiGrid"> <p><a href="https://link.springer.com/chapter/10.1007/11744085_44">A Fast Approximation of the Bilateral Filter using a Signal Processing Approach</a>, in ECCV 2006 <a href="#fnref:BiGrid" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Guided"> <p><a href="https://ieeexplore.ieee.org/document/6319316/">Guided Image Filtering</a>, in T-PAMI 2013 <a href="#fnref:Guided" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:BiGuided"> <p><a href="https://dl.acm.org/doi/10.1145/2980179.2982423">Bilateral guided upsampling</a>, in SIGGRAPH Asia 2016 <a href="#fnref:BiGuided" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:NightSight"> <p><a href="https://research.google/blog/night-sight-seeing-in-the-dark-on-pixel-phones/">Night Sight: Seeing in the Dark on Pixel Phones</a> <a href="#fnref:NightSight" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:NightSightPaper"> <p><a href="https://dl.acm.org/doi/10.1145/3355089.3356508">Handheld Mobile Photography in Very Low Light</a>, in TOG 2019 <a href="#fnref:NightSightPaper" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:FCCC"> <p><a href="https://research.google/pubs/fast-fourier-color-constancy/">Fast Fourier Color Constancy</a>, in CVPR 2017 <a href="#fnref:FCCC" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Astrophotography"> <p><a href="https://research.google/blog/astrophotography-with-night-sight-on-pixel-phones/">Astrophotography with Night Sight on Pixel Phones</a> <a href="#fnref:Astrophotography" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Sky"> <p><a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Liba_Sky_Optimization_Semantically_Aware_Image_Processing_of_Skies_in_Low-Light_CVPRW_2020_paper.pdf">Sky Optimization: Semantically aware image processing of skies in low-light photography</a>, in CVPRW 2020 <a href="#fnref:Sky" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:SuperRes"> <p><a href="https://research.google/blog/see-better-and-further-with-super-res-zoom-on-the-pixel-3/">See Better and Further with Super Res Zoom on the Pixel 3</a> <a href="#fnref:SuperRes" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:SuperRes:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:RAISR"> <p><a href="https://ieeexplore.ieee.org/iel7/6745852/6960042/07744595.pdf">Rapid and Accurate Image Super-Resolution</a>, in TCI 2017 <a href="#fnref:RAISR" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:SKR"> <p><a href="http://ieeexplore.ieee.org/document/4060955/">Kernel Regression for Image Processing and Reconstruction</a>, in T-IP 2007 <a href="#fnref:SKR" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Portrait"> <p><a href="https://research.google/blog/portrait-mode-on-the-pixel-2-and-pixel-2-xl-smartphones/">Portrait mode on the Pixel 2 and Pixel 2 XL smartphones</a> <a href="#fnref:Portrait" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Portrait2"> <p><a href="https://research.google/blog/learning-to-predict-depth-on-the-pixel-3-phones/">Learning to Predict Depth on the Pixel 3 Phones</a> <a href="#fnref:Portrait2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:SynDepth"> <p><a href="https://dl.acm.org/doi/10.1145/3197517.3201329">Synthetic depth-of-field with a single-camera mobile phone</a>, in TOG 2018 <a href="#fnref:SynDepth" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Matting"> <p><a href="https://research.google/blog/accurate-alpha-matting-for-portrait-mode-selfies-on-pixel-6/">Accurate Alpha Matting for Portrait Mode Selfies on Pixel 6</a> <a href="#fnref:Matting" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Relight"> <p><a href="https://augmentedperception.github.io/total_relighting/">Learning to Relight Portraits for Background Replacement</a>, in SIGGRPAH 2021 <a href="#fnref:Relight" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Deblur"> <p><a href="https://research.google/blog/take-all-your-pictures-to-the-cleaners-with-google-photos-noise-and-blur-reduction/">Take All Your Pictures to the Cleaners, with Google Photos Noise and Blur Reduction</a> <a href="#fnref:Deblur" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Polyblur"> <p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502555">Polyblur: Removing Mild Blur by Polynomial Reblurring</a>, in T-CI 2017 <a href="#fnref:Polyblur" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Marc"> <p><a href="https://graphics.stanford.edu/~levoy/">Marc Levoy</a> <a href="#fnref:Marc" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Peymann"> <p><a href="https://sites.google.com/view/milanfarhome/">Peyman Milanfar</a> <a href="#fnref:Peymann" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="all"/><category term="computational-photography"/><summary type="html"><![CDATA[A deep dive into the evolution of Google Camera's algorithms and features, exploring key milestones like HDR+, Night Sight, and Super Res Zoom.]]></summary></entry><entry><title type="html">Image and Video Compression</title><link href="https://ddlee-cn.github.io/blog/2022/Image-Video-Codec/" rel="alternate" type="text/html" title="Image and Video Compression"/><published>2022-07-25T22:29:00+00:00</published><updated>2022-07-25T22:29:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2022/Image-Video-Codec</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2022/Image-Video-Codec/"><![CDATA[<h2 id="i-concepts-terminology-and-principles"><strong>I. Concepts, Terminology, and Principles</strong></h2> <h3 id="common-concepts-and-terminology">Common Concepts and Terminology</h3> <ul> <li><strong>Lossless Compression / Lossy Compression / The interdependent triangle</strong> <ul> <li><strong>Lossless compression</strong> techniques reduce the size of data by identifying and eliminating statistical redundancy without discarding any information from the original source. This means that the original data can be perfectly reconstructed, bit for bit, from the compressed version. Lossless compression is essential in scenarios where data integrity is paramount and no loss of information is tolerable. This includes executable programs, text documents, source code, and certain types of archival data. In image compression, formats like PNG (Portable Network Graphics) and GIF (Graphics Interchange Format) utilize lossless techniques. Lossless compression is also frequently employed as a stage within lossy compression schemes, for example, to compress motion vectors or the quantized transform coefficients after the primary lossy steps have been applied.</li> <li><strong>Lossy compression</strong> achieves significantly higher compression ratios than lossless methods by permanently discarding some information from the original data. The key principle is to remove data that is considered less important or less perceptible to human senses. For images and video, this often involves transforming the data into a domain (like the frequency domain using DCT) where perceptual importance can be more easily assessed, followed by quantization, where less important information (e.g., high-frequency details or subtle color variations) is represented with less precision or discarded entirely. Chroma subsampling<sup id="fnref:Chroma"><a href="#fn:Chroma" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> is another common lossy technique specific to color video. The original data cannot be perfectly reconstructed from the compressed version; only an approximation is recovered. There is a fundamental trade-off for lossy compression between the degree of compression (file size or bitrate) and the resulting quality (fidelity to the original). Lossy compression is ubiquitous in multimedia applications due to its ability to drastically reduce file sizes. Common examples include JPEG for still images, MP3 and AAC for audio, and nearly all modern video formats such as MPEG-2, H.264/AVC, and H.265/HEVC.</li> <li><strong>The interdependent triangle</strong> is a critical ballance to be managed by the codec between three factors: space (file size/bitrate), quality (distortion), and time (complexity). This involves the compression efficiency (how much the data size is reduced, often measured by bitrate), the amount of distortion introduced (particularly in lossy schemes, measured by fidelity to the original), and the computational resources (processing power, memory, and time) required for both encoding and decoding. Improving one aspect often comes at a cost to one or both of the others.</li> </ul> </li> </ul> <pre><code class="language-mermaid">graph TD
        subgraph " "
        direction LR
        S["&lt;b&gt;Space&lt;/b&gt;&lt;br/&gt;(File Size / Bitrate)&lt;br/&gt;&lt;i&gt;Compression Efficiency&lt;/i&gt;"]
        Q["&lt;b&gt;Quality&lt;/b&gt;&lt;br/&gt;(Fidelity vs. Distortion)&lt;br/&gt;&lt;i&gt;e.g., for Lossy Schemes&lt;/i&gt;"]
        T["&lt;b&gt;Time&lt;/b&gt;&lt;br/&gt;(Computational Resources)&lt;br/&gt;&lt;i&gt;Processing Power, Memory, Complexity&lt;/i&gt;"]
        end

    S &lt;--&gt;  Q
    Q &lt;--&gt;  T
    T &lt;--&gt;  S
</code></pre> <ul> <li><strong>Encoder / Decoder</strong> <ul> <li>An <strong>encoder</strong> is a device or, more commonly in modern systems, a software algorithm that transforms information from its original representation into a coded format, which is typically compressed. A <strong>decoder</strong> is a device or software algorithm that performs the inverse operation of an encoder. Its function is to convert the encoded (compressed) data bitstream back into a representation of the original information. The encoder and decoder are not standalone entities but rather two integral halves of a complete system, defined by a specific codec or standard. The bitstream generated by the encoder is meticulously structured with syntax and semantics that a compatible decoder is designed to interpret. The encoder makes numerous decisions during the compression process (e.g., which prediction modes to use, what quantization levels to apply) and signals these choices within the bitstream. The decoder must parse these signals correctly and apply the precise inverse processes to reconstruct the media. This tight coupling necessitates strict adherence to standardized specifications to ensure interoperability across different media and platforms.</li> <li>Common hardware codecs in GPUs or specialized System-on-Chips (SoCs) include: NVIDIA (NVENC/NVDEC)<sup id="fnref:NVIDIA"><a href="#fn:NVIDIA" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, Apple (A-series and M-series chips)<sup id="fnref:Apple"><a href="#fn:Apple" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. Common software codecs are: x264<sup id="fnref:x264"><a href="#fn:x264" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>/x265<sup id="fnref:x265"><a href="#fn:x265" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>, libavcodec(Ffmpeg)<sup id="fnref:ffmpeg"><a href="#fn:ffmpeg" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>, libaom (Alliance for Open Media)<sup id="fnref:libaom"><a href="#fn:libaom" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>.</li> </ul> </li> <li><strong>Container Format / Codec</strong> <ul> <li>A <strong>container format</strong>, also known as a wrapper format or media container, is a file format whose specification describes how different data elements and metadata coexist in a computer file. In the context of digital media, its primary purpose is to bundle various data streams together into a single file. A <strong>codec (Coder-Decoder)</strong> is an algorithm or software/hardware implementation responsible for the actual compression (encoding) and decompression (decoding) of the audio and video data streams (e.g., H.264 is a video codec, AAC is an audio codec). The container format, on the other hand, does not dictate the compression method itself; rather, it defines how the already compressed data (produced by one or more codecs) and any associated information are organized, stored, and interleaved within a single file.</li> <li><strong>Multiplexing (muxing)</strong> is the process of combining separate elementary streams into a single, cohesive bitstream suitable for storage or transmission, without changing the codec itself. In contrast, video <strong>transcoding</strong> is a broader scope of converting a video file from one digital format to another. This is a comprehensive operation that can involve changing one or more parameters of the video, including: the video codec and parameters (e.g., resolution), the audio codec and parameters (e.g., sample rate), and the container format.</li> </ul> </li> </ul> <h3 id="principles-reducing-data-redundancy-and-visual-redundancy">Principles: Reducing Data Redundancy and Visual Redundancy</h3> <ol> <li><strong>Exploiting and Eliminating Redundancy:</strong> Digital images and videos inherently contain a lot of repetitive or predictable information. The compression mindset seeks to identify and remove this redundancy in a way that either allows for perfect reconstruction (lossless) or minimizes noticeable loss (lossy). <ul> <li><strong>Spatial Redundancy (Within a single image or video frame):</strong> Images often have areas with similar or identical pixel values, like a large patch of blue sky or a plain wall. Instead of storing each pixel’s data individually, compression techniques find more compact ways to represent these uniform regions.</li> <li><strong>Temporal Redundancy (Between consecutive video frames):</strong> In a video, much of the scene often remains the same from one frame to the next, or objects move in predictable ways. Video compression heavily relies on this by encoding only the <em>differences</em> between frames or by describing how blocks of pixels have moved (motion estimation and compensation).</li> <li><strong>Statistical Redundancy (Coding Redundancy):</strong> Some pixel values or symbols appear more frequently than others in image or video data. Entropy encoding methods, like Huffman coding or Arithmetic coding, assign shorter binary codes to more common symbols and longer codes to rarer ones, reducing the overall bit count without losing information.</li> </ul> </li> <li><strong>Leveraging the Limits of Human Perception</strong> This is the cornerstone of <em>lossy</em> compression, which achieves much higher compression ratios by permanently discarding some information.The mindset here is to be selective about what data is thrown away, focusing on information that the human visual system is less sensitive to or is unlikely to perceive. <ul> <li><strong>Psycho-visual Redundancy:</strong> <ul> <li><strong>Sensitivity to Brightness over Color:</strong> The human eye is more sensitive to changes in brightness (luma) than to changes in color (chroma). Compression techniques exploit this through <em>chroma subsampling</em><sup id="fnref:Chroma:1"><a href="#fn:Chroma" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, where color information is stored at a lower resolution than brightness information, leading to significant data savings with often imperceptible impact on visual quality.</li> <li><strong>Sensitivity to Spatial Frequencies:</strong> We are generally less able to perceive very fine details or rapid changes (high spatial frequencies) compared to larger, smoother areas (low spatial frequencies). Transforms like the Discrete Cosine Transform (DCT), used in JPEG, convert image blocks into frequency components. This “energy compaction” concentrates most of the visually important information into a few low-frequency coefficients. The less critical high-frequency coefficients can then be <em>quantized</em> more aggressively—represented with less precision or even discarded entirely—further reducing data size.</li> </ul> </li> </ul> </li> </ol> <h2 id="ii-image-compression-jpeg"><strong>II. Image Compression (JPEG)</strong></h2> <h3 id="standardizing-body-jpeg-joint-photographic-experts-group">Standardizing Body: JPEG (Joint Photographic Experts Group)</h3> <p>JPEG refers to both the committee, the Joint Photographic Experts Group, and the widely implemented image compression standard they developed. The foundational standard is formally designated as ISO/IEC 10918-1<sup id="fnref:JPEGISO"><a href="#fn:JPEGISO" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>.</p> <h3 id="jpeg-encoding-pipeline">JPEG encoding pipeline</h3> <p>Recommendation: <a href="http://www.youtube.com/watch?v=Kv1Hiv3ox8I">Explainer video by Branch Education</a></p> <pre><code class="language-mermaid">graph TD
    A[Image] --&gt; B(Color Space Transformation &lt;br&gt; e.g., RGB to Y'CbCr);
    B --&gt; C(Chroma Subsampling &lt;br&gt; e.g., 4:2:0, 4:2:2 - Lossy);
    C --&gt; D(Image Division &lt;br&gt; into 8x8 Pixel Blocks &lt;br&gt; for Y', Cb, Cr components);
    D --&gt; E(Discrete Cosine Transform  &lt;br&gt; Applied to each 8x8 block &lt;br&gt; Converts spatial to frequency domain);
    E --&gt; F(Quantization &lt;br&gt; Divide DCT coefficients by Q-Table values &lt;br&gt; Round to nearest integer);

    F --&gt; G[Entropy Encoding - Lossless];

    subgraph G [Entropy Encoding]
        direction LR
        G1(Zigzag Scan &lt;br&gt; Reorders quantized AC coefficients) --&gt; G2;
        G2(Run-Length Encoding  &lt;br&gt; Encodes runs of zeros for AC coefficients) --&gt; G4;
        G3(Differential Pulse Code Modulation  &lt;br&gt; Encodes difference of current DC from previous DC) --&gt; G4;
        G4(Huffman Coding / Arithmetic Coding &lt;br&gt; Assigns variable-length codes &lt;br&gt; to DPCM-coded DC &amp; RLE-coded AC symbols);
    end

    G --&gt; H[Bits];

    classDef lossyStep fill:#ffdddd,stroke:#333,stroke-width:2px;
    class C,F lossyStep;
</code></pre> <ol> <li><strong>Color Space Transformation (e.g., RGB to Y’CbCr):</strong> <ul> <li><strong>Purpose:</strong> Digital images are often captured or represented in the RGB (Red, Green, Blue) color space. For compression, JPEG often converts the image data into a color space like Y’CbCr<sup id="fnref:YCbCr"><a href="#fn:YCbCr" class="footnote" rel="footnote" role="doc-noteref">9</a></sup>. The human eye is more sensitive to changes in brightness (luma) than to changes in color (chroma).[4, 8, 6, 9, 10] This separation allows the chroma components to be compressed more aggressively in subsequent steps.</li> <li><strong>Process:</strong> <ul> <li><strong>Y’ (Luma):</strong> Represents the brightness or luminance component (the black and white information) of the image.</li> <li><strong>Cb and Cr (Chroma):</strong> Represent the blue-difference and red-difference color components, respectively.</li> </ul> </li> </ul> </li> <li><strong>Chroma Subsampling (Downsampling):</strong> <ul> <li><strong>Purpose:</strong> To reduce the amount of color data based on the human visual system’s lower acuity for color detail. This is a lossy step. This step significantly reduces the amount of data for the color channels with minimal perceived impact on image quality for most images.</li> <li><strong>Process:</strong> The Cb and Cr (chroma) components are sampled at a lower spatial resolution than the Y’ (luma) component. Common schemes include: <ul> <li><strong>4:4:4:</strong> No subsampling. Luma and chroma have the same resolution.</li> <li><strong>4:2:2:</strong> Chroma is sampled at half the horizontal resolution of luma.</li> <li><strong>4:2:0:</strong> Chroma is sampled at half the horizontal and half the vertical resolution of luma. This is very common.</li> </ul> </li> </ul> </li> </ol> <p><img src="https://i.imgur.com/AU8EzR2.png" alt=""/></p> <ol> <li><strong>Image Division into 8x8 Pixel Blocks:</strong> <ul> <li><strong>Purpose:</strong> To prepare the image data for the Discrete Cosine Transform (DCT).</li> <li><strong>Process:</strong> Each component of the image (Y’, Cb, and Cr, after any subsampling) is divided into non-overlapping 8x8 blocks of pixels. If the dimensions of a component are not a multiple of 8, padding might be applied.</li> </ul> </li> <li><strong>Discrete Cosine Transform (DCT):</strong> <ul> <li><strong>Purpose:</strong> To convert the spatial information of each 8x8 pixel block into frequency domain information. The DCT has excellent “energy compaction” properties, meaning it concentrates most of the visually significant information into a few coefficients. By transforming to the frequency domain, it becomes easier to separate perceptually important information (low frequencies) from less important information (high frequencies). The DCT itself is not lossy, except for potential rounding errors in computation.</li> <li><strong>Process:</strong> A 2D DCT (typically Type-II DCT) is applied to each 8x8 block. This transforms the 64 pixel values into 64 DCT coefficients. <ul> <li>The top-left coefficient is the <strong>DC coefficient</strong>, representing the average value of the 64 pixels in the block.</li> <li>The other 63 coefficients are <strong>AC coefficients</strong>, representing successively higher spatial frequencies (details and textures).</li> </ul> </li> </ul> </li> <li><strong>Quantization:</strong> <ul> <li><strong>Purpose:</strong> This is the primary lossy step in JPEG compression. It reduces the precision of the DCT coefficients, especially the high-frequency ones that are less perceptible to the human eye.</li> <li><strong>Process:</strong> Each of the 64 DCT coefficients in a block is divided by a corresponding value from an 8x8 <strong>Quantization Matrix</strong> (also called a Q-table), and the result is rounded to the nearest integer. <ul> <li>JPEG typically uses two Q-tables: one for the luma component and one for the chroma components (which are usually more aggressively quantized).</li> <li>The values in the Q-table are larger for higher-frequency AC coefficients, leading to more aggressive compression (many become zero) for those details. The “quality” setting in JPEG encoders often scales these Q-table values.</li> </ul> </li> </ul> </li> <li><strong>Entropy Encoding:</strong> This is a lossless stage that further compresses the quantized DCT coefficients. It typically involves several sub-steps: <ul> <li><strong>Zigzag Scan:</strong> <ul> <li><strong>Purpose:</strong> To group low-frequency coefficients (which are more likely to be non-zero after quantization) together and create long runs of zeros for high-frequency coefficients.</li> <li><strong>Process:</strong> The 63 AC coefficients in each 8x8 quantized block are read out in a zigzag pattern, from the top-left (low frequency) to the bottom-right (high frequency).</li> </ul> </li> <li><strong>Run-Length Encoding (RLE) for AC Coefficients:</strong> <ul> <li><strong>Purpose:</strong> To efficiently encode the many zero-valued AC coefficients that result from quantization, especially after the zigzag scan.</li> <li><strong>Process:</strong> Instead of coding each zero, RLE typically encodes a pair: <code class="language-plaintext highlighter-rouge">(run_length_of_preceding_zeros, next_non_zero_AC_coefficient_value)</code>. A special End-of-Block (EOB) code (often (0,0)) indicates that all remaining AC coefficients in the block are zero.</li> </ul> </li> <li><strong>Differential Pulse Code Modulation (DPCM) for DC Coefficients:</strong> <ul> <li><strong>Purpose:</strong> To exploit the correlation between the DC coefficients of adjacent 8x8 blocks (average block intensities often change gradually).</li> <li><strong>Process:</strong> Instead of encoding the absolute value of each DC coefficient, the difference between the current block’s DC coefficient and the DC coefficient of the <em>previous</em> block is encoded.</li> </ul> </li> <li><strong>Huffman Coding (or Arithmetic Coding):</strong> <ul> <li><strong>Purpose:</strong> To assign shorter binary codes to more frequently occurring symbols (DPCM-coded DC values and RLE-coded AC pairs) and longer codes to less frequent ones.</li> <li><strong>Process (Huffman):</strong> <ul> <li>For DC coefficients, the <em>category</em> (size in bits) of the DPCM difference is Huffman coded, followed by the actual bits of the difference.</li> <li>For AC coefficients, the combined <code class="language-plaintext highlighter-rouge">(run_length, category_of_next_non_zero_AC_coefficient)</code> symbol is Huffman coded, followed by the actual bits of the non-zero AC coefficient.</li> </ul> </li> <li><strong>Arithmetic Coding:</strong> An alternative, often slightly more efficient entropy coding method also specified in the JPEG standard, but less commonly used due to historical patent issues and wider Huffman support.</li> </ul> </li> </ul> </li> </ol> <p>The result of these steps is a compressed bitstream representing the image, which is then typically wrapped in a file format like JPEG. The decompression process essentially reverses these steps, using the stored Q-tables and Huffman tables to reconstruct an approximation of the original image.</p> <h3 id="further-notes">Further Notes</h3> <ul> <li><strong>The basis function.</strong> For image compression, ideal basis functions should possess several properties: they should allow the energy (or information content) of typical image signals to be “compacted” into a relatively small number of transform coefficients; they should be computationally efficient to calculate; and they should ideally have good decorrelation properties, meaning they transform correlated pixel data into less correlated coefficients.</li> <li><strong>The trade-off.</strong> The Quantization Matrix serves as the direct control mechanism for the lossiness and, consequently, the compression-versus-quality trade-off in JPEG. The specific values chosen for the 64 entries in this matrix (often scaled by a single “quality” factor in user interfaces) dictate precisely how much information is discarded from each corresponding frequency component of the DCT. By employing larger divisor values for high-frequency DCT coefficients (to which the human visual system is less sensitive) and smaller divisor values for the DC and low-frequency AC coefficients (which are more critical for perceived quality), JPEG tailors the information loss profile to align with human visual perception.</li> <li><strong>The scanning order.</strong> The zigzag scanning pattern applied to the quantized AC coefficients before RLE is not an arbitrary choice. It systematically orders the coefficients from the lowest spatial frequencies (excluding the DC coefficient, which is handled separately) to the highest. This ordering is synergistic with the effects of DCT and quantization: since high-frequency coefficients are more likely to be quantized to zero, the zigzag scan tends to create longer contiguous runs of zeros. This, in turn, maximizes the effectiveness of Run-Length Encoding<sup id="fnref:Huffman"><a href="#fn:Huffman" class="footnote" rel="footnote" role="doc-noteref">10</a></sup>, as RLE achieves greater compression when it encounters longer sequences of identical symbols.</li> </ul> <h2 id="iv-video-compression-h26x"><strong>IV. Video Compression (H.26x)</strong></h2> <h3 id="standardizing-bodies-m-jepg-mpeg-h26x">Standardizing Bodies: M-JEPG, MPEG, H.26x</h3> <ul> <li><strong>Motion-JPEG (M-JPEG)</strong> is a straightforward video compression format where each individual frame of the video sequence is compressed independently using the JPEG still image compression standard.34 Essentially, an M-JPEG stream is a sequence of complete JPEG images.</li> <li><strong>MPEG (Moving Picture Experts Group)</strong> is a working group operating under the joint auspices of ISO (International Organization for Standardization) and IEC (International Electrotechnical Commission), specifically as part of ISO/IEC JTC 1/SC 29 (Coding of audio, picture, multimedia and hypermedia information). MPEG has produced a highly influential family of standards that have become foundational to digital television, optical disc media (DVD, Blu-ray), internet streaming, and many other digital video applications. Key video coding standards include MPEG-1 (used for Video CD), MPEG-2 (used for DVD and digital broadcast), MPEG-4 Part 2 (Visual), MPEG-4 Part 10 (Advanced Video Coding, also H.264), and MPEG-H Part 2 (High Efficiency Video Coding, also H.265).</li> <li><strong>H.262 / MPEG-2 Part 2 Video</strong> is a standard developed and maintained collaboratively by the ITU-T (International Telecommunication Union - Telecommunication Standardization Sector) as Recommendation H.262, and by ISO/IEC MPEG as Part 2 of the MPEG-2 standard (formally ISO/IEC 13818-2).</li> <li><strong>H.264 / MPEG-4 Part 10 Advanced Video Coding (AVC)</strong> was a landmark joint development effort by the ITU-T Video Coding Experts Group (VCEG), where it is known as Recommendation H.264<sup id="fnref:H264"><a href="#fn:H264" class="footnote" rel="footnote" role="doc-noteref">11</a></sup>, and by ISO/IEC MPEG, where it is designated as MPEG-4 Part 10, Advanced Video Coding (formally ISO/IEC 14496-10). The collaborative body responsible for its development was known as the Joint Video Team (JVT). H.264/AVC rapidly became the most widely adopted video codec due to its superior compression performance. It is used in Blu-ray Discs, numerous streaming services (e.g., YouTube, Netflix), digital television broadcasting (especially for HD content), video conferencing systems, mobile devices, and digital security cameras.</li> <li><strong>H.265 / High Efficiency Video Coding (HEVC)</strong> was also jointly developed by ITU-T VCEG (as Recommendation H.265<sup id="fnref:H265"><a href="#fn:H265" class="footnote" rel="footnote" role="doc-noteref">12</a></sup>) and ISO/IEC MPEG (as MPEG-H Part 2, High Efficiency Video Coding, formally ISO/IEC 23008-2).45 The collaborative body for HEVC was the Joint Collaborative Team on Video Coding (JCT-VC).</li> </ul> <h3 id="core-concepts">Core Concepts</h3> <p><img src="https://i.imgur.com/YYeEB3R.jpeg" alt=""/></p> <ul> <li><strong>Group of Pictures (GoP)</strong> is a sequence of consecutively coded frames in a video stream, typically starting with an I-frame and followed by a specific arrangement of P-frames and B-frames. The GoP structure defines the dependencies between frames and dictates how often I-frames (random access points) occur. A common GoP structure might be represented as I B B P B B P B B P B B I…. The pattern (e.g., IBBP) repeats between I-frames. Key parameters describing a GoP are its length (N, the distance between successive I-frames) and the distance between anchor frames (I or P frames, denoted M, which also implies the number of B-frames between them).</li> <li><strong>I-frame (Intra-coded picture)</strong> is coded entirely using intra-frame prediction techniques, meaning it makes no reference to any other frames in the video sequence. It is essentially a self-contained, independently decodable picture, compressed similarly to a still image (e.g., using DCT, quantization, and entropy coding on its own pixel data). The frequency of I-frames (i.e., the GoP length N) directly impacts <strong>random access</strong> capability. In case of transmission errors that might corrupt data in P-frames or B-frames, an I-frame acts as a “refresh” point. Errors cannot propagate past an I-frame because its decoding is independent. I-frames are the least compressed type of frame because they do not exploit temporal redundancy, resulting in the largest size among the frame types.</li> <li><strong>P-frame (Predictive-coded picture)</strong> is coded using inter-frame prediction from one or more <em>preceding</em> (in display or decoding order) reference frames, which are typically an earlier I-frame or another P-frame. P-frames encode motion vectors (indicating where the predicted blocks come from in the reference frame) and the quantized residual (the difference between the actual block and its prediction).</li> <li><strong>B-frame (Bi-predictive-coded picture)</strong> is coded using inter-frame prediction that can reference one or more <em>preceding</em> frames and/or one or more <em>future</em> (in display order, but already decoded and stored) reference frames. This means a B-frame can find predictor blocks by looking “backwards” in time, “forwards” in time, or by interpolating between blocks from both past and future reference frames. B-frames typically offer the highest compression efficiency among the three frame types. The ability to use future frames as references often allows for better matches (e.g., for uncovered regions) or more accurate motion interpolation. However, the use of B-frames introduces greater encoding and decoding delay because the decoder needs to have future reference frames available before it can decode a B-frame that depends on them. This also means the decoding order of frames can differ from their display order.</li> <li><strong>Macroblock / Coding Tree Unit (CTU)</strong> refer to the fundamental rectangular blocks into which a video frame is partitioned for the purpose of compression processing. Most key coding operations, including motion estimation, motion compensation, spatial prediction, transform coding, and quantization, are performed on these units or sub-units derived from them. <strong>H.264/AVC</strong> primarily uses a <strong>Macroblock</strong> as its basic processing unit. A macroblock in H.264 typically covers a 16x16 luma sample region, along with the corresponding chroma samples (e.g., two 8x8 chroma blocks in 4:2:0 subsampling). These 16x16 macroblocks can be further partitioned into smaller blocks (e.g., 16x8, 8x16, 8x8, and down to 4x4) for motion compensation, allowing for finer adaptation to motion details. <strong>H.265/HEVC</strong> introduces the concept of a <strong>Coding Tree Unit (CTU)</strong>, which replaces the macroblock as the fundamental processing unit. CTUs can be significantly larger than H.264 macroblocks, with configurable sizes such as 64x64, 32x32, or 16x16 luma samples (with 64x64 being common for UHD video). Each CTU is then recursively partitioned using a quadtree structure into smaller <strong>Coding Units (CUs)</strong>. CUs can vary in size (e.g., from 64x64 down to 8x8 in the HEVC main profile). Each CU is then further divided into one or more <strong>Prediction Units (PUs)</strong>, which define the block size and mode for prediction (intra or inter), and one or more <strong>Transform Units (TUs)</strong>, which define the block size for the transform and quantization of the residual signal.</li> <li><strong>Motion Vector (MV)</strong> is a two-dimensional vector, typically represented as (dx, dy), that specifies the displacement of a block or region in the current frame relative to its corresponding matching block or region in a reference frame. MVs are the direct output of the motion estimation process performed by the encoder. They are then used by the motion compensation process (at both encoder and decoder) to generate the predicted block from the reference frame.</li> <li><strong>Reference Frame(s) / Picture(s)</strong> are previously decoded frames (which could be I-frames or P-frames, and in some advanced schemes, even B-frames) that are stored in a memory buffer called the Decoded Picture Buffer (DPB) at both the encoder and decoder.44 These stored frames serve as the source from which predictor blocks are formed during the inter-prediction of subsequent P-frames or B-frames. Modern video coding standards like H.264/AVC and H.265/HEVC allow for the use of multiple reference frames for predicting a single block. This means the encoder can choose the best match from a larger set of candidate blocks in different temporal locations, often leading to more accurate predictions and improved compression efficiency, especially in scenes with complex motion patterns, occlusions, or repetitive movements.</li> <li><strong>Residual (Prediction Error)</strong> is the sample-by-sample difference between the actual pixel values of a block in the current frame and the pixel values of its corresponding motion-compensated predictor block (for inter-prediction) or intra-predicted block (for intra-prediction). Typically, the residual block undergoes a transform (like DCT or a variant), quantization, and entropy coding.</li> </ul> <h3 id="prediction-techniques">Prediction Techniques</h3> <p>Prediction is the heart of video compression, aiming to create a model of the current picture based on already coded information. The difference between the actual picture and the prediction (the residual) is what gets coded.</p> <p><img src="https://i.imgur.com/ZWewD7f.jpeg" alt=""/></p> <ul> <li><strong>Intra-frame Prediction</strong>, or simply “intra-prediction,” exploits spatial redundancy by predicting the pixel values of a current block based on the values of already coded and reconstructed neighboring pixels within the <em>same</em> frame or picture.41 The encoder selects the prediction mode (e.g., a specific direction from which to extrapolate pixel values) that yields the best match, and then encodes the chosen mode and the prediction residual.</li> <li><strong>Inter-frame Prediction</strong>, or “inter-prediction,” exploits temporal redundancy by predicting a block of pixels in the current frame from similar blocks located in one or more previously coded and reconstructed frames, known as reference frames. This is the fundamental technique used for P-frames and B-frames. This process involves two key components: motion estimation (at the encoder) and motion compensation (at both encoder and decoder). <ul> <li><strong>Motion Estimation (ME).</strong> The goal of Motion Estimation is to determine the displacement (motion) of objects or regions between the current frame being encoded and one or more reference frames that have already been coded and reconstructed. In block-matching ME, the current frame is divided into blocks (e.g., macroblocks in H.264, or Prediction Units (PUs) derived from CUs in HEVC). For each block in the current frame, the encoder searches for a block in a designated search window within a reference frame that provides the “best match”. The “best match” is typically identified by minimizing a cost function, such as the Sum of Absolute Differences (SAD) or Mean Squared Error (MSE) between the pixel values of the current block and the candidate block in the reference frame.</li> <li><strong>Motion Compensation (MC).</strong> Once a motion vector (MV) has been determined for a block by the ME process, Motion Compensation (MC) or Motion Compensated Prediction (MCP) is the process of actually forming the predicted block. The decoder (and the encoder, for reconstructing its own reference frames to ensure synchronization) uses this MV to identify and retrieve the corresponding predictor block from the specified reference frame. This predictor block is then used as the prediction for the current block. Typically, the decoded residual (prediction error) is added to this predictor block to reconstruct the final pixel values of the current block. A crucial aspect of modern MCP is handling MVs that point to sub-pixel locations (e.g., half-pixel or quarter-pixel positions). Since these fractional positions don’t directly correspond to stored pixel values in the reference frame, interpolation filters are used to generate the required sample values at these sub-pixel coordinates.</li> </ul> </li> </ul> <h3 id="pipeline">Pipeline</h3> <p><img src="https://i.imgur.com/gaQ5Mhb.jpeg" alt=""/></p> <p>The above diagram summerizes the HEVC<sup id="fnref:HEVC"><a href="#fn:HEVC" class="footnote" rel="footnote" role="doc-noteref">13</a></sup> pipeline. Key steps are:</p> <ul> <li>Frame Preparation &amp; Coding Tree Unit (CTU) Partitioning</li> <li>Prediction Unit (PU) and Transform Unit (TU) Specification</li> <li>Prediction (Enhanced Intra &amp; Inter)</li> <li>Residual Calculation &amp; Transform</li> <li>Quantization &amp; Scaling</li> <li>Entropy Coding: Context-Adaptive Binary Arithmetic Coding (CABAC)</li> <li>In-Loop Filtering: Deblocking Filter (DBF) and Sample Adaptive Offset (SAO)</li> <li>Bitstream Formation</li> </ul> <h3 id="recent-advances">Recent Advances</h3> <p>The evolution<sup id="fnref:HEVCTutorial"><a href="#fn:HEVCTutorial" class="footnote" rel="footnote" role="doc-noteref">14</a></sup> from H.264/AVC to H.265/HEVC are</p> <ul> <li>Higher Compression Efficiency: About 50% bitrate reduction for similar quality.</li> <li>Larger and More Flexible Coding Units: CTUs and adaptable CU/PU/TU partitioning.</li> <li>More Sophisticated Prediction: More intra modes, advanced motion vector prediction, merge mode.</li> <li>Larger Transform Sizes &amp; Transform Skip.</li> <li>Sample Adaptive Offset (SAO): Additional in-loop filter for improved quality.</li> <li>Enhanced Parallel Processing Tools: Tiles and WPP for efficient multi-core encoding/decoding.</li> </ul> <h2 id="v-final-remarks"><strong>V. Final Remarks</strong></h2> <h3 id="ai-based-compression">AI-based Compression</h3> <p>AI-based compression, often referred to as “learned” or “neural” compression, utilizes deep learning—specifically, neural networks—to replace or enhance parts of the traditional pipeline. Instead of relying on predefined mathematical formulas, these systems are trained on vast datasets of images and videos. They learn the most efficient ways to represent visual information, often achieving significantly higher compression ratios and better perceptual quality.</p> <p>The core of many AI-based image codecs is a type of neural network called an autoencoder. It consists of two main parts:</p> <ul> <li>An encoder network that analyzes an image and compresses it into a compact latent representation (a highly compressed set of data).</li> <li>A decoder network that takes this latent representation and reconstructs the image.</li> </ul> <p>The entire system is trained end-to-end to minimize the difference between the original and reconstructed images, as well as the size of the latent representation. This approach allows the AI to discover more complex and efficient ways to encode visual data than human-engineered algorithms.</p> <p>Although in the academic literature, many papers are published claiming outperforming latest standards, the adoption of AI solutions faces multiple challenges:</p> <ul> <li>Computational Complexity: Unlike traditional codecs that use well-defined mathematical operations (like the DCT), AI models require significant processing power, often relying on specialized hardware like GPUs or NPUs (Neural Processing Units). This creates a high barrier to entry for low-power or older devices, potentially delaying widespread adoption until such hardware is ubiquitous.</li> <li>Interpretability: Traditional algorithms are deterministic and transparent; their steps can be followed and understood. Many advanced AI models are considered “black boxes.”</li> <li>Data Bias and Privacy Issue: A significant ethical hurdle is that any biases present in the training data will be learned and potentially amplified by the AI model. Many large datasets are created by scraping the internet, which often involves using personal photos and copyrighted material without explicit consent. This raises serious privacy concerns and legal issues regarding intellectual property.</li> <li>Difficulty in Standardization: With an AI model, simply standardizing the model’s architecture isn’t enough, as the trained “weights” (the learned parameters) are what define its behavior. Distributing and standardizing these large sets of weights is a new challenge.</li> </ul> <h3 id="proprietary-vs-open-source">Proprietary VS. Open-Source</h3> <p>Through the years, the video compression field is dominated by proprietary standards developed jointly by the ISO/IEC Moving Picture Experts Group (MPEG) and the ITU-T Video Coding Experts Group (VCEG), e.g., HEVC/H.265. Proprietary standards are built upon a collection of patents owned by various companies. To legally use the standard, a device manufacturer or software developer must acquire licenses for this intellectual property, which almost always involves paying royalties. Instead of a single, clear entity to negotiate with, patent holders often group themselves into “patent pools.”<sup id="fnref:Pool"><a href="#fn:Pool" class="footnote" rel="footnote" role="doc-noteref">15</a></sup></p> <p><img src="https://i.imgur.com/cQvZ6ld.jpeg" alt=""/> On the other hand, the proprietary model also hinder the application in the scenario of web streaming. The proprietary codecs have struggled to gain native support in major web browsers like Chrome and Firefox. These projects rely on open-source components and cannot easily integrate technology with royalty obligations.</p> <p>Device manufacturers, software developers, and content creators may choose to stick with an older, “good enough” standard (like H.264) whose licensing situation is settled and well-understood, rather than invest in a new one with unpredictable costs. Currently, H.264 remains the most widely compatible codec despite its age.</p> <p>The formation of the Alliance for Open Media (AOMedia) by giants like Google, Amazon, Netflix, and Apple was a direct response to the frustrating licensing landscape of HEVC. Their creation of the royalty-free AV1 codec has permanently fragmented the market. Now, many streaming services must support both HEVC (for the broadcast and Apple ecosystems) and AV1 (for the web and Android ecosystems), which increases complexity and cost.</p> <h3 id="coding-for-machine-vision">Coding for Machine Vision</h3> <p>The core idea of Coding for Machine Vision (CMV), also known as Video Coding for Machines (VCM), is to fundamentally shift the objective of compression. Instead of optimizing for human perceptual quality, the goal is to compress video while preserving the specific information that artificial intelligence algorithms need to perform their tasks accurately and efficiently. The problem is that traditional codecs like H.264 and HEVC are “unaware” of what a machine is looking for. They might discard subtle, high-frequency textures or details that they deem irrelevant to human vision, but which could be critical for an AI to distinguish between two objects.</p> <p>Instead of compressing the pixels of the video, the AI model at the source (e.g., in the camera) first extracts the essential information—the “features”—it needs for its task. The system then compresses and transmits only these features. This method offers the highest possible compression, as all visually redundant pixel data is discarded. The downside is that it’s highly task-specific, and you cannot reconstruct a viewable video from the stream.</p> <h3 id="coding-of-generative-models">Coding of Generative Models</h3> <p>The emergence of generative models like NeRF (Neural Radiance Fields) represents a fundamental shift in how we capture and represent visual reality. Instead of storing a collection of 2D images or a 3D mesh, these models create a “digital twin” of a scene within the weights of a neural network. This allows for the rendering of photorealistic, novel views from any position or angle.</p> <p>In the sense of generative model, the definition of the information changes from visual output to the parametric model that generates the scene. This has given rise to a new and rapidly evolving field of research: the coding and compression of generative models. The core idea is a complete departure from traditional video compression. Instead of compressing the output (the pixels of an image or video), the goal is to compress the generative model itself.</p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:Chroma"> <p><a href="https://en.wikipedia.org/wiki/Chroma_subsampling">Chroma subsampling - Wikipedia</a> <a href="#fnref:Chroma" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:Chroma:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:NVIDIA"> <p><a href="https://developer.nvidia.com/video-codec-sdk">NVIDIA Codec SDK</a> <a href="#fnref:NVIDIA" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Apple"> <p><a href="https://en.wikipedia.org/wiki/Apple_silicon">Apple Silicon</a> <a href="#fnref:Apple" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:x264"> <p><a href="https://www.videolan.org/developers/x264.html">x264 - VideoLan</a> <a href="#fnref:x264" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:x265"> <p><a href="https://www.videolan.org/developers/x264.html">x265 - VideoLan</a> <a href="#fnref:x265" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:ffmpeg"> <p><a href="https://www.ffmpeg.org/libavcodec.html">Libavcodec Documentation</a> <a href="#fnref:ffmpeg" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:libaom"> <p><a href="https://aomedia.googlesource.com/aom/">Alliance for Open Media</a> <a href="#fnref:libaom" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:JPEGISO"> <p><a href="https://www.iso.org/standard/18902.html">ISO/IEC 10918-1:1994</a> <a href="#fnref:JPEGISO" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:YCbCr"> <p><a href="https://en.wikipedia.org/wiki/YCbCr">YCbCr - Wikipedia</a> <a href="#fnref:YCbCr" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Huffman"> <p><a href="https://en.wikibooks.org/wiki/JPEG_-_Idea_and_Practice/The_Huffman_coding">JPEG - Idea and Practice/The Huffman coding</a> <a href="#fnref:Huffman" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:H264"> <p><a href="https://www.itu.int/rec/T-REC-H.264">H.264 : Advanced video coding for generic audiovisual services</a> <a href="#fnref:H264" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:H265"> <p><a href="https://www.itu.int/rec/T-REC-H.265">H.265 / High Efficiency Video Coding (HEVC)</a> <a href="#fnref:H265" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:HEVC"> <p><a href="https://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf">Overview of the High Efficiency Video Coding (HEVC) Standard</a>, in T-CSVT, 2012 <a href="#fnref:HEVC" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:HEVCTutorial"> <p><a href="https://eems.mit.edu/wp-content/uploads/2014/06/H.265-HEVC-Tutorial-2014-ISCAS.pdf">Design and Implementa/on of Next Genera/on Video Coding Systems (H.265/HEVC Tutorial)</a>, in ISCAS Tutorial 2014 <a href="#fnref:HEVCTutorial" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:Pool"> <p><a href="https://accessadvance.com/hevc-worldwide-patent-landscape/">HEVC Worldwide Essential Patents Landscape</a> <a href="#fnref:Pool" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="all"/><category term="streaming-display"/><summary type="html"><![CDATA[An note of the fundamentals of image and video compression techniques.]]></summary></entry><entry><title type="html">Image Signal Processing (ISP) Pipeline and 3A Algorithms</title><link href="https://ddlee-cn.github.io/blog/2022/ISP/" rel="alternate" type="text/html" title="Image Signal Processing (ISP) Pipeline and 3A Algorithms"/><published>2022-04-21T22:29:00+00:00</published><updated>2022-04-21T22:29:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2022/ISP</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2022/ISP/"><![CDATA[<h2 id="i-the-purpose-handling-imperfections-and-matching-human-vision"><strong>I. The Purpose: Handling Imperfections and Matching Human Vision</strong></h2> <p>The Image Signal Processor (ISP) is a crucial component in digital imaging systems, acting as the bridge between the raw data captured by an image sensor and the final, viewable image. Its primary purpose is to perform a series of complex processing steps to convert this raw, often imperfect, sensor data into a high-quality digital image suitable for display or storage. Key functions of the ISP include demosaicing (reconstructing a full-color image from the sensor’s color filter array), noise reduction, color correction, white balance adjustment, auto exposure control, and image enhancement tasks like sharpening and contrast adjustment. Essentially, the ISP takes the initial, unprocessed output from the sensor, which can suffer from issues like noise, incorrect colors, and improper brightness, and refines it into a visually appealing and accurate representation of the scene.</p> <p>In essence, the ISP and 3A algorithms work in concert to process the raw sensor data in a way that compensates for the limitations and characteristics of the electronic sensor, aiming to produce a final image that more closely matches the rich, adaptive, and consistent visual experience of human eyesight.</p> <h3 id="a-brief-overview-of-the-isps-function">A Brief overview of the ISP’s function</h3> <p>The input to the ISP is typically a single-channel (monochrome) image, often arranged in a specific pattern of color filters known as a Color Filter Array (CFA), most commonly the Bayer pattern. The output is usually a standard color image format, such as sRGB.</p> <p>The conversion is not a single operation but a cascade of processing stages, collectively referred to as the <strong>ISP pipeline</strong>. Each stage in this pipeline performs a specific task to progressively enhance the image quality and correct for imperfections inherent in the raw sensor data and the image capture process. Key transformations within a typical ISP pipeline include, but are not limited to:</p> <ul> <li><strong>Preprocessing:</strong> Black level correction, defective pixel correction, lens shading correction.</li> <li><strong>CFA Processing:</strong> Demosaicing (or debayering) to reconstruct a full-color image from the CFA data.</li> <li><strong>Noise Reduction:</strong> Filtering to remove various types of noise.</li> <li><strong>Color and Tone Adjustments:</strong> White balance correction, color correction (e.g., using a color correction matrix), gamma correction, and tone mapping.</li> <li><strong>Image Enhancement / Photo-finishing:</strong> Sharpening, contrast adjustment, and stylish enhancement, etc.</li> <li><strong>Output Formatting:</strong> Color space conversion and image compression.</li> </ul> <p><img src="https://i.imgur.com/EybiGXI.jpeg" alt=""/></p> <p>The above diagram is from Prof. Mi S. Brown<sup id="fnref:MSBrown"><a href="#fn:MSBrown" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>’s MUST-READ tutorial<sup id="fnref:ICCVTutorial"><a href="#fn:ICCVTutorial" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> for ISP at ICCV 2019.</p> <h3 id="the-role-and-importance-of-3a-algorithms">The role and importance of 3A algorithms</h3> <p>The term <strong>3A</strong> refers to three crucial sets of automated control algorithms: <strong>Auto Exposure (AE)</strong>, <strong>Auto White Balance (AWB)</strong>, and <strong>Auto Focus (AF)</strong>. These algorithms are typically implemented within or in close coordination with the ISP and are designed to emulate the adaptive capabilities of the human visual system, automatically optimizing key image capture parameters.</p> <p><img src="https://i.imgur.com/hkaveiz.jpeg" alt=""/></p> <p>The overarching goal of 3A algorithms is to ensure that the images captured by a camera are:</p> <ul> <li><strong>Clear and Sharp (AF):</strong> The subject of interest is accurately focused.</li> <li><strong>Well-Exposed (AE):</strong> The image brightness is appropriate, neither too dark (underexposed) nor too bright (overexposed), preserving detail in both shadows and highlights where possible.</li> <li><strong>Color-Accurate (AWB):</strong> Colors are rendered naturally, free from unrealistic casts caused by different lighting conditions, ensuring that white objects appear white.</li> </ul> <p>The importance of 3A algorithms cannot be overstated. They are fundamental to the user experience in consumer digital cameras and smartphones, allowing even novice users to capture high-quality images across a wide spectrum of scenes and lighting conditions with minimal manual intervention.10 For professional photographers, while manual controls are often preferred, reliable 3A functions can provide excellent starting points or handle rapidly changing situations. Beyond consumer photography, in fields such as robotics, autonomous vehicles, medical imaging, and surveillance, robust and accurate 3A performance is critical for reliable visual perception, data acquisition, and subsequent decision-making processes.10 The Android camera Hardware Abstraction Layer (HAL), for instance, defines specific state machines and control mechanisms for 3A modes, underscoring their systemic importance in mobile imaging platforms.</p> <h2 id="iii-core-image-signal-processing-stages"><strong>III. Core Image Signal Processing Stages</strong></h2> <h3 id="typical-processing-blocks">Typical Processing Blocks</h3> <p>The <strong>Image Signal Processor (ISP)</strong> is a sophisticated pipeline of interconnected processing modules, realized either in dedicated hardware, software, or a combination thereof. Its fundamental role is to take the raw, often imperfect, data from an image sensor and transform it into a high-quality digital image suitable for human viewing or for interpretation by machine vision algorithms.1 This involves correcting various sensor and lens artifacts, reconstructing color information, reducing noise, and enhancing visual appeal.</p> <p>A typical ISP pipeline, while varying in specific implementation details between manufacturers and applications, generally includes the following categories of processing blocks, often in a sequence similar to this:</p> <ol> <li><strong>Preprocessing Stages:</strong> These operations prepare the raw data for subsequent, more complex processing. <ul> <li><strong>Analog-to-Digital Conversion (ADC):</strong> While often considered part of the sensor unit, the ADC converts the analog voltage from each photosite into a digital value, marking the entry point into the digital domain.3</li> <li><strong>Black Level Correction (or Subtraction):</strong> Image sensors exhibit a “dark current,” producing a small signal even in complete darkness. Black level correction subtracts this baseline offset from the pixel values to ensure true blacks.</li> <li><strong>Defective Pixel Correction (DPC):</strong> Manufacturing imperfections can result in sensor pixels that are stuck (always bright), dead (always dark), or noisy. DPC algorithms identify and interpolate values for these defective pixels using information from their neighbors.</li> <li><strong>Lens Shading Correction (LSC):</strong> Lenses, especially wide-angle ones, can cause non-uniform brightness (vignetting) and color shifts across the image field, typically with corners being darker or having a color cast. LSC applies a spatially varying gain to compensate for these effects.</li> </ul> </li> <li><strong>CFA-Related Processing (Primarily for Bayer sensors):</strong> <ul> <li><strong>Demosaicing (Debayering):</strong> As each pixel in a CFA-based sensor captures only one color, demosaicing algorithms reconstruct a full-color (typically RGB) image by interpolating the missing two color values at each pixel location.1 This is a critical and often computationally intensive step, detailed further below.</li> </ul> </li> <li><strong>Noise Management:</strong> <ul> <li><strong>Denoising and Filtering:</strong> Various types of noise (shot noise, read noise, thermal noise) are inherent in image capture, especially in low-light conditions or at high ISO settings. Denoising algorithms aim to reduce this noise while preserving image detail. This is also discussed in more detail later.</li> </ul> </li> <li><strong>Color and Tone Processing:</strong> <ul> <li><strong>White Balance (AWB):</strong> Corrects for the color cast introduced by the scene’s illuminant, ensuring that white objects appear white and other colors are rendered naturally.</li> <li><strong>Color Correction Matrix (CCM):</strong> Transforms the camera’s native, sensor-specific RGB values into a standard color space (e.g., sRGB, CIE XYZ) for consistent and accurate color reproduction.</li> <li><strong>Gamma Correction:</strong> Applies a non-linear transformation to the pixel intensities to account for the non-linear response of display devices or to match human perceptual characteristics of brightness.</li> <li><strong>Tone Mapping / Dynamic Range Compression (DRC):</strong> If the scene has a high dynamic range (HDR) – a large difference between the darkest and brightest areas – tone mapping compresses this range to fit within the capabilities of a standard display, while attempting to preserve local contrast and detail.</li> </ul> </li> <li><strong>Image Enhancement:</strong> <ul> <li><strong>Sharpening / Edge Enhancement:</strong> Increases the acutance (perceived sharpness) of edges in the image, often by amplifying high-frequency components.</li> <li><strong>Contrast Adjustment:</strong> Modifies the global or local contrast of the image to enhance visual impact.</li> </ul> </li> <li><strong>Output Formatting:</strong> <ul> <li><strong>Color Space Conversion:</strong> May convert the image from an internal processing color space (e.g., linear RGB) to an output color space (e.g., sRGB for display, YUV for video compression).</li> <li><strong>Image Compression:</strong> Often, the final processed image is compressed (e.g., into JPEG or HEIC format) to reduce file size for storage or transmission.</li> <li><strong>Resizing/Scaling:</strong> Adjusts the image dimensions if required.</li> </ul> </li> </ol> <p>The following is a example illustration of the above processes that presents in a seminal paper regarding software ISP<sup id="fnref:SoftISP"><a href="#fn:SoftISP" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, with a visualization of intermediate results after each transformation.</p> <p><img src="https://i.imgur.com/kAKYTeP.jpeg" alt=""/></p> <p>The exact sequence and specific algorithms employed within these blocks can be proprietary and vary significantly. Some ISPs are highly configurable, allowing tuning for different sensors or application requirements. The stages within an ISP are not independent silos; they are highly interconnected. The output quality of one stage directly influences the input and potential effectiveness of all subsequent stages. For example, inadequate black level correction can skew all downstream calculations. Residual noise that isn’t effectively removed by the denoising stage can lead to more pronounced artifacts during demosaicing, as interpolation algorithms might misinterpret noise as detail. Similarly, if demosaicing introduces false colors or blurs edges, the accuracy of white balance and color correction will be compromised, and sharpening algorithms might amplify these unwanted artifacts. This deep interdependence means that optimizing an ISP requires a holistic view.</p> <h3 id="demosaic-debayering">Demosaic (Debayering)</h3> <p>Demosaicing, also commonly referred to as debayering or color reconstruction, is the digital image processing algorithm used to reconstruct a full-color image from the spatially undersampled color samples output by an image sensor overlaid with a CFA, such as the Bayer filter. Demosaicing is a fundamental and computationally critical stage in any ISP pipeline handling raw data from single-sensor CFA cameras. The quality of the demosaicing algorithm profoundly impacts the final image’s color fidelity, perceived resolution, sharpness, and the presence or absence of visual artifacts. A poor demosaicing algorithm can severely degrade image quality, regardless of how well other ISP stages perform. For Bayer sensors, this stage is not merely processing existing information but is effectively “creating” two-thirds of the color data at each pixel site. The accuracy of this “creation” is therefore paramount for all subsequent color-dependent operations in the ISP.</p> <h3 id="denoising-and-filtering">Denoising and Filtering</h3> <p>Image denoising is the process aimed at removing or attenuating noise – random fluctuations in pixel values that are not part of the true scene information – from a digital image. Noise can arise from various sources, including the inherent quantum nature of light (shot noise), thermal agitation in the sensor (thermal noise), electronics in the signal path (read noise), and the quantization process during analog-to-digital conversion. It is often more pronounced in images taken in low-light conditions, which necessitate higher sensor gain or longer exposure times. The fundamental challenge in image denoising lies in effectively distinguishing unwanted noise from meaningful image details such as edges, textures, and fine structures, as all can manifest as high-frequency components. Overly aggressive denoising can lead to a loss of these details, resulting in a blurred or overly smooth, artificial appearance. Conversely, insufficient denoising leaves distracting noise patterns. Common filters for denoising include Gaussian filter, Bilateral filter, Guided filter, etc.</p> <h3 id="color-correction-and-management">Color Correction and Management</h3> <p>Color correction is the process within the ISP that transforms the camera’s native (sensor-specific) RGB color values into a standard, device-independent color space, such as sRGB, Adobe RGB, or the CIE XYZ color space. This transformation is essential for ensuring that colors are rendered accurately and consistently when viewed on different display devices or reproduced in print. The primary goal of color correction is to achieve color fidelity, meaning the colors in the final processed image should appear as close as possible to the true colors of the original scene as perceived by a human observer under a standard illuminant. The raw RGB values generated by an image sensor are inherently device-dependent. This dependency arises from the unique Spectral Sensitivity Functions (SSFs) of the sensor’s color channels (R, G, B) and the characteristics of its Color Filter Array (CFA).</p> <p>The most common method for color correction involves applying a 3x3 linear transformation matrix, known as the Color Correction Matrix (CCM), to the RGB pixel values from the sensor (after white balance).55 If RGBcam​ represents the camera’s RGB vector for a pixel, and RGBstd​ represents the corresponding vector in a standard color space. More complex, non-linear methods, such as polynomial regression or the use of 3D Look-Up Tables (LUTs), can also be employed to achieve higher color accuracy, especially if the camera’s response has significant non-linearities that cannot be adequately modeled by a simple 3x3 matrix. The core aim of color correction, particularly the application of a CCM, is to bridge the significant gap between how a specific image sensor “perceives” color—dictated by its unique SSFs—and how humans perceive color or how standard display devices are designed to render color. It is a fundamental step in transforming device-dependent sensor data into a universally interpretable and consistent color representation. This process is pivotal for achieving predictable and faithful color reproduction, which is essential across a vast range of applications, from casual photo sharing, where pleasing colors are expected, to professional photography, print, and video workflows, where color accuracy is paramount. The often-discussed “color preference” (style/taste) of a particular camera/smartphone brand is heavily influenced by the quality and characteristics of its color correction pipeline and the subsequent aesthetic choices made in color rendering and tone mapping, i.e., photo-finishing.</p> <h2 id="iv-auto-white-balance-awb"><strong>IV. Auto White Balance (AWB)</strong></h2> <p>Auto White Balance (AWB) is a critical 3A function within the ISP, responsible for ensuring that the colors in a digital image are rendered naturally, irrespective of the color temperature of the light source illuminating the scene. It aims to mimic the human visual system’s ability to perceive colors consistently under varying illumination.</p> <h3 id="fundamental-concepts"><strong>Fundamental Concepts</strong></h3> <p><img src="https://i.imgur.com/A4a12iH.jpeg" alt=""/></p> <p><strong>Color Constancy</strong> refers to the remarkable ability of the human visual system (HVS) to perceive the intrinsic color of an object as relatively stable, even when the spectral characteristics of the illuminant (the light source) change significantly. For instance, a white sheet of paper is perceived as white whether viewed outdoors under bluish daylight, indoors under yellowish incandescent light, or under greenish fluorescent light, despite the fact that the actual spectrum of light reflected from the paper to the eye differs dramatically in these conditions. The primary objective of AWB algorithms in digital cameras is to computationally emulate this perceptual phenomenon. AWB strives to analyze the image data, estimate the properties of the scene illuminant, and then apply a correction to the image’s color data. This correction aims to remove the color cast introduced by the illuminant, rendering objects as if they were viewed under a neutral, or “white,” light source. The above figure shows the famous “Grey Strawberries” example.</p> <p><strong>Chromatic adaptation</strong> is the physiological and perceptual process by which the HVS adjusts its sensitivity to different colors of illumination, thereby contributing significantly to achieving color constancy. When exposed to a colored illuminant for a period, our visual system “adapts” by reducing its sensitivity to that color, making the illuminant appear less chromatic and helping object colors remain stable. AWB algorithms often incorporate mathematical models of chromatic adaptation to transform the image colors as captured under the estimated scene illuminant to how they would appear under a predefined reference illuminant (typically a standard daylight illuminant like CIE D65 or D50). This process involves first estimating the scene illuminant’s chromaticity and then applying a chromatic adaptation transform (CAT) to the image data. For example, an algorithm might adjust camera output based on the color of a detected face under an unknown illuminant to match what that face color would be under a standard illuminant, which is a form of chromatic adaptation. Chromatic adaptation is one of the key underlying mechanisms that enables the HVS to achieve color constancy.</p> <p><strong>Neutral Point / White Point</strong> refers to a color that, after appropriate white balancing, should appear achromatic – that is, devoid of any hue, such as pure white, a shade of gray, or black. A core objective of AWB algorithms is to identify the chromaticity of the scene illuminant and then transform the image’s color balance such that objects in the scene that are presumed to be neutral (e.g., a white wall, a gray card) are rendered as neutral in the final image.67 Essentially, the AWB algorithm attempts to map the color of the estimated illuminant itself to a “pure white” or a defined neutral gray in the output color space.</p> <p><img src="https://i.imgur.com/IJREpKO.jpeg" alt=""/></p> <p><strong>Illuminant</strong> is the source of light that illuminates the scene being photographed. Its defining characteristic is its Spectral Power Distribution (SPD), which describes the amount of energy the light source emits at each wavelength across the visible spectrum. The SPD determines the “color” of the light. The color of an object as captured by a camera (or perceived by the eye) is a result of the interaction between the object’s surface reflectance properties (which wavelengths it absorbs and reflects) and the SPD of the illuminant. If the illuminant changes (e.g., from sunlight to incandescent light), the spectral composition of the light reaching the camera sensor from the same object will also change, leading to different raw color signals. <strong>Common Illuminants</strong> with distinct SPDs and color temperatures (measured in Kelvin, K) include:</p> <ul> <li>Daylight: Varies significantly from warm (low K, e.g., sunrise/sunset) to neutral (medium K, e.g., noon sunlight) to cool (high K, e.g., overcast sky, open shade). Standard daylight illuminants like D65 (average daylight, ~6500K) and D50 (horizon light, ~5000K) are often used as references.</li> <li>Incandescent/Tungsten Light: Emits warm, yellowish light (typically around 2700-3200K).</li> <li>Fluorescent Light: Can have a wide range of SPDs and color temperatures, often with strong spectral peaks, making them challenging for AWB.</li> <li>LED Light: Also highly variable SPDs depending on the LED technology, and can be designed to mimic other light sources or produce specific colors.</li> <li>Flash: Typically designed to be close to daylight color temperature (~5500-6000K).</li> </ul> <h3 id="assumptions-and-algorithms">Assumptions and Algorithms</h3> <p><strong>von Kries hypothesis</strong>, proposed by Johannes von Kries in 1902, is a foundational and widely referenced model for explaining and computing chromatic adaptation. It is based on the hypothesis that the human visual system’s adaptation to changes in illumination color occurs through independent sensitivity adjustments (gain controls) in the three types of cone photoreceptors in the retina: Long-wavelength sensitive (L), Medium-wavelength sensitive (M), and Short-wavelength sensitive (S) cones. The model posits that to achieve color constancy (i.e., for an object to appear the same color under different illuminants), the responses of the L, M, and S cones are independently scaled. These scaling factors (or gains) are determined by the relationship between the cone responses to a reference white surface under the current (source) illuminant and a target (reference) illuminant. The idea is that the adapted responses to a white object should be the same regardless of the illuminant. The von Kries transformation typically involves the following steps:</p> <ol> <li><strong>Transformation to Cone Space:</strong> The camera’s RGB values (which are device-dependent) are first transformed into an LMS-like cone response space using a 3x3 linear matrix, \(M_{RGB→LMS}\)​. This matrix aims to approximate the spectral sensitivities of the human cones: \(T=M_{RGB→LMS}​×T\).</li> <li><strong>Gain Calculation:</strong> The cone responses for a reference white surface under the source illuminant (\(L_{src}​,M_{src}​,S_{src}\)​) and the target illuminant (\(L_{tgt}​,M_{tgt}​,S_{tgt}\)​) are determined. The von Kries adaptation coefficients (gains) are then computed as the ratios: \(k_L​=\frac{L_{src}}{​L_{tgt}}​​,k_M​=\frac{M_{src}}{​M_{tgt}}​​,k_S​=\frac{S_{src}}{​_{Sw,tgt}}\).​​</li> <li><strong>Application of Gains:</strong> These gains are applied independently to the LMS values of each pixel in the image: \(L_{adapted}​=k_L​⋅L_{image}​,M_{adapted​}=k_M​⋅M_{image}​,S_{adapted}​=k_S​⋅S_{image}\).​</li> <li><strong>Transformation to Output Space:</strong> The adapted LMS values are then transformed back to an output RGB color space using another 3x3 matrix, \(M_{LMS→RGB}\).</li> </ol> <p>The von Kries model provides a simple yet powerful conceptual framework for chromatic adaptation. It forms the theoretical basis for many modern Chromatic Adaptation Transforms (CATs) used in color science, color management, and AWB algorithms, such as CAT02 and CAT16. These often incorporate refined transformation matrices to LMS-like spaces, non-linearities, or different ways of calculating the adaptation gains to better predict experimental data on corresponding colors (how colors appear to match under different illuminants). The term “Wrong von Kries” is sometimes used to describe the application of similar diagonal scaling directly in an RGB color space that is not well-aligned with the LMS cone fundamentals, which can be effective despite wrong.</p> <p><strong>Auto White Balancing (AWB) algorithms</strong> aim to automatically estimate the chromaticity of the dominant illuminant in a scene and then apply a color correction to the image data to neutralize any color cast caused by that illuminant, making white objects appear white and other colors appear natural. Most AWB algorithms can be conceptually divided into two main stages:</p> <ol> <li><strong>Illuminant Estimation:</strong> This stage analyzes the image data (and sometimes metadata) to determine the color (chromaticity) of the light source(s) illuminating the scene.</li> <li><strong>Color Correction (Chromatic Adaptation):</strong> Once the illuminant is estimated, a transformation (often a von Kries type scaling or a more advanced CAT) is applied to the image’s R, G, and B values to shift the colors as if the scene were lit by a reference neutral illuminant.</li> </ol> <p><strong>Common statistical Approaches for Illuminant Estimation:</strong></p> <ul> <li><strong>Gray World Algorithm:</strong> This classic algorithm is based on the assumption that, on average, the reflectances of all surfaces in a typical, diverse scene will average out to a neutral gray.</li> <li><strong>White Patch Algorithm (Max-RGB or Perfect Reflector):</strong> This approach assumes that the brightest pixel (or a small patch of the brightest pixels) in the image corresponds to a perfectly white surface or a specular highlight that directly reflects the color of the illuminant.</li> <li><strong>Gray-Edge / Edge-Based Algorithms:</strong> These methods operate on the assumption that the average color of differences (gradients) across edges in an image tends towards the illuminant color, or that edges themselves provide robust cues.</li> <li><strong>Gamut Mapping / Color by Correlation / Gamut Constraint Algorithms:</strong> These algorithms work with the distribution of colors (the gamut) present in the image. The underlying idea is that the scene illuminant restricts the range of colors that can be observed from a set of surfaces. By comparing the gamut of colors in the captured image to a set of canonical gamuts (pre-calculated or learned for various known illuminants), the algorithm can infer the most likely scene illuminant. This often involves finding the illuminant under which the image’s color distribution “fits best” within the expected boundaries.</li> </ul> <h2 id="v-auto-focus-af"><strong>V. Auto Focus (AF)</strong></h2> <p>Auto Focus (AF) is the 3A system component responsible for automatically adjusting the camera’s lens to ensure that the desired subject in the scene appears sharp and clear in the captured image or video. Modern AF systems employ a variety of technologies and techniques to achieve speed, accuracy, and reliability across diverse shooting conditions<sup id="fnref:AF"><a href="#fn:AF" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>.</p> <h3 id="active-af-and-passive-af">Active AF and Passive AF</h3> <p><strong>Active AF</strong> systems operate by emitting a beam of energy – typically infrared light or ultrasonic sound waves – towards the scene. The system then measures the reflected energy or the time it takes for the emitted energy to travel to the subject and return to the camera.82 Based on this measurement, the camera calculates the distance to the subject and adjusts the lens position accordingly to achieve focus. Examples of Active AF include:</p> <ul> <li><strong>Infrared (IR) AF:</strong> This method uses an infrared LED to project a beam of IR light.</li> <li><strong>Ultrasonic AF:</strong> This system emits ultrasonic sound waves and measures the time taken for the echo to return from the subject, much like sonar. This was famously used in Polaroid instant cameras.</li> </ul> <p>The primary advantage of active AF is its ability to function effectively in very low light conditions or even in complete darkness, where passive systems (which rely on analyzing scene light) often struggle or fail. Active AF systems have a limited effective range (e.g., infrared systems are often effective up to around 6 meters or 20 feet). Their performance can be compromised if the subject is non-reflective or highly absorbent to the emitted energy (e.g., black fur might absorb IR light). The emitted beam can also be blocked by intervening objects (like cage bars or a fence) or reflect off surfaces like glass, preventing focus on the intended subject beyond it. The emitted signal itself might be undesirable or distracting in certain environments. While less common as the primary AF mechanism in modern high-performance digital cameras, the principle of active ranging remains relevant in AF-assist lamps and, more significantly, in the dedicated depth-sensing technologies like Time-of-Flight (ToF) sensors that are increasingly used to augment passive AF systems.</p> <p><strong>Passive AF</strong> systems do not emit their own energy source for ranging. Instead, they analyze the characteristics of the light from the scene that passes through the camera lens and falls onto the image sensor (or a dedicated AF sensor) to determine the state of focus. The two main passive AF techniques are <strong>Contrast Detection Auto Focus (CDAF)</strong> and <strong>Phase Detection Auto Focus (PDAF)</strong>. Passive AF systems can focus on subjects at virtually any distance, as they are not limited by the range of an emitted signal. They are generally more versatile for a wider array of subjects and shooting conditions, provided there is sufficient ambient light and some level of contrast or detail in the subject. Their performance typically degrades in very low light levels or when attempting to focus on subjects with very low contrast (e.g., a plain, uniformly lit wall or a clear blue sky) because they rely on analyzing image features. Passive AF is the dominant autofocus technology in today’s digital SLRs (DSLRs), mirrorless cameras, and advanced smartphone cameras, with on-sensor PDAF variants being particularly prevalent due to their excellent balance of speed, accuracy, and tracking performance.</p> <h3 id="common-af-techniques">Common AF Techniques</h3> <p><img src="https://i.imgur.com/N1DgSj9.jpeg" alt=""/></p> <p><strong>Contrast Detection Auto Focus (CDAF)</strong> operates on the fundamental optical principle that an image is sharpest (i.e., in focus) when the contrast between adjacent pixels or regions is maximized. An out-of-focus image appears blurry because fine details and edges (which represent high-frequency information) are softened, reducing local contrast. The camera’s image processor analyzes the image data directly from the main image sensor (or a portion of it corresponding to the selected AF point). It instructs the lens to make a series of small incremental movements through its focusing range – a process often described as “hunting” or “scanning”. At each lens position, the processor measures the contrast in the designated AF region. This contrast measurement can be based on various metrics, such as the intensity difference between adjacent pixels, the amplitude of high-frequency components derived from a spatial filter (e.g., a Laplacian or Sobel filter), or other sharpness metrics. The lens continues to move, and the system tracks the contrast values. The position at which the measured contrast reaches its peak is considered the point of optimal focus.89 The lens will typically move slightly past this peak and then return to it to confirm.</p> <p><img src="https://i.imgur.com/3XYIawB.jpeg" alt=""/></p> <p>CDAF has high potential accuracy due to direct image sensor feedback and is relatively simpler to implement in terms of sensor hardware compared to dedicated PDAF sensor modules, as it can leverage the main imaging sensor.<br/> But, CDAF is generally slower than PDAF. The iterative hunting process – moving the lens back and forth to find the contrast peak – takes time. It has no inherent knowledge of <em>which direction</em> to move the lens initially or <em>how far</em> to move it, so it must search. This makes it less suitable for tracking fast-moving subjects. Performance degrades significantly in low light and with low-contrast subjects, as there may not be enough contrast variation for the system to reliably detect a peak.</p> <p><strong>Phase Detection Auto Focus (PDAF)</strong> determines focus by analyzing and comparing two (or more) separate images of the subject that are formed by light rays passing through different parts of the camera lens aperture – typically from opposite sides of the lens. In a DSLR camera, when the mirror is down (for viewfinder use), a portion of the light coming through the lens is reflected by a secondary mirror down to a dedicated PDAF sensor module, usually located in the base of the camera body. This module contains an array of paired microlenses and small line sensors (often CCDs). Each pair of microlenses splits the incoming light from a specific region of the lens into two distinct beams, which then fall onto the corresponding pair of line sensors. If the subject is perfectly in focus, these two light beams (and the patterns they form on the line sensors) will converge and align precisely. If the subject is out of focus, the two beams will be misaligned – they will be “out of phase.” The amount of this misalignment (the phase difference) and the direction of the misalignment (e.g., whether one beam is shifted left or right relative to the other) directly indicate not only that the image is out of focus, but critically, <em>how far</em> the lens is from the correct focus position and <em>in which direction</em> (towards infinity or towards minimum focus distance) the lens needs to be moved to achieve focus. The AF system can then drive the lens directly to the calculated focus position.</p> <p>Since mirrorless cameras and smartphones lack the mirror box and dedicated PDAF sensor of a DSLR, PDAF functionality is implemented directly on the main imaging sensor (On-Sensor PDAF). This is achieved by modifying a certain percentage of the sensor’s pixels to act as phase detection sites. There are various ways to do this:<br/> - <strong>Masked Pixels:</strong> Some pixels are partially masked so that they only receive light from one side (e.g., the left or right half) of the lens aperture. By comparing the signals from a “left-looking” masked pixel and a nearby “right-looking” masked pixel, a phase difference can be determined.<br/> - <strong>Split Photodiodes (Dual Pixel AF is an advanced example):</strong> Each PDAF pixel (or in the case of Dual Pixel AF, nearly every pixel) is designed with two separate photodiodes. Each photodiode effectively sees the scene through a slightly different portion of the lens pupil. The signals from these two photodiodes are compared to detect phase differences.</p> <p><img src="https://i.imgur.com/MykupAB.jpeg" alt=""/></p> <p>The key advantage of PDAF is its speed. Because it can determine both the direction and magnitude of defocus from a single measurement (without iterative hunting), it allows the lens to be driven quickly and decisively to the correct focus point. This makes PDAF particularly effective for capturing action and tracking moving subjects. For traditional DSLR PDAF, the dedicated PDAF sensor must be perfectly aligned with the main image sensor. Any misalignment can lead to systematic front-focus or back-focus errors (where the lens consistently focuses slightly in front of or behind the intended subject), which may require lens/camera calibration (micro-adjustment).92 The number and coverage of AF points are typically limited to a central area of the frame. The system adds cost and complexity. For On-Sensor PDAF, when some sensor pixels are dedicated (or partially dedicated) to AF, their light-gathering capability for imaging might be reduced, potentially requiring interpolation from neighboring pixels to fill in image data (though this is often managed to be visually negligible in modern sensors). On-sensor PDAF can be more susceptible to noise in very low light conditions compared to dedicated PDAF sensors with larger photosites. Certain subject patterns (e.g., fine horizontal lines if the PDAF pixels are primarily sensitive to vertical phase differences) can sometimes pose challenges for on-sensor PDAF systems, though cross-type PDAF pixels (sensitive in two orientations) help mitigate this.</p> <h2 id="vi-auto-exposure-ae--metering"><strong>VI. Auto Exposure (AE) / Metering</strong></h2> <p>Auto Exposure (AE) is the third pillar of the 3A system, working to ensure that images are captured with an appropriate level of brightness, avoiding overexposure (loss of detail in highlights) or underexposure (loss of detail in shadows).</p> <p><img src="https://i.imgur.com/ihowMyb.jpeg" alt=""/></p> <p>This balance of light is controlled by three fundamental pillars: aperture, shutter speed, and ISO. Collectively known as the exposure triangle, these three elements work in concert to determine the brightness of an image. The core process underlying AE is metering.</p> <p><strong>Metering</strong> in photography is the process by which a camera’s internal light meter measures the intensity of light in a scene to determine the optimal exposure settings.10 These settings primarily include the <strong>shutter speed</strong> (duration the sensor is exposed to light), <strong>lens aperture</strong> (the size of the opening allowing light to pass through the lens), and <strong>ISO sensitivity</strong> (the sensor’s amplification of the light signal). The AE system uses the information gathered from metering to automatically adjust one or more of these parameters to achieve what it deems a “correct” exposure. Most in-camera light meters are designed to measure <strong>reflected light</strong> – that is, the light that bounces off the subject(s) and other elements within the scene and then enters the camera lens to strike the sensor (or a dedicated metering sensor). This is distinct from incident light meters (often handheld), which measure the light falling onto the subject.</p> <p><strong>The 18% Gray Assumption (Middle Gray)</strong> is a crucial concept in understanding how most camera meters work is the <strong>18% gray assumption</strong> (also known as “middle gray” or “Zone V” in the Zone System).115 Camera meters are typically calibrated to interpret the scene they are measuring as if, on average, it reflects 18% of the incident light. The exposure settings are then calculated to render this average scene brightness as a medium gray tone in the final image. This standard helps in achieving consistent exposure across a variety of typical scenes. However, this assumption is also the primary reason why automatic exposure can sometimes be “fooled.”</p> <ul> <li>If a scene is predominantly very bright (e.g., a snowy landscape, a subject against a white background), the meter, trying to make this bright scene average out to 18% gray, will tend to underexpose the image, making the snow or white background appear grayish.</li> <li>Conversely, if a scene is predominantly very dark (e.g., a black cat on a dark rug, a subject against a black background), the meter, again trying to force this dark average to 18% gray, will tend to overexpose the image, making the black subject or background appear too light or washed out. Experienced photographers often use exposure compensation to manually override the camera’s metered exposure in such situations to achieve the desired result. The histogram display on digital cameras is an invaluable tool for objectively assessing exposure, showing the distribution of tones from black to white and helping to identify potential underexposure (graph bunched to the left, “clipped shadows”) or overexposure (graph bunched to the right, “blown highlights”).</li> </ul> <p>The metering process effectively measures scene luminance (brightness). Exposure is measured in units of lux-seconds, and can be related to an <strong>Exposure Value (EV)</strong>, which combines shutter speed and aperture into a single number representing a given level of exposure. A change of 1 EV corresponds to a doubling or halving of the amount of light reaching the sensor (a one-stop change).</p> <h2 id="vii-trends"><strong>VII. Trends</strong></h2> <p><strong>Automatic ISP Tuning.</strong> The intricate nature of ISP tuning, which involves adjusting numerous interdependent parameters across various processing blocks, has traditionally been a labor-intensive and time-consuming task performed by highly skilled engineers. This complexity, where the ISP can often resemble a “black box,” has spurred significant research into <strong>AI-driven 3A/ISP tuning</strong>. Such approaches aim to automatically optimize ISP parameters, or even the pipeline structure itself, for specific imaging tasks (e.g., enhancing object detection performance for a machine vision system) or for improved perceptual image quality.</p> <p><strong>AI-ISP and ISP for Machine Vision.</strong> Another trend is the application of data driven approach to certain ISP stages, e.g., denoising, tone mapping, and photo-finishing. Beyond better matching human vision, the AI accelerator on smartphones, or even sensor itself (e.g., IMX500<sup id="fnref:IMX500"><a href="#fn:IMX500" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>), empower the deployment of advanced recognition models to further provide inteligence sensing ability for the device throught the camera eye. This trend towards intelligent and adaptive ISPs promises to further enhance image quality and democratize access to advanced imaging capabilities.</p> <p><strong>References</strong></p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:MSBrown"> <p><a href="https://www.cse.yorku.ca/~mbrown/">Michael S. Brown</a> <a href="#fnref:MSBrown" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:ICCVTutorial"> <p><a href="https://www.eecs.yorku.ca/~mbrown/ICCV19_Tutorial_MSBrown.pdf">Understanding color &amp; the in-camera image processing pipeline for computer vision</a>, ICCV 2019 Tutorial <a href="#fnref:ICCVTutorial" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:SoftISP"> <p><a href="https://karaimer.github.io/camera-pipeline/">A Software Platform for Manipulating the Camera Imaging Pipeline</a>, in ECCV 2016 <a href="#fnref:SoftISP" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:AF"> <p><a href="https://www.imaginated.com/photography/focusing/focus-modes/autofocus/">Understanding Autofocus in Photography - Imaginated</a> <a href="#fnref:AF" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:IMX500"> <p><a href="https://developer.sony.com/imx500">IMX500 - Sony</a> <a href="#fnref:IMX500" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="all"/><category term="computational-photography"/><summary type="html"><![CDATA[A Comprehensive Guide to Understanding Image Signal Processing and Automatic Exposure, Focus, and White Balance Adjustment]]></summary></entry><entry><title type="html">Restoration &amp;amp; Generation: A Correspondence Perspective</title><link href="https://ddlee-cn.github.io/blog/2021/Correspondence-Paper/" rel="alternate" type="text/html" title="Restoration &amp;amp; Generation: A Correspondence Perspective"/><published>2021-07-25T00:00:00+00:00</published><updated>2021-07-25T00:00:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2021/Correspondence-Paper</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2021/Correspondence-Paper/"><![CDATA[<h2 id="visual-correspondence-sparse-or-dense">Visual Correspondence: Sparse or Dense</h2> <ul> <li>Cross-view: sparse correspondence</li> </ul> <p><img src="https://i.imgur.com/gOp5qGS.jpeg" alt="16495958031466"/></p> <ul> <li>Adjacent frame: dense optical flow</li> </ul> <p><img src="https://i.imgur.com/fbEYY5s.jpeg" alt="16495958142885"/></p> <ul> <li>Cross-domain semantic correspondence</li> </ul> <p><img src="https://i.imgur.com/EaDIJ2X.jpeg" alt="16495958251179"/></p> <p>(Neural Best-buddies, Aberman et al. in ACM SIGGRAPH 2018)</p> <h2 id="why-to-incorporate-additional-inputs-use-correspondence">Why to incorporate additional inputs/ use correspondence</h2> <h3 id="make-the-task-easier">Make the Task Easier</h3> <p><img src="https://i.imgur.com/Rl65jHW.jpeg" alt="16495959533428"/></p> <p>(Robust flash deblurring, Zhuo et al. in CVPR 2010)</p> <p><img src="https://i.imgur.com/IOHLIz1.jpeg" alt="16495959652573"/></p> <p>(Scene Completion Using Millions of Photographs, Hays et al. in ACM SIGGRAPH 2007)</p> <h3 id="control-the-results">Control the Results</h3> <p><img src="https://i.imgur.com/DFB0Yu0.jpeg" alt="16495959948564"/></p> <p>(Visual Attribute Transfer through Deep Image Analogy, Liao et al. in ACM SIGGRAPH 2017)</p> <p><img src="https://i.imgur.com/mL4LhQw.jpeg" alt="16495960065440"/></p> <p>(Cross-domain Correspondence Learning for Exemplar-based Image Translation, Zhang et al. in CVPR 2020)</p> <h3 id="naturally-availableinevitable">Naturally Available/Inevitable</h3> <p><img src="https://i.imgur.com/BatQli9.jpeg" alt="16495960341401"/></p> <p>(Across Scales &amp; Across Dimensions: Temporal Super-Resolution using Deep Internal Learning, Zuckerman et al. in ECCV 2020)</p> <p><img src="https://i.imgur.com/EwVL6U7.jpeg" alt="16495960395746"/></p> <p>(Light Field Super-Resolution with Zero-Shot Learning, Cheng et al. in CVPR 2021)</p> <h2 id="how-to-incorporate-correspondence">How to incorporate correspondence?</h2> <p><img src="https://i.imgur.com/OQUTyK1.jpeg" alt="16495969370613"/></p> <ul> <li>Implicit Usage(latent code)</li> <li>Explicit Usage(warp field)</li> <li>Multimodality</li> </ul> <h2 id="paper-1-swapping-autoencoder-for-deep-image-manipulation--neurips-2020">Paper #1: Swapping Autoencoder for Deep Image Manipulation – NeurIPS 2020</h2> <p><img src="https://i.imgur.com/gbVyzF5.jpeg" alt="16495970300334"/></p> <p>Target: Controllable Image Manipulation Conditional Generation needs additional prior(edge/layout) Finding semantically meaningful latent code is non-trivial</p> <p>Key idea: image swapping as a pretext task</p> <h3 id="approach">Approach</h3> <ul> <li>Structure-texture disentangled embedding space</li> <li>Co-occurrence patch discriminator</li> </ul> <p><img src="https://i.imgur.com/Ze4VkhL.jpeg" alt="16495970499321"/></p> <ul> <li>Style Latent code &amp; Modulation</li> </ul> <p><img src="https://i.imgur.com/JleAjps.jpeg" alt="16495970705304"/></p> <ul> <li>Co-occurrence Patch discriminator</li> </ul> <p><img src="https://i.imgur.com/USxWaQn.jpeg" alt="16495971667609"/></p> <h3 id="background-of-modulation-how-to-interact-between-two-latent-spaceintegrate-conditional-signal">Background of Modulation: How to interact between two latent space/integrate conditional signal?</h3> <p><img src="https://i.imgur.com/bC1vaMx.jpeg" alt="16495971094565"/></p> <p>Origin:</p> <p>Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization, Huang et al. ICCV 2017</p> <p>FiLM: Visual Reasoning with a General Conditioning Layer, Perez et al. AAAI 2018</p> <p>Application:</p> <p>Recovering Realistic Texture in Image Super-resolution byDeep Spatial Feature Transform, Wang et al. CVPR 2018</p> <p>Guided Image-to-Image Translation with Bi-Directional Feature Transformation, AlBahar and Huang, ICCV 2019</p> <p>SEAN: Image Synthesis with Semantic Region-Adaptive Normalization, Zhu et al. CVPR 2020</p> <p>Analyzing and Improving the Image Quality of StyleGAN, Karras et al. CVPR 2020</p> <p>Adaptive Convolutions for Structure-Aware Style Transfer, Chandran et al. CVPR 2021</p> <h3 id="results">Results</h3> <p><img src="https://i.imgur.com/OUIB0TA.jpeg" alt="16495972057514"/></p> <h3 id="summary">Summary</h3> <ul> <li>New application with an old technique</li> <li>Cooperation with other designs</li> </ul> <h2 id="paper-2-adaptive-convolutions-for-structure-aware-style-transfer-chandran-et-al-cvpr-2021">Paper #2: Adaptive Convolutions for Structure-Aware Style Transfer, Chandran et al. CVPR 2021</h2> <p><img src="https://i.imgur.com/IGYLc00.jpeg" alt="16495981647384"/></p> <p>Target: Few-shot Synthesis(Hard to train or adapt) Key idea: Transfer relationships/similarities</p> <h3 id="approach-1">Approach</h3> <p><img src="https://i.imgur.com/a9WBrhv.jpeg" alt="16495981793319"/></p> <p>Pairwise similarity Constraint</p> <p><img src="https://i.imgur.com/I2pqP5v.jpeg" alt="16495982159180"/></p> <p>(1) Cross-domain consistency loss L_dist aims to preserve the relative pairwise distances between source and target generations. In this case, the relative similarities between synthesized images from z_0 and other latent codes are encouraged to be similar. (2) Relaxed realism is implemented by using two discriminators, D_img for noise sampled from the anchor region (z_anch) and D_patch otherwise.</p> <h2 id="takeaways">Takeaways</h2> <ul> <li>What? Sparse vs. Dense; RGB to Semantics to Cross-Modality</li> <li>Why? Easier/Controllable/Available/Inevitable</li> <li>How? Explicit to Implicit</li> <li>Implicit Usage</li> <li> <ul> <li>Latent code + Modulation/Normalization</li> </ul> </li> <li> <ul> <li>Constraints on “2-order” relationships</li> </ul> </li> <li> <ul> <li>Weighting/Attention Mechanism and more</li> </ul> </li> </ul> <h2 id="more-papers">More papers</h2> <p>StyleGAN2 Distillation for Feed-forward Image Manipulation, Viazovetskyi et al. ECCV20</p> <p>Controlling Style and Semantics in Weakly-Supervised Image Generation, Pavllo et al. ECCV 2020</p> <p>COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder, Saito et al. ECCV 2020</p> <p>Example-Guided Image Synthesis using Masked Spatial-Channel Attention and Self-Supervision, Zheng et la. ECCV 2020</p> <p>Online Exemplar Fine-Tuning for Image-to-Image Translation, Kang et al. ArXiv 2020</p> <p>Swapping Autoencoder for Deep Image Manipulation, Park et al. NeurIPS 2020</p> <p>Conditional Generative Modeling via Learning the Latent Space, Ramasinghe et al. ICLR 2021</p> <p>Semantic Layout Manipulation with High-Resolution Sparse Attention, Zheng et al. CVPR 2021</p> <p>Spatially-Invariant Style-Codes Controlled Makeup Transfer, Deng et al. CVPR 2021</p> <p>Adaptive Convolutions for Structure-Aware Style Transfer, Chandran et al. CVPR 2021</p> <p>ReMix: Towards Image-to-Image Translation with Limited Data, Cao et al. CVPR 2021</p> <p>Learning Semantic Person Image Generation by Region-Adaptive Normalization, Lv et al. CVPR 2021 Spatially-Adaptive Pixelwise Networks for Fast Image Translation, Shaham et al. CVPR 2021</p> <p>Bi-level Feature Alignment for Versatile Image Translation and Manipulation, Zhan et al. ArXiv 2021</p> <p>Controllable Person Image Synthesis with Spatially-Adaptive Warped Normalization, Zhang et al. ArXiv 2021</p> <p>Sketch Your Own GAN, Wang et al. ICCV 2021</p>]]></content><author><name></name></author><category term="all"/><category term="rendering-generative-ai"/><category term="paper-reading"/><summary type="html"><![CDATA[Slides for paper reading topic from a correspondence perspective, focusing on restoration and generation tasks in computer vision.]]></summary></entry><entry><title type="html">Anchor-Free Object Detection</title><link href="https://ddlee-cn.github.io/blog/2020/Anchor-Free-Object-Detection/" rel="alternate" type="text/html" title="Anchor-Free Object Detection"/><published>2020-04-12T00:00:00+00:00</published><updated>2020-04-12T00:00:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2020/Anchor-Free-Object-Detection</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2020/Anchor-Free-Object-Detection/"><![CDATA[<h2 id="cornernet-detecting-objects-as-paired-keypoints---eccv-2018"><strong>CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018</strong></h2> <p><img src="https://i.imgur.com/npESC4v.jpg" alt="CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018"/></p> <p>The model detect an object as a pair of bounding box corners grouped together. A convolutional network outputs a heatmap for all top-left corners, a heatmap for all bottom-right corners, and an embedding vector for each detected corner. The network is trained to predict similar embeddings for corners that belong to the same object.</p> <p><img src="https://i.imgur.com/wHrOzEW.jpg" alt="CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018"/></p> <p>The backbone network is followed by two prediction modules, one for the top-left corners and the other for the bottom-right corners. Using the predictions from both modules, we locate and group the corners.</p> <p><img src="https://i.imgur.com/YyHEujb.jpg" alt="CornerNet: Detecting Objects as Paired Keypoints - ECCV 2018"/></p> <p><strong>Code</strong></p> <p><a href="https://github.com/princeton-vl/CornerNet">PyTorch</a></p> <h2 id="cornernet-lite-efficient-keypoint-based-object-detection"><strong>CornerNet-Lite: Efficient Keypoint Based Object Detection</strong></h2> <p>CornerNet-Saccade speeds up inference by reducing the number of pixels to process. It uses an attention mechanism similar to saccades in human vision. It starts with a downsized full image and generates an attention map, which is then zoomed in on and processed further by the model. This differs from the original CornerNet in that it is applied fully convolutionally across multiple scales.</p> <p><img src="https://i.imgur.com/Zzu1wRl.jpg" alt="CornerNet-Lite: Efficient Keypoint Based Object Detection"/></p> <p>We predict a set of possible object locations from the attention maps and bounding boxes generated on a downsized full image. We zoom into each location and crop a small region around that location. Then we detect objects in each region. We control the efficiency by ranking the object locations and choosing top k locations to process. Finally, we merge the detections by NMS.</p> <p><img src="https://i.imgur.com/DTkcDKz.png" alt="CornerNet-Lite: Efficient Keypoint Based Object Detection"/></p> <p>CornerNet-Squeeze speeds up inference by reducing the amount of processing per pixel. It incorporates ideas from SqueezeNet and MobileNets, and introduces a new, compact hourglass backbone that makes extensive use of 1×1 convolution, bottleneck layer, and depth-wise separable convolution.</p> <p><img src="https://i.imgur.com/iXqDI10.png" alt="CornerNet-Lite: Efficient Keypoint Based Object Detection"/></p> <h2 id="extremenetbottom-up-object-detection-by-grouping-extreme-and-center-points---cvpr-2019"><strong>(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019</strong></h2> <p><img src="https://i.imgur.com/AsbOUJw.png" alt="(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019"/></p> <p>In this paper, we propose ExtremeNet, a bottom-up object detection framework that detects four extreme points (top-most, left-most, bottom-most, right-most) of an object. We use a state-of-the-art keypoint estimation framework to find extreme points, by predicting four multi-peak heatmaps for each object category. In addition, we use one heatmap per category predicting the object center, as the average of two bounding box edges in both the x and y dimension. We group extreme points into objects with a purely geometry-based approach. We group four extreme points, one from each map, if and only if their geometric center is predicted in the center heatmap with a score higher than a pre-defined threshold. We enumerate all $O(n^4)$combinations of extreme point prediction, and select the valid ones.</p> <p><img src="https://i.imgur.com/Ek7WhEw.png" alt="(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019"/></p> <p>Given four extreme points t, b, r, l extracted from heatmaps Ŷ (t) , Ŷ (l) , Ŷ (b) , Ŷ (r), we compute their geometric center $c=\left(\frac{l_{x}+t_{x}}{2}, \frac{t_{y}+b_{y}}{2}\right)$. If this center is predicted2 2with a high response in the center map Ŷ (c), we commit the extreme points as a valid detection: Ŷcx ,cy ≥ τc for a threshold τc. We then enumerate over all quadruples of keypoints t, b, r, l in a brute force manner. We extract detections foreach class independently.</p> <p><img src="https://i.imgur.com/TYhe4Al.png" alt="(ExtremeNet)Bottom-up Object Detection by Grouping Extreme and Center Points - CVPR 2019"/></p> <p><strong>Code</strong></p> <p><a href="https://github.com/xingyizhou/ExtremeNet">PyTorch</a></p> <h2 id="centernet-dcenternet-keypoint-triplets-for-object-detection---iccv-2019"><strong>(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019</strong></h2> <p><img src="https://i.imgur.com/6pFsCoQ.png" alt="(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019"/></p> <p>In this paper, we present a low-cost yet effective solution named CenterNet, which explores the central part of a proposal, i.e., the region that is close to the geometric center of a box, with one extra keypoint. We intuit that if a predicted bounding box has a high IoU with the ground-truth box, then the probability that the center keypoint in the central region of the bounding box will be predicted as the same class is high, and vice versa. Thus, during inference, after a proposal is generated as a pair of corner keypoints, we determine if the proposal is indeed an object by checking if there is a center keypoint of the same class falling within its central region.</p> <p><img src="https://i.imgur.com/MFkqXVY.jpg" alt="(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019"/></p> <p>A convolutional backboneoutput two corner heatmaps and a center keypoint heatmap, respectively.the similar embeddings are used to detect a potential boundingthe final bounding boxes. network applies cascade corner pooling and center pooling to Similar to CornerNet, a pair of detected corners and box. Then the detected center keypoints are used to determine the final bounding boxes.</p> <p><img src="https://i.imgur.com/ozuZHNW.jpg" alt="(CenterNet-D)CenterNet: Keypoint Triplets for Object Detection - ICCV 2019"/></p> <p><strong>Code</strong></p> <p><a href="https://github.com/Duankaiwen/CenterNet">PyTorch</a></p> <h2 id="centernet-zobjects-as-points"><strong>(CenterNet-Z)Objects as Points</strong></h2> <p><img src="https://i.imgur.com/OCXfp70.png" alt="(CenterNet-Z)Objects as Points"/></p> <p>We represent objects by a single point at their bounding box center. Other properties, such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Object detection is then a standard keypoint estimation problem. We simply feed the input image to a fully convolutional network that generates a heatmap. Peaks in this heatmap correspond to object centers. Image features at each peak predict the objects bounding box height and weight. The model trains using standard dense supervised learning. Inference is a single network forward-pass, without non-maximal suppression for post-processing.</p> <p><img src="https://i.imgur.com/qwbBPGi.png" alt="(CenterNet-Z)Objects as Points"/></p> <p>The numbers in the boxes represent the stride to the image. (a): Hourglass Network, We use it as is in CornerNet.. (b): ResNet with transpose convolutions. We add one 3 × 3 deformable convolutional layer before each up-sampling layer. Specifically, we first use deformable convolution to change the channels and then use transposed convolution to upsample the feature map (such two steps are shown separately in 32 → 16. We show these two steps together as a dashed arrow for 16 → 8 and 8 → 4). (c): The original DLA-34 for semantic segmentation. (d): Our modified DLA-34. We add more skip connections from the bottom layers and upgrade every convolutional layer in upsampling stages to deformable convolutional layer.</p> <p><img src="https://i.imgur.com/GA6wNG7.png" alt="(CenterNet-Z)Objects as Points"/></p> <p><strong>Code</strong></p> <p><a href="https://github.com/xingyizhou/CenterNet">PyTorch</a></p> <h2 id="fsaffeature-selective-anchor-free-module-for-single-shot-object-detection---cvpr-2019"><strong>(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019</strong></h2> <p><img src="https://i.imgur.com/ObEG4iU.png" alt="(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019"/></p> <p>The motivation is to let each instance select the best level of feature freely to optimize the network, so there should be no anchor boxes to constrain the feature selection in our module. Instead, we encode the instances in an anchor-free manner to learn the parameters for classification and regression. An anchor-free branch is built per level of feature pyramid, independent to the anchor-based branch. Similar to the anchor-based branch, it consists of a classification subnet and a regression subnet. An instance can be assigned to arbitrary level of the anchor-free branch. During training, we dynamically select the most suitable level of feature for each instance based on the instance content instead of just the size of instance box. The selected level of feature then learns to detect the assigned instances. At inference, the FSAF module can run independently or jointly with anchor-based branches.</p> <p><img src="https://i.imgur.com/rspa42e.png" alt="(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019"/></p> <p>The FSAF module only introduces two additional conv layers (dashed feature maps) per pyramid level, keeping the architecture fully convolutional.</p> <p><img src="https://i.imgur.com/3tsiCZG.png" alt="(FSAF)Feature Selective Anchor-Free Module for Single-Shot Object Detection - CVPR 2019"/></p> <p>Online feature selection mechanism. Each instance is passing through all levels of anchor-free branches to compute the averaged classification (focal) loss and regression (IoU) loss over effective regions. Then the level with minimal summation of two losses is selected to set up the supervision signals for that instance.</p> <p><img src="https://i.imgur.com/GmH06Q0.png" alt=""/></p> <h2 id="foveabox-beyond-anchor-based-object-detector"><strong>FoveaBox: Beyond Anchor-based Object Detector</strong></h2> <p><img src="https://i.imgur.com/Y5K4Uan.png" alt="FoveaBox: Beyond Anchor-based Object Detector"/></p> <p>FoveaBox is motivated from the fovea of human eyes: the center of the vision field (object) is with the highest visual acuity. FoveaBox jointly predicts the locations where the object’s center area is likely to exist as well as the bounding box at each valid location. Thanks to the feature pyramidal representations, different scales of objects are naturally detected from multiple levels of features.</p> <p><img src="https://i.imgur.com/nTougKV.png" alt="FoveaBox: Beyond Anchor-based Object Detector"/></p> <p>The design of the architecture follows RetinaNet to make a fair comparison. FoveaBox uses a Feature Pyramid Network backbone on top of a feedforward ResNet architecture. To this backbone, FoveaBox attaches two subnetworks, one for classifying the corresponding cells and one for predict the (x1 , y1 , x2 , y2 ) of ground-truth object boxes. For each spatial output location, the FoveaBox predicts one score output for each class and the corresponding 4-dimensional box, which is different from previous works attaching A anchors in each position (usually A = 9).</p> <p><img src="https://i.imgur.com/ea6VsgH.png" alt="FoveaBox: Beyond Anchor-based Object Detector"/></p> <p><strong>Code</strong></p> <p><a href="https://github.com/taokong/FoveaBox">PyTorch</a></p> <h2 id="fcos-fully-convolutional-one-stage-object-detection---iccv-2019"><strong>FCOS: Fully Convolutional One-Stage Object Detection - ICCV 2019</strong></h2> <p>In order to suppress these low-quality detections, we introduce a novel “center-ness” branch (only one layer) to predict the deviation of a pixel to the center of its corresponding bounding box. This score is then used to down-weight low-quality detected bounding boxes and merge the detection results in NMS. The simple yet effective center-ness branch allows the FCN-based detector to outperform anchor-based counterparts under exactly the same training and testing settings.</p> <p><img src="https://i.imgur.com/aKH5xYC.png" alt="FCOS: Fully Convolutional One-Stage Object Detection - ICCV 2019"/></p> <p>The network architecture of FCOS, where C3, C4, and C5 denote the feature maps of the backbone network and P3 to P7 are the feature levels used for the final prediction. H × W is the height and width of feature maps. ‘/s’ (s = 8, 16, …, 128) is the downsampling ratio of the feature maps at the level to the input image. As an example, all the numbers are computed with an 800 × 1024 input.</p> <p><img src="https://i.imgur.com/JqIcW0A.png" alt="FCOS: Fully Convolutional One-Stage Object Detection - ICCV 2019"/></p> <p><strong>Code</strong></p> <p><a href="https://github.com/tianzhi0549/FCOS">PyTorch</a></p> <h2 id="reppoints-point-set-representation-for-object-detection---iccv-2019"><strong>RepPoints: Point Set Representation for Object Detection - ICCV 2019</strong></h2> <p>RepPoints is a set of points that learns to adaptively position themselves over an object in a manner that circumscribes the object’s spatial extent and indicates semantically significant local areas. The training of RepPoints is driven jointly by object localization and recognition targets, such that the RepPoints are tightly bound by the ground-truth bounding box and guide the detector toward correct object classification. This adaptive and differentiable representation can be coherently used across the different stages of a modern object detector, and does not require the use of anchors to sample over a space of bounding boxes.</p> <p><img src="https://i.imgur.com/5fhfyee.jpg" alt="RepPoints: Point Set Representation for Object Detection - ICCV 2019"/></p> <p>Overview of the proposed RPDet (RepPoints detector). Whilebone, we only draw the afterwards pipeline of one scale of FPN featureshare the same afterwards network architecture and the same model weights. feature pyramidal networks (FPN) are adopted as the backmaps for clear illustration. Note all scales of FPN feature maps share the same afterwards network architecture and the same model weights.</p> <p><img src="https://i.imgur.com/UCX7qCq.png" alt="RepPoints: Point Set Representation for Object Detection - ICCV 2019"/></p> <p><strong>Code</strong></p> <p><a href="https://github.com/microsoft/RepPoints">PyTorch</a></p>]]></content><author><name></name></author><category term="all"/><category term="intelligent-sensing"/><category term="object-detection"/><summary type="html"><![CDATA[An overview and summary of anchor-free series detection works.]]></summary></entry><entry><title type="html">Deep Generative Models</title><link href="https://ddlee-cn.github.io/blog/2019/Generative-Models/" rel="alternate" type="text/html" title="Deep Generative Models"/><published>2019-08-18T00:00:00+00:00</published><updated>2019-08-18T00:00:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2019/Generative-Models</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2019/Generative-Models/"><![CDATA[<h2 id="taxonomy"><strong>Taxonomy</strong></h2> <p>A Generative Model learns a probability distribution from data with prior knowledge, producing new images from learned distribution.</p> <p><img src="https://i.imgur.com/yQfmFIT.png" alt="Deep Generative Models: A Taxonomy"/></p> <h2 id="key-choices"><strong>Key choices</strong></h2> <h3 id="representation">Representation</h3> <p>There are two main choices for learned representation: <strong>factorized model</strong> and <strong>latent variable model</strong>.</p> <p><strong>Factorized model</strong> writes probability distribution as a product of simpler terms, via chain rule. <img src="https://i.imgur.com/5n5o8yl.png" alt="Deep Generative Models: A Taxonomy"/></p> <p><strong>Latent variable model</strong> defines a latent space to extract the core information from data, which is much smaller than the original one.</p> <p><img src="https://i.imgur.com/U4o2qem.png" alt="Deep Generative Models: A Taxonomy"/></p> <h3 id="learning">Learning</h3> <p><strong>Max Likelihood Estimation</strong></p> <ul> <li>fully-observed graphical models: PixelRNN &amp; PixelCNN -&gt; PixelCNN++, WaveNet(audio)</li> <li>latent-variable models: VAE -&gt; VQ-VAE</li> <li>latent-variable invertible models(Flow-based): NICE, Real NVP -&gt; MAF, IAF, Glow</li> </ul> <p><strong>Adversarial Training</strong></p> <ul> <li>GANs: Vanilla GAN -&gt; improved GAN, DCGAN, cGAN -&gt; WGAN, ProGAN -&gt; SAGAN, StyleGAN, BigGAN</li> </ul> <p>Comparison of GAN, VAE and Flow-based Models <img src="https://i.imgur.com/brtrqi4.png" alt="Deep Generative Models: A Taxonomy"/></p> <h2 id="vae-variational-autoencoder"><strong>VAE: Variational AutoEncoder</strong></h2> <h3 id="auto-encoding-variational-bayes---kingma---iclr-2014">Auto-Encoding Variational Bayes - Kingma - ICLR 2014</h3> <ul> <li>Title: <strong>Auto-Encoding Variational Bayes</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: D. P. Kingma and M. Welling</li> <li>Date: Dec. 2013</li> <li>Arxiv: <a href="https://arxiv.org/abs/1312.6114">1312.6114</a></li> <li>Published: ICLR 2014</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>A reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods</li> <li>For i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator</li> </ul> <p>The key idea: approximate the posterior $p_θ(z|x)$ with a simpler, tractable distribution $q_ϕ(z|x)$. <img src="https://i.imgur.com/LE4oMbx.jpg" alt="Auto-Encoding Variational Bayes - Kingma - ICLR 2014"/></p> <table> <tbody> <tr> <td>The graphical model involved in Variational Autoencoder. Solid lines denote the generative distribution $p_θ(.)$ and dashed lines denote the distribution $q_ϕ(z</td> <td>x)$ to approximate the intractable posterior $p_θ(z</td> <td>x)$.</td> </tr> </tbody> </table> <p><img src="https://i.imgur.com/26oV5mL.jpg" alt="Auto-Encoding Variational Bayes - Kingma - ICLR 2014"/></p> <p><strong>Loss Function: ELBO</strong> Using KL Divergence: \(D_{\mathrm{KL}}\left(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z} | \mathbf{x})\right)=\log p_{\theta}(\mathbf{x})+D_{\mathrm{KL}}\left(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z})\right)-\mathbb{E}_{\mathbf{z} \sim q_{\phi}(\mathbf{z} | \mathbf{x})} \log p_{\theta}(\mathbf{x} | \mathbf{z})\)</p> <p>ELOB defined as: \(\begin{aligned} L_{\mathrm{VAE}}(\theta, \phi) &amp;=-\log p_{\theta}(\mathbf{x})+D_{\mathrm{KL}}\left(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z} | \mathbf{x})\right) \\ &amp;=-\mathbb{E}_{\mathbf{z} \sim q_{\phi}(\mathbf{z} | \mathbf{x})} \log p_{\theta}(\mathbf{x} | \mathbf{z})+D_{\mathrm{KL}}\left(q_{\phi}(\mathbf{z} | \mathbf{x}) \| p_{\theta}(\mathbf{z})\right) \\ \theta^{*}, \phi^{*} &amp;=\arg \min _{\theta, \phi} L_{\mathrm{VAE}} \end{aligned}\)</p> <p>By minimizing the loss we are maximizing the lower bound of the probability of generating real data samples.</p> <p><strong>The Reparameterization Trick</strong></p> <p>The expectation term in the loss function invokes generating samples from $z∼q_ϕ(z|x)$. Sampling is a stochastic process and therefore we cannot backpropagate the gradient. To make it trainable, the reparameterization trick is introduced: It is often possible to express the random variable $z$ as a deterministic variable $\mathbf{z}=\mathcal{T}<em>{\phi}(\mathbf{x}, \boldsymbol{\epsilon})$, where $ϵ$ is an auxiliary independent random variable, and the transformation function $\mathcal{T}</em>{\phi}$ parameterized by $ϕ$ converts $ϵ$ to $z$.</p> <p>For example, a common choice of the form of $q_ϕ(z|x)$ ltivariate Gaussian with a diagonal covariance structure: \(\begin{array}{l}{\mathbf{z} \sim q_{\phi}\left(\mathbf{z} | \mathbf{x}^{(i)}\right)=\mathcal{N}\left(\mathbf{z} ; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)} \boldsymbol{I}\right)} \\ {\mathbf{z}=\boldsymbol{\mu}+\boldsymbol{\sigma} \odot \boldsymbol{\epsilon}, \text { where } \boldsymbol{\epsilon} \sim \mathcal{N}(0, \boldsymbol{I})}\end{array}\) where $⊙$ refers to element-wise product.</p> <p><img src="https://i.imgur.com/61JQRa8.jpg" alt="Auto-Encoding Variational Bayes - Kingma - ICLR 2014"/></p> <h3 id="vq-vaeneural-discrete-representation-learning---van-den-oord---nips-2017">(VQ-VAE)Neural Discrete Representation Learning - van den Oord - NIPS 2017</h3> <ul> <li>Title: <strong>Neural Discrete Representation Learning</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: A. van den Oord, O. Vinyals, and K. Kavukcuoglu</li> <li>Date: Nov. 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1711.00937">1711.00937</a></li> <li>Published: NIPS 2017</li> <li>Affiliation: Google DeepMind</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>Discrete representation for data distribution</li> <li>The prior is learned instead of random</li> </ul> <p><strong>Vector Quantisation(VQ)</strong> Vector quantisation (VQ) is a method to map $K$-dimensional vectors into a finite set of “code” vectors. The encoder output $E(\mathbf{x})=\mathbf{z}_{e}$ goes through a nearest-neighbor lookup to match to one of $K$ embedding vectors and then this matched code vector becomes the input for the decoder $D(.)$:</p> \[z_{q}(x)=e_{k}, \quad \text { where } \quad k=\operatorname{argmin}_{j}\left\|z_{e}(x)-e_{j}\right\|_{2}\] <p>The dictionary items are updated using Exponential Moving Averages(EMA), which is similar to EM methods like K-Means.</p> <p><img src="https://i.imgur.com/O8c2e05.png" alt="(VQ-VAE)Neural Discrete Representation Learning"/></p> <p><strong>Loss Design</strong></p> <ul> <li>Reconstruction loss</li> <li>VQ loss: The L2 error between the embedding space and the encoder outputs.</li> <li>Commitment loss: A measure to encourage the encoder output to stay close to the embedding space and to prevent it from fluctuating too frequently from one code vector to another.</li> </ul> \[L=\underbrace{\left\|\mathbf{x}-D\left(\mathbf{e}_{k}\right)\right\|_{2}^{2}}_{\text { reconstruction loss }}+\underbrace{\left\|\operatorname{sg}[E(\mathbf{x})]-\mathbf{e}_{k}\right\|_{2}^{2}}_{\text { VQ loss }}+\underbrace{\beta\left\|E(\mathbf{x})-\operatorname{sg}\left[\mathbf{e}_{k}\right]\right\|_{2}^{2}}_{\text { commitment loss }}\] <p>where sq[.] is the <code class="language-plaintext highlighter-rouge">stop_gradient</code> operator.</p> <p>Training PixelCNN and WaveNet for images and audio respectively on learned latent space, the VA-VAE model avoids “posterior collapse” problem which VAE suffers from.</p> <h3 id="generating-diverse-high-fidelity-images-with-vq-vae-2---razavi---2019">Generating Diverse High-Fidelity Images with VQ-VAE-2 - Razavi - 2019</h3> <ul> <li>Title: <strong>Generating Diverse High-Fidelity Images with VQ-VAE-2</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: A. Razavi, A. van den Oord, and O. Vinyals</li> <li>Date: Jun. 2019</li> <li>Arxiv: <a href="https://arxiv.org/abs/1906.00446">1906.00446</a></li> <li>Affiliation: Google DeepMind</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>Diverse generated results</li> <li>A multi-scale hierarchical organization of VQ-VAE</li> <li>Self-attention mechanism over autoregressive model</li> </ul> <p><img src="https://i.imgur.com/kNEGBCj.png" alt="Generating Diverse High-Fidelity Images with VQ-VAE-2"/></p> <p><strong>Stage 1: Training hierarchical VQ-VAE</strong> The design of hierarchical latent variables intends to separate local patterns (i.e., texture) from global information (i.e., object shapes). The training of the larger bottom level codebook is conditioned on the smaller top level code too, so that it does not have to learn everything from scratch.</p> <p><img src="https://i.imgur.com/HmBVGcm.png" alt="Generating Diverse High-Fidelity Images with VQ-VAE-2"/></p> <p><strong>Stage 2: Learning a prior over the latent discrete codebook</strong> The decoder can receive input vectors sampled from a similar distribution as the one in training. A powerful autoregressive model enhanced with multi-headed self-attention layers is used to capture the correlations in spatial locations that are far apart in the image with a larger receptive field.</p> <p><img src="https://i.imgur.com/kbiYRcN.png" alt="Generating Diverse High-Fidelity Images with VQ-VAE-2"/></p> <h2 id="normalizing-flow-nice-real-nvp-vae-flow-maf-iaf-and-glow"><strong>Normalizing Flow: NICE, Real NVP, VAE-Flow, MAF, IAF and Glow</strong></h2> <p>There are two types of flow: normalizing flow and autoregressive flow.</p> <ul> <li>fully-observed graphical models: PixelRNN &amp; PixelCNN -&gt; PixelCNN++, WaveNet(audio)</li> <li>latent-variable invertible models(Flow-based): NICE, Real NVP -&gt; MAF, IAF, Glow</li> </ul> <h3 id="variational-inference-with-normalizing-flows---rezende---icml-2015">Variational Inference with Normalizing Flows - Rezende - ICML 2015</h3> <ul> <li>Title: <strong>Variational Inference with Normalizing Flows</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: D. J. Rezende and S. Mohamed</li> <li>Date: May 2015</li> <li>Arxiv: <a href="https://arxiv.org/abs/1505.05770">1505.05770</a></li> <li>Published: ICML 2015</li> </ul> <p>A normalizing flow transforms a simple distribution into a complex one by applying a sequence of invertible transformation functions. Flowing through a chain of transformations, we repeatedly substitute the variable for the new one according to the change of variables theorem and eventually obtain a probability distribution of the final target variable. <img src="https://i.imgur.com/SpNbE5o.jpg" alt="Variational Inference with Normalizing Flows - Rezende - ICML 2015"/> Illustration of a normalizing flow model, transforming a simple distribution $p_0(z_0)$ to a complex one $p_K(z_K)$ step by step.</p> <h3 id="nice-non-linear-independent-components-estimation---dinh---iclr-2015">NICE: Non-linear Independent Components Estimation - Dinh - ICLR 2015</h3> <ul> <li>Title: <strong>NICE: Non-linear Independent Components Estimation</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: L. Dinh, D. Krueger, and Y. Bengio</li> <li>Date: Oct. 2014</li> <li>Arxiv: <a href="https://arxiv.org/abs/1410.8516">1410.8516</a></li> <li>Published: ICLR 2015</li> </ul> <p>NICE defines additive coupling layer: \(\left\{\begin{array}{l}{\mathbf{y}_{1 : d}=\mathbf{x}_{1 : d}} \\ {\mathbf{y}_{d+1 : D}=\mathbf{x}_{d+1 : D}+m\left(\mathbf{x}_{1 : d}\right)}\end{array} \Leftrightarrow\left\{\begin{array}{l}{\mathbf{x}_{1 : d}=\mathbf{y}_{1 : d}} \\ {\mathbf{x}_{d+1 : D}=\mathbf{y}_{d+1 : D}-m\left(\mathbf{y}_{1 : d}\right)}\end{array}\right.\right.\)</p> <h3 id="real-nvp---dinh---iclr-2017">Real NVP - Dinh - ICLR 2017</h3> <ul> <li>Title: <strong>Density estimation using Real NVP</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: L. Dinh, J. Sohl-Dickstein, and S. Bengio</li> <li>Date: May 2016</li> <li>Arxiv: <a href="https://arxiv.org/abs/1605.08803">1605.08803</a></li> <li>Published: ICLR 2017</li> </ul> <p><img src="https://i.imgur.com/Rtci5Hb.jpg" alt="Density estimation using Real NVP - Dinh - ICLR 2017"/></p> <p>Real NVP implements a normalizing flow by stacking a sequence of invertible bijective transformation functions. In each bijection $f:x↦y$, known as affine coupling layer, the input dimensions are split into two parts:</p> <ul> <li>The first $d$ dimensions stay same;</li> <li>The second part, $d+1$ to $D$ dimensions, undergo an affine transformation (“scale-and-shift”) and both the scale and shift parameters are functions of the first $d$ dimensions.</li> </ul> \[\begin{aligned} \mathbf{y}_{1 : d} &amp;=\mathbf{x}_{1 : d} \\ \mathbf{y}_{d+1 : D} &amp;=\mathbf{x}_{d+1 : D} \odot \exp \left(s\left(\mathbf{x}_{1 : d}\right)\right)+t\left(\mathbf{x}_{1 : d}\right) \end{aligned}\] <p>where $s(.)$ and $t(.)$ are scale and translation functions and both map $\mathbb{R}^{d} \mapsto \mathbb{R}^{D-d}$. The $⊙$ operation is the element-wise product.</p> <p><img src="https://i.imgur.com/4mIvZn1.png" alt="Density estimation using Real NVP - Dinh - ICLR 2017"/></p> <h3 id="mafmasked-autoregressive-flow-for-density-estimation---papamakarios---nips-2017">(MAF)Masked Autoregressive Flow for Density Estimation - Papamakarios - NIPS 2017</h3> <ul> <li>Title: <strong>Masked Autoregressive Flow for Density Estimation</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: G. Papamakarios, T. Pavlakou, and I. Murray</li> <li>Date: May 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1705.07057">1705.07057</a></li> <li>Published: NIPS 2017</li> </ul> <p>Masked Autoregressive Flow is a type of normalizing flows, where the transformation layer is built as an autoregressive neural network. MAF is very similar to Inverse Autoregressive Flow (IAF) introduced later. See more discussion on the relationship between MAF and IAF in the next section.</p> <p>Given two random variables, $z∼π(z)$ and $x∼p(x)$ and the probability density function $π(z)$ is known, MAF aims to learn $p(x)$. MAF generates each $x_i$ conditioned on the past dimensions $x_{1:i−1}$.</p> <p>Precisely the conditional probability is an affine transformation of $z$, where the scale and shift terms are functions of the observed part of $x$.</p> <p>Data generation, producing a new $x$: \(x_{i} \sim p\left(x_{i} | \mathbf{x}_{1 : i-1}\right)=z_{i} \odot \sigma_{i}\left(\mathbf{x}_{1 : i-1}\right)+\mu_{i}\left(\mathbf{x}_{1 : i-1}\right), \text { where } \mathbf{z} \sim \pi(\mathbf{z})\) Density estimation, given a known $x$: \(p(\mathbf{x})=\prod_{i=1}^{D} p\left(x_{i} | \mathbf{x}_{1 : i-1}\right)\)</p> <p>The generation procedure is sequential, so it is slow by design. While density estimation only needs one pass the network using architecture like MADE. The transformation function is trivial to inverse and the Jacobian determinant is easy to compute too.</p> <p><img src="https://i.imgur.com/6iGl1so.png" alt=" Masked Autoregressive Flow for Density Estimation - Papamakarios - NIPS 2017"/></p> <p>The gray unit $x_i$ is the unit we are trying to compute, and the blue units are the values it depends on. αi and μi are scalars that are computed by passing $x_{1:i−1}$ through neural networks (magenta, orange circles). Even though the transformation is a mere scale-and-shift, the scale and shift can have complex dependencies on previous variables. For the first unit $x_1$, $μ$ and $α$ are usually set to learnable scalar variables that don’t depend on any $x$ or $u$.</p> <p>The inverse pass: <img src="https://i.imgur.com/f0StdP1.png" alt=" Masked Autoregressive Flow for Density Estimation - Papamakarios - NIPS 2017"/></p> <h3 id="iafimproving-variational-inference-with-inverse-autoregressive-flow---kingma---nips-2016">(IAF)Improving Variational Inference with Inverse Autoregressive Flow - Kingma - NIPS 2016</h3> <ul> <li>Title: <strong>Improving Variational Inference with Inverse Autoregressive Flow</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: D. P. Kingma, T. Salimans, R. Jozefowicz, X. Chen, I. Sutskever, and M. Welling</li> <li>Date: Jun. 2016.</li> <li>Arxiv: <a href="https://arxiv.org/abs/1606.04934">1606.04934</a></li> <li>Published: NIPS 2016</li> </ul> <p>Similar to MAF, Inverse autoregressive flow (IAF; Kingma et al., 2016) models the conditional probability of the target variable as an autoregressive model too, but with a reversed flow, thus achieving a much efficient sampling process.</p> <p>First, let’s reverse the affine transformation in MAF: \(z_{i}=\frac{x_{i}-\mu_{i}\left(\mathbf{x}_{1 : i-1}\right)}{\sigma_{i}\left(\mathbf{x}_{1 : i-1}\right)}=-\frac{\mu_{i}\left(\mathbf{x}_{1 : i-1}\right)}{\sigma_{i}\left(\mathbf{x}_{1 : i-1}\right)}+x_{i} \odot \frac{1}{\sigma_{i}\left(\mathbf{x}_{1 : i-1}\right)}\) if let: \(\begin{array}{l}{\mathbf{x}=\mathbf{z}, p( .)=\pi( .), \mathbf{x} \sim p(\mathbf{x})} \\ {\mathbf{z}=\mathbf{x}, \pi( .)=p( .), \mathbf{z} \sim \pi(\mathbf{z})}\end{array}\\ \begin{aligned} \mu_{i}\left(\mathbf{z}_{i : i-1}\right) &amp;=\mu_{i}\left(\mathbf{x}_{1 : i-1}\right)=-\frac{\mu_{i}\left(\mathbf{x}_{1 : i-1}\right)}{\sigma_{i}\left(\mathbf{x}_{1 : i-1}\right)} \\ \sigma\left(\mathbf{z}_{i : i-1}\right) &amp;=\sigma\left(\mathbf{x}_{1 : i-1}\right)=\frac{1}{\sigma_{i}\left(\mathbf{x}_{1 : i-1}\right)} \end{aligned}\) Then we have: <img src="https://i.imgur.com/oXhXKRT.jpg" alt="Improving Variational Inference with Inverse Autoregressive Flow - Kingma - NIPS 2016"/></p> <p>IAF intends to estimate the probability density function of $x̃$ given that $π̃ (z̃ )$ is already known. The inverse flow is an autoregressive affine transformation too, same as in MAF, but the scale and shift terms are autoregressive functions of observed variables from the known distribution $π̃ (z̃)$.</p> <p><img src="https://i.imgur.com/QbbEmSy.jpg" alt="Improving Variational Inference with Inverse Autoregressive Flow - Kingma - NIPS 2016"/></p> <p>Like other normalizing flows, drawing samples from an approximate posterior with Inverse AutoregressiveFlow(IAF) consists of an initial sample $z$ drawn from a simple distribution, such as a Gaussian with diagonal covariance, followed by a chain of nonlinear invertible transformations of z, each with a simple Jacobian determinants.</p> <p><img src="https://i.imgur.com/yyg4OBS.png" alt="Improving Variational Inference with Inverse Autoregressive Flow - Kingma - NIPS 2016"/></p> <p><img src="https://i.imgur.com/ZNZbvUy.jpg" alt="Improving Variational Inference with Inverse Autoregressive Flow - Kingma - NIPS 2016"/></p> <p>Computations of the individual elements $x̃ i$ do not depend on each other, so they are easily parallelizable (only one pass using MADE). The density estimation for a known $x̃ $ is not efficient, because we have to recover the value of $z̃ i$ in a sequential order, $z̃ i=(x̃ i−μ̃ i(z̃ 1:i−1))/σ̃ i(z̃ 1:i−1)$ thus D times in total.</p> <p><img src="https://i.imgur.com/t9KYagm.jpg" alt="Improving Variational Inference with Inverse Autoregressive Flow - Kingma - NIPS 2016"/></p> <h3 id="glow-generative-flow-with-invertible-1x1-convolutions---kingma--dhariwal---nips-2018">Glow: Generative Flow with Invertible 1x1 Convolutions - Kingma &amp; Dhariwal - NIPS 2018</h3> <ul> <li>Title: <strong>Glow: Generative Flow with Invertible 1x1 Convolutions</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: D. P. Kingma and P. Dhariwal</li> <li>Date: Jul. 2018</li> <li>Arxiv: <a href="https://arxiv.org/abs/1807.03039">1807.03039</a></li> <li>Published: NIPS 2018</li> </ul> <p><strong>The proposed flow</strong> <img src="https://i.imgur.com/e65n6xN.jpg" alt="Glow: Generative Flow with Invertible 1x1 Convolutions - Kingma &amp; Dhariwal - NIPS 2018"/></p> <p>The authors propose a generative flow where each step (left) consists of an actnorm step, followed by an invertible 1 × 1 convolution, followed by an affine transformation (Dinh et al., 2014). This flow is combined with a multi-scale architecture (right).</p> <p>There are three steps in one stage of flow in Glow.</p> <p>Step 1:<strong>Activation normalization</strong> (short for “actnorm”)</p> <p>It performs an affine transformation using a scale and bias parameter per channel, similar to batch normalization, but works for mini-batch size 1. The parameters are trainable but initialized so that the first minibatch of data have mean 0 and standard deviation 1 after actnorm.</p> <p>Step 2: <strong>Invertible 1x1 conv</strong></p> <p>Between layers of the RealNVP flow, the ordering of channels is reversed so that all the data dimensions have a chance to be altered. A 1×1 convolution with equal number of input and output channels is a generalization of any permutation of the channel ordering.</p> <table> <tbody> <tr> <td>Say, we have an invertible 1x1 convolution of an input $h×w×c$ tensor $h$ with a weight matrix $W$ of size $c×c$. The output is a $h×w×c$ tensor, labeled as $ f=𝚌𝚘𝚗𝚟𝟸𝚍(h;W)$. In order to apply the change of variable rule, we need to compute the Jacobian determinant $</td> <td>det∂f/∂h</td> <td>$.</td> </tr> </tbody> </table> <p>Both the input and output of 1x1 convolution here can be viewed as a matrix of size $h×w$. Each entry $x_{ij}$($i=1,2…h, j=1,2,…,w$) in $h$ is a vector of $c$ channels and each entry is multiplied by the weight matrix $W$ to obtain the corresponding entry $y_{ij}$ in the output matrix respectively. The derivative of each entry is $\partial \mathbf{x}<em>{i j} \mathbf{W} / \partial \mathbf{x}</em>{i j}=\mathbf{w}$ and there are $h×w$ such entries in total:</p> <p>The inverse 1x1 convolution depends on the inverse matrix $W^{−1}$ . Since the weight matrix is relatively small, the amount of computation for the matrix determinant (tf.linalg.det) and inversion (tf.linalg.inv) is still under control.</p> <p>Step 3: Affine coupling layer</p> <p>The design is same as in RealNVP.</p> <p><img src="https://i.imgur.com/CjlvJLu.jpg" alt="Glow: Generative Flow with Invertible 1x1 Convolutions - Kingma &amp; Dhariwal - NIPS 2018"/></p> <p>The three main components of proposed flow, their reverses, and their log-determinants. Here, $x$ signifies the input of the layer, and $y$ signifies its output. Both $x$ and $y$ are tensors of shape $[h × w × c]$ with spatial dimensions (h, w) and channel dimension $c$. With $(i, j)$ we denote spatial indices into tensors $x$ and $y$. The function NN() is a nonlinear mapping, such as a (shallow) convolutional neural network like in ResNets (He et al., 2016) and RealNVP (Dinh et al., 2016).</p> <h2 id="autoregressive-flow-pixelrnn-pixelcnn-gated-pixelcnn-wavenet-and-pixelcnn"><strong>AutoRegressive Flow: PixelRNN, PixelCNN, Gated PixelCNN, WaveNet and PixelCNN++</strong></h2> <h3 id="pixelrnn--pixelcnnpixel-recurrent-neural-networks---van-den-oord---icml-2016">(PixelRNN &amp; PixelCNN)Pixel Recurrent Neural Networks - van den Oord - ICML 2016</h3> <ul> <li>Title: <strong>Pixel Recurrent Neural Networks</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu</li> <li>Date: Jan 2016</li> <li>Arxiv: <a href="https://arxiv.org/abs/1601.06759">1601.06759</a></li> <li>Published: ICML 2016(Best Paper Award)</li> <li>Affiliation: Google DeepMind</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>Fully tractable modeling of image distribution</li> <li>PixelRNN &amp; PixelCNN</li> </ul> <p><strong>Design</strong> To estimate the joint distribution $p(x)$ we write it as the product of the conditional distributions over the pixels:</p> \[p(\mathbf{x})=\prod_{i=1}^{n^{2}} p\left(x_{i} | x_{1}, \ldots, x_{i-1}\right)\] <p>Generating pixel-by-pixel with CNN, LSTM: <img src="https://i.imgur.com/Et9vL70.png" alt="Pixel Recurrent Neural Networks"/></p> <h3 id="conditional-image-generation-with-pixelcnn-decoders---van-den-oord---nips-2016">Conditional Image Generation with PixelCNN Decoders - van den Oord - NIPS 2016</h3> <ul> <li>Title: <strong>Conditional Image Generation with PixelCNN Decoders</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: A. van den Oord, N. Kalchbrenner, O. Vinyals, L. Espeholt, A. Graves, and K. Kavukcuoglu</li> <li>Date: Jun. 2016</li> <li>Arxiv: <a href="https://arxiv.org/abs/1606.05328">1606.05328</a></li> <li>Published: NIPS 2016</li> <li>Affiliation: Google DeepMind</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>Conditional with class labels or conv embeddings</li> <li>Can also serve as a powerful decoder</li> </ul> <p><strong>Design</strong> Typically, to make sure the CNN can only use information about pixels above and to the left of the current pixel, the filters of the convolution in PixelCNN are masked. However, its computational cost rise rapidly when stacked.</p> <p>The gated activation unit: \(\mathbf{y}=\tanh \left(W_{k, f} * \mathbf{x}\right) \odot \sigma\left(W_{k, g} * \mathbf{x}\right),\) where $σ$ is the sigmoid non-linearity, $k$ is the number of the layer, $⊙$ is the element-wise product and $∗$ is the convolution operator.</p> <p>Add a high-level image description represented as a latent vector $h$: \(\mathbf{y}=\tanh \left(W_{k, f} * \mathbf{x}+V_{k, f}^{T} \mathbf{h}\right) \odot \sigma\left(W_{k, g} * \mathbf{x}+V_{k, g}^{T} \mathbf{h}\right)\)</p> <p><img src="https://i.imgur.com/DTseuKt.png" alt="Conditional Image Generation with PixelCNN Decoders"/></p> <h3 id="wavenet-a-generative-model-for-raw-audio---van-den-oord---ssw-2016">WaveNet: A Generative Model for Raw Audio - van den Oord - SSW 2016</h3> <ul> <li>Title: <strong>WaveNet: A Generative Model for Raw Audio</strong></li> <li>Task: <strong>Text to Speech</strong></li> <li>Author: A. van den Oord et al.</li> <li>Arxiv: <a href="https://cvnote.ddlee.cc/1609.03499">1609.03499</a></li> <li>Date: Sep. 2016.</li> <li>Published: SSW 2016</li> </ul> <p>WaveNet consists of a stack of causal convolution which is a convolution operation designed to respect the ordering: the prediction at a certain timestamp can only consume the data observed in the past, no dependency on the future. In PixelCNN, the causal convolution is implemented by masked convolution kernel. The causal convolution in WaveNet is simply to shift the output by a number of timestamps to the future so that the output is aligned with the last input element.</p> <p><img src="https://i.imgur.com/V10Et4B.png" alt=""/></p> <p>One big drawback of convolution layer is a very limited size of receptive field. The output can hardly depend on the input hundreds or thousands of timesteps ago, which can be a crucial requirement for modeling long sequences. WaveNet therefore adopts dilated convolution (animation), where the kernel is applied to an evenly-distributed subset of samples in a much larger receptive field of the input.</p> <p><img src="https://i.imgur.com/uCOWLbN.jpg" alt="CleanShot 2019-08-20 at 16.08.23@2x"/></p> <h3 id="pixelcnn-improving-the-pixelcnn-with-discretized-logistic-mixture-likelihood-and-other-modification---salimans---iclr-2017">PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modification - Salimans - ICLR 2017</h3> <ul> <li>Title: <strong>PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications</strong></li> <li>Task: <strong>Image Generation</strong></li> <li>Author: T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma</li> <li>Date: Jan. 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1701.05517">1701.05517</a></li> <li>Published: ICLR 2017</li> <li>Affiliation: OpenAI</li> </ul> <p><strong>Highlights</strong></p> <ul> <li>A discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which speeds up training.</li> <li>Condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure.</li> <li>Downsampling to efficiently capture structure at multiple resolutions.</li> <li>Additional shortcut connections to further speed up optimization.</li> <li>Regularize the model using dropout</li> </ul> <p><strong>Design</strong> By choosing a simple continuous distribution for modeling $ν$ we obtain a smooth and memory efficient predictive distribution for $x$. Here, we take this continuous univariate distribution to be a mixture of logistic distributions which allows us to easily calculate the probability on the observed discretized value $x$ For all sub-pixel values $x$ excepting the edge cases 0 and 255 we have: \(\nu \sim \sum_{i=1}^{K} \pi_{i} \operatorname{logistic}\left(\mu_{i}, s_{i}\right)\)</p> \[P(x | \pi, \mu, s)=\sum_{i=1}^{K} \pi_{i}\left[\sigma\left(\left(x+0.5-\mu_{i}\right) / s_{i}\right)-\sigma\left(\left(x-0.5-\mu_{i}\right) / s_{i}\right)\right]\] <p>The output of our network is thus of much lower dimension, yielding much denser gradients of the loss with respect to our parameters.</p> <p><img src="https://i.imgur.com/MN4a9m1.png" alt="PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modification"/></p> <h2 id="gans-generative-adversarial-network"><strong>GANs: Generative Adversarial Network</strong></h2> <h3 id="generative-adversarial-networks---goodfellow---nips-2014">Generative Adversarial Networks - Goodfellow - NIPS 2014</h3> <ul> <li>Title: <strong>Generative Adversarial Networks</strong></li> <li>Author: I. J. Goodfellow et al</li> <li>Date: Jun. 2014.</li> <li>Arxiv: <a href="https://arxiv.org/abs/1406.2661">1406.2661</a></li> <li>Published: NIPS 2014</li> </ul> <p>General structure of a Generative Adversarial Network, where the generator G takes a noise vector z as input and output a synthetic sample G(z), and the discriminator takes both the synthetic input G(z) and true sample x as inputs and predict whether they are real or fake. <img src="https://i.imgur.com/Qct8It9.jpg" alt="CleanShot 2019-08-20 at 20.18.19@2x"/></p> <p>Generative Adversarial Net (GAN) consists of two separate neural networks: a generator G that takes a random noise vector z, and outputs synthetic data G(z); a discriminator D that takes an input x or G(z) and output a probability D(x) or D(G(z)) to indicate whether it is synthetic or from the true data distribution, as shown in Figure 1. Both of the generator and discriminator can be arbitrary neural networks.</p> <p>In other words, D and G play the following two-player minimax game with value function $V (G, D)$: \(\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}}(\boldsymbol{z})[\log (1-D(G(\boldsymbol{z})))]\)</p> <p><img src="https://i.imgur.com/1dZBs4C.jpg" alt="CleanShot 2019-08-20 at 20.26.08@2x"/></p> <p>The main loop of GAN training. Novel data samples, $x′$, may be drawn by passing random samples, $z$ through the generator network. The gradient of the discriminator may be updated $k$ times before updating the generator.</p> <p><img src="https://i.imgur.com/SyHJPFB.jpg" alt="CleanShot 2019-08-20 at 20.41.11@2x"/></p> <p>GAN provide an implicit way to model data distribution, which is much more versatile than explicit ones like PixelCNN.</p> <h3 id="cgan---mirza---2014">cGAN - Mirza - 2014</h3> <ul> <li>Title: <strong>Conditional Generative Adversarial Nets</strong></li> <li>Author: M. Mirza and S. Osindero</li> <li>Date: Nov. 2014</li> <li>Arxiv: <a href="https://arxiv.org/abs/1411.1784">1411.1784</a></li> </ul> <p><img src="https://i.imgur.com/a4Fgbe1.jpg" alt="CleanShot 2019-08-20 at 20.41.27@2x"/></p> <p>In the original GAN, we have no control of what to be generated, since the output is only dependent on random noise. However, we can add a conditional input $c$ to the random noise $z$ so that the generated image is defined by $G(c,z)$ . Typically, the conditional input vector c is concatenated with the noise vector z, and the resulting vector is put into the generator as it is in the original GAN. Besides, we can perform other data augmentation on $c$ and $z$. The meaning of conditional input $c$ is arbitrary, for example, it can be the class of image, attributes of object or an embedding of text descriptions of the image we want to generate.</p> <p>The objective function of a two-player minimax game would be: \(\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\mathrm{data}}(\boldsymbol{x})}[\log D(\boldsymbol{x} | \boldsymbol{y})]+\mathbb{E}_{\boldsymbol{z} \sim p_{z}}(\boldsymbol{z})[\log (1-D(G(\boldsymbol{z} | \boldsymbol{y})))]\) <img src="https://i.imgur.com/2xeCrNd.jpg" alt="CleanShot 2019-08-20 at 20.31.23@2x"/></p> <p>Architecture of GAN with auxiliary classifier, where $y$ is the conditional input label and $C$ is the classifier that takes the synthetic image $G(y, z)$ as input and predict its label $\hat{y}$.</p> <h3 id="dcgan---radford---iclr-2016">DCGAN - Radford - ICLR 2016</h3> <ul> <li>Title: <strong>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</strong></li> <li>Author: A. Radford, L. Metz, and S. Chintala</li> <li>Date: Nov. 2015.</li> <li>Arxiv: <a href="https://arxiv.org/abs/1511.06434">1511.06434</a></li> <li>Published: ICLR 2016</li> </ul> <p><img src="https://i.imgur.com/rm7tS4V.jpg" alt="CleanShot 2019-08-20 at 20.19.15@2x"/> Building blocks of DCGAN, where the generator uses transposed convolution, batch-normalization and ReLU activation, while the discriminator uses convolution, batch-normalization and LeakyReLU activation.</p> <p>DCGAN provides significant contributions to GAN in that its suggested convolution neural network (CNN)architecture greatly stabilizes GAN training. DCGAN suggests an architecture guideline in which the generator is modeled with a transposed CNN, and the discriminator is modeled with a CNN with an output dimension 1. It also proposes other techniques such as batch normalization and types of activation functions for the generator and the discriminator to help stabilize the GAN training. As it solves the instability of training GAN only through architecture, it becomes a baseline for modeling various GANs proposed later.</p> <h3 id="improved-gan---salimans---nips-2016">Improved GAN - Salimans - NIPS 2016</h3> <ul> <li>Title: <strong>Improved Techniques for Training GANs</strong></li> <li>Author: T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen</li> <li>Date: Jun. 2016</li> <li>Arxiv: <a href="https://arxiv.org/abs/1606.03498">1606.03498</a></li> <li>Published: NIPS 2016</li> </ul> <p>Improved GAN proposed several useful tricks to stabilize the training of GANs.</p> <p><strong>Feature matching</strong> This technique substitutes the discriminator’s output in the objective function with an activation function’s output of an intermediate layer of the discriminator to prevent overfitting from the current discriminator. Feature matching does not aim on the discriminator’s output, rather it guides the generator to see the statistics or features of real training data, in an effort to stabilize training.</p> <p><strong>Label smoothing</strong> As mentioned previously, $V (G, D)$ is a binary cross entropy loss whose real data label is 1 and its generated data label is 0. However, since a deep neural network classifier tends to output a class probability with extremely high confidence, label smoothing encourages a deep neural network classifier to produce a more soft estimation by assigning label values lower than 1. Importantly, for GAN, label smoothing has to be made for labels of real data, not for labels of fake data, since, if not, the discriminator can act incorrectly.</p> <p><strong>Minibatch Discrimination</strong></p> <p>With minibatch discrimination, the discriminator is able to digest the relationship between training data points in one batch, instead of processing each point independently.</p> <p>In one minibatch, we approximate the closeness between every pair of samples, $c(x_i,x_j)$, and get the overall summary of one data point by summing up how close it is to other samples in the same batch, $o\left(x_{i}\right)=\sum_{i} c\left(x_{i}, x_{i}\right)$. Then $o(x_i)$ is explicitly added to the input of the model.</p> <p><strong>Historical Averaging</strong></p> <p>For both models, add $\left|\mathbb{E}<em>{x \sim p</em>{r}} f(x)-\mathbb{E}<em>{z \sim p</em>{z}(z)} f(G(z))\right|_{2}^{2}$into the loss function, where $Θ$ is the model parameter and $Θ_i$ is how the parameter is configured at the past training time $i$. This addition piece penalizes the training speed when $Θ$ is changing too dramatically in time.</p> <p><strong>Virtual Batch Normalization (VBN)</strong></p> <p>Each data sample is normalized based on a fixed batch (“reference batch”) of data rather than within its minibatch. The reference batch is chosen once at the beginning and stays the same through the training.</p> <p><strong>Adding Noises</strong></p> <p>Based on the discussion in the previous section, we now know $p_r$ and $p_g$ are disjoint in a high dimensional space and it causes the problem of vanishing gradient. To artificially “spread out” the distribution and to create higher chances for two probability distributions to have overlaps, one solution is to add continuous noises onto the inputs of the discriminator $D$.</p> <p><strong>Use Better Metric of Distribution Similarity</strong></p> <p>The loss function of the vanilla GAN measures the JS divergence between the distributions of $p_r$ and $p_g$. This metric fails to provide a meaningful value when two distributions are disjoint.</p> <p><strong>The theoretical and practical issues of GAN</strong></p> <ul> <li>Because the supports of distributions lie on low dimensional manifolds, there exists the perfect discriminator whose gradients vanish on every data point. Optimizing the generator may be difficult because it is not provided with any information from the discriminator.</li> <li>GAN training optimizes the discriminator for the fixed generator and the generator for fixed discriminator simultaneously in one loop, but it sometimes behaves as if solving a maximin problem, not a minimax problem. It critically causes a mode collapse. In addition, the generator and the discriminator optimize the same objective function $V(G,D)$ in opposite directions which is not usual in classical machine learning, and often suffers from oscillations causing excessive training time.</li> <li>The theoretical convergence proof does not apply in practice because the generator and the discriminator are modeled with deep neural networks, so optimization has to occur in the parameter space rather than in learning the probability density function itself.</li> </ul> <h3 id="wganwasserstein-gan---arjovsky---icml-2017">(WGAN)Wasserstein GAN - Arjovsky - ICML 2017</h3> <ul> <li>Title: Wasserstein GAN</li> <li>Author: M. Arjovsky, S. Chintala, and L. Bottou</li> <li>Date: Jan. 2017</li> <li>Published: ICML 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1701.07875">1701.07875</a></li> </ul> <p>The Kullback-Leibler (KL) divergence \(K L\left(\mathbb{P}_{r} \| \mathbb{P}_{g}\right)=\int \log \left(\frac{P_{r}(x)}{P_{g}(x)}\right) P_{r}(x) d \mu(x)\) where both $P_r$ and $P_g$ are assumed to be absolutely continuous, and therefore admit densities, with respect to a same measure $μ$ defined on $\mathcal{X}^2$ The KL divergence is famously assymetric and possibly infinite when there are points such that $P_g(x) = 0$ and $P_r(x) &gt; 0$.</p> <p>The Jensen-Shannon (JS) divergence \(J S\left(\mathbb{P}_{r}, \mathbb{P}_{g}\right)=K L\left(\mathbb{P}_{r} \| \mathbb{P}_{m}\right)+K L\left(\mathbb{P}_{g} \| \mathbb{P}_{m}\right)\) where $P_m$ is the mixture $(P_r + P_g)/2$. This divergence is symmetrical and always defined because we can choose $μ = P_m$.</p> <p>The Earth-Mover (EM) distance or Wasserstein-1</p> <p>where$Π(Pr,Pg)$denotes the set of all joint distributions $γ(x,y)$, whose marginals are respectively Pr and Pg. Intuitively, $γ(x,y)$ indicates how much “mass” must be transported from x to y in order to transform the distributions $P_r$ into the distribution $P_g$. The EM distance then is the “cost” of the optimal transport plan.</p> <p><img src="https://i.imgur.com/7O2vg4v.png" alt="(WGAN)Wasserstein GAN - Arjovsky - ICML 2017"/></p> <p>Compared to the original GAN algorithm, the WGAN undertakes the following changes:</p> <ul> <li>After every gradient update on the critic function, clamp the weights to a small fixed range, $[−c,c]$.</li> <li>Use a new loss function derived from the Wasserstein distance, no logarithm anymore. The “discriminator” model does not play as a direct critic but a helper for estimating the Wasserstein metric between real and generated data distribution.</li> <li>Empirically the authors recommended RMSProp optimizer on the critic, rather than a momentum based optimizer such as Adam which could cause instability in the model training. I haven’t seen clear theoretical explanation on this point through.</li> </ul> <h3 id="wgan-gp---gulrajani---nips-2017">WGAN-GP - Gulrajani - NIPS 2017</h3> <ul> <li>Title: <strong>Improved Training of Wasserstein GANs</strong></li> <li>Author: I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville</li> <li>Date: Mar. 2017</li> <li>Arxiv: <a href="https://arxiv.org/abs/1704.00028">1704.00028</a></li> <li>Published: NIPS 2017</li> </ul> <p><img src="https://i.imgur.com/WaXxFhK.jpg" alt="(WGAN-GP)Improved Training of Wasserstein GANs - Gulrajani - NIPS 2017"/></p> <p>(left) Gradient norms of deep WGAN critics during training on the Swiss Roll dataset either explode or vanish when using weight clipping, but not when using a gradient penalty. (right) Weight clipping (top) pushes weights towards two values (the extremes of the clipping range), unlike gradient penalty (bottom).</p> <p><strong>Gradient penalty</strong></p> <p><img src="https://i.imgur.com/PdrZXNH.jpg" alt="(WGAN-GP)Improved Training of Wasserstein GANs - Gulrajani - NIPS 2017"/></p> <p>The authors implicitly define $Pxˆ $ sampling uniformly along straight lines between pairs of points sampled from the data distribution $P_r$ and the generator distribution $P_g$. This is motivated by the fact that the optimal critic contains straight lines with gradient norm 1 connecting coupled points from $P_r$ and $P_g$. Given that enforcing the unit gradient norm constraint everywhere is intractable, enforcing it only along these straight lines seems sufficient and experimentally results in good performance.</p> <p><img src="https://i.imgur.com/YjAMKfW.jpg" alt="(WGAN-GP)Improved Training of Wasserstein GANs - Gulrajani - NIPS 2017"/></p> <h3 id="progan---karras---iclr-2018">ProGAN - Karras - ICLR 2018</h3> <ul> <li>Title: <strong>Progressive Growing of GANs for Improved Quality, Stability, and Variation</strong></li> <li>Author: T. Karras, T. Aila, S. Laine, and J. Lehtinen</li> <li>Date: Oct. 2017.</li> <li>Arxiv: <a href="https://arxiv.org/abs/1710.10196">1710.10196</a></li> <li>Published: ICLR 2018</li> </ul> <p>Generating high resolution images is highly challenging since a large scale generated image is easily distinguished by the discriminator, so the generator often fails to be trained. Moreover, there is a memory issue in that we are forced to set a low mini-batch size due to the large size of neural networks. Therefore, some studies adopt hierarchical stacks of multiple generators and discriminators. This strategy divides a large complex generator’s mapping space step by step for each GAN pair, making it easier to learn to generate high resolution images. However, Progressive GAN succeeds in generating high resolution images in a single GAN, making training faster and more stable.</p> <p><img src="https://i.imgur.com/xkBUg3H.jpg" alt="(ProGAN)Progressive Growing of GANs for Improved Quality, Stability, and Variation - Karras - ICLR 2018"/></p> <p>Progressive GAN generates high resolution images by stacking each layer of the generator and the discriminator incrementally. It starts training to generate a very low spatial resolution (e.g. 4×4), and progressively doubles the resolution of generated images by adding layers to the generator and the discriminator incrementally. In addition, it proposes various training techniques such as pixel normalization, equalized learning rate and mini-batch standard deviation, all of which help GAN training to become more stable.</p> <p><img src="https://i.imgur.com/t284grj.jpg" alt="(ProGAN)Progressive Growing of GANs for Improved Quality, Stability, and Variation - Karras - ICLR 2018"/></p> <p>The training starts with both the generator (G) and discriminator (D) having a low spatial resolution of 4×4 pixels. As the training advances, we incrementally add layers to G and D, thus increasing the spatial resolution of the generated images. All existing layers remain trainable throughout the process. Here refers to convolutional layers operating on N × N spatial resolution. This allows stable synthesis in high resolutions and also speeds up training considerably. One the right we show six example images generated using progressive growing at 1024 × 1024.</p> <h3 id="saganself-attention-gan---zhang---jmlr-2019">(SAGAN)Self-Attention GAN - Zhang - JMLR 2019</h3> <ul> <li>Title: <strong>Self-Attention Generative Adversarial Networks</strong></li> <li>Author: H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena,</li> <li>Date: May 2018.</li> <li>Arxiv: <a href="https://arxiv.org/abs/1805.08318">1805.08318</a></li> <li>Published: JMLR 2019</li> </ul> <p><img src="https://i.imgur.com/KsexAhw.jpg" alt="(SAGAN)Self-Attention Generative Adversarial Networks - Zhang - JMLR 2019"/></p> <p>For GAN models trained with ImageNet, they are good at classes with a lot of texture (landscape, sky) but perform much worse for structure. For example, GAN may render the fur of a dog nicely but fail badly for the dog’s legs. While convolutional filters are good at exploring spatial locality information, the receptive fields may not be large enough to cover larger structures. We can increase the filter size or the depth of the deep network but this will make GANs even harder to train. Alternatively, we can apply the attention concept. For example, to refine the image quality of the eye region (the red dot on the left figure), SAGAN only uses the feature map region on the highlight area in the middle figure. As shown below, this region has a larger receptive field and the context is more focus and more relevant. The right figure shows another example on the mouth area (the green dot).</p> <p>Code: <a href="https://github.com/heykeetae/Self-Attention-GAN">PyTorch</a>, <a href="https://github.com/brain-research/self-attention-gan">TensorFlow</a></p> <h3 id="stylegan---karras---cvpr-2019">StyleGAN - Karras - CVPR 2019</h3> <ul> <li>Title: <strong>A Style-Based Generator Architecture for Generative Adversarial Networks</strong></li> <li>Author: T. Karras, S. Laine, and T. Aila</li> <li>Date: Dec. 2018</li> <li>Arxiv: <a href="https://arxiv.org/abs/1812.04948">1812.04948</a></li> <li>Published: CVPR 2019</li> </ul> <p>The StyleGAN architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis.</p> <p><img src="https://i.imgur.com/hYlwKqv.jpg" alt="(StyleGAN)A Style-Based Generator Architecture for Generative Adversarial Networks - Karras - CVPR 2019"/></p> <p>While a traditional generator feeds the latent code though the input layer only, we first map the input to an intermediate latent space W, which then controls the generator through adaptive instance normalization (AdaIN) at each convolution layer. Gaussian noise is added after each convolution, be- fore evaluating the nonlinearity. Here “A” stands for a learned affine transform, and “B” applies learned per-channel scaling factors to the noise input. The mapping network f consists of 8 layers and the synthesis network g consists of 18 layers—two for each resolution ($4^2 − 1024^2$)</p> <p>Code: <a href="https://github.com/NVlabs/stylegan">PyTorch</a></p> <h3 id="biggan---brock---iclr-2019">BigGAN - Brock - ICLR 2019</h3> <ul> <li>Title: <strong>Large Scale GAN Training for High Fidelity Natural Image Synthesis</strong></li> <li>Author: A. Brock, J. Donahue, and K. Simonyan</li> <li>Date: Sep. 2018.</li> <li>Arxiv: <a href="https://arxiv.org/abs/1809.11096">1809.11096</a></li> <li>Published: ICLR 2019</li> </ul> <p>Code: <a href="https://github.com/ajbrock/BigGAN-PyTorch">PyTorch</a></p> <p>The authors demonstrate that GANs benefit dramatically from scaling, and train models with two to four times as many parameters and eight times the batch size compared to prior art. We introduce two simple, general architectural changes that improve scalability, and modify a regularization scheme to improve conditioning, demonstrably boosting performance.</p> <p>As a side effect of our modifications, their models become amenable to the “truncation trick,” a simple sampling technique that allows explicit, fine-grained control of the trade- off between sample variety and fidelity.</p> <p>They discover instabilities specific to large scale GANs, and characterize them empirically. Leveraging insights from this analysis, we demonstrate that a combination of novel and existing techniques can reduce these instabilities, but complete training stability can only be achieved at a dramatic cost to performance.</p> <p><img src="https://i.imgur.com/c7zug0E.jpg" alt=" (BigGAN)Large Scale GAN Training for High Fidelity Natural Image Synthesis - Brock - ICLR 2019"/></p> <p>(a) A typical architectural layout for BigGAN’s G; details are in the following tables. (b) A Residual Block (ResBlock up) in BigGAN’s G. (c) A Residual Block (ResBlock down) in BigGAN’s D.</p> <p><img src="https://i.imgur.com/K82QKzy.jpg" alt=" (BigGAN)Large Scale GAN Training for High Fidelity Natural Image Synthesis - Brock - ICLR 2019"/></p> <p>(a) A typical architectural layout for BigGAN-deep’s G; details are in the following tables. (b) A Residual Block (ResBlock up) in BigGAN-deep’s G. (c) A Residual Block (ResBlock down) in BigGAN-deep’s D. A ResBlock (without up or down) in BigGAN-deep does not include the Upsample or Average Pooling layers, and has identity skip connections.</p> <p><strong>References</strong></p> <ul> <li><a href="https://ermongroup.github.io/generative-models/">IJCAI 2018 Tutorial: Deep Generative Models</a></li> <li><a href="https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html">Flow-based Deep Generative Models</a>, blog</li> <li><a href="http://akosiorek.github.io/ml/2018/04/03/norm_flows.html">Normalizing Flows</a>, blog</li> </ul>]]></content><author><name></name></author><category term="all"/><category term="rendering-generative-ai"/><category term="generative-models"/><summary type="html"><![CDATA[A brief introduction to deep generative models, including variational autoencoders (VAEs), generative adversarial networks (GANs), and other related topics.]]></summary></entry><entry><title type="html">Deep Learning in Scientific Research: My Best Practices</title><link href="https://ddlee-cn.github.io/blog/2019/Best-Practice/" rel="alternate" type="text/html" title="Deep Learning in Scientific Research: My Best Practices"/><published>2019-05-04T23:39:00+00:00</published><updated>2019-05-04T23:39:00+00:00</updated><id>https://ddlee-cn.github.io/blog/2019/Best-Practice</id><content type="html" xml:base="https://ddlee-cn.github.io/blog/2019/Best-Practice/"><![CDATA[<p>Inspired by Karpathy’s impressive post <a href="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks</a>, I decided to review and share my daily route of engineering works on Machine Learning related research. Compared to Karpathy’s advice, I’d like to talk more about approaches or practices instead of strategies. This article is merely a personal revision from a newbie, so any suggestions are welcomed.</p> <h2 id="the-philosophy"><strong>The Philosophy</strong></h2> <blockquote> <p>Everything is a file.</p> <p>– <cite>one of defining features of Unix</cite></p> </blockquote> <p>I believe that it is extremely important to learn from failures, before which recording and observing may be the first step. So I’m obsessed with store and organize everything for future review.</p> <h2 id="dataset"><strong>Dataset+</strong></h2> <p>I added a plus symbol to emphasize the difference between the original dataset and experiment-usable dataset. From my point of view, a dataset+ may include the following parts:</p> <ul> <li>raw images and label file provided by the authors</li> <li>an explore script/notebook to view images and labels</li> <li>a README file which contains info about the dataset: image numbers, image shape, etc</li> <li>a toy dataset whose folder structure is exactly the same as original dataset for running toy example and debug quickly</li> <li>framework-specific formats for fast I/O during training: <code class="language-plaintext highlighter-rouge">tf.record</code>, <code class="language-plaintext highlighter-rouge">.lmdb</code>, etc</li> <li>several list file which contains image names and absolute path of image file</li> </ul> <h2 id="devlopment-environment"><strong>Devlopment Environment</strong></h2> <p>Because of a) the complicated and unstable optimization process b) fast-developing frameworks c) data dependency, the reproduction problem is one of bad reputations of learning-based methods. For a proposed learning-based model, not only the algorithms described in paper, but also its entire reproduction stack as well as the datasets should be taken notice of.</p> <p><img src="/assets/img/posts/ML-Stack.png" alt="ML-stack"/></p> <p>Thanks to container technology and Python environment management tools, namely Docker and <code class="language-plaintext highlighter-rouge">virtualenv</code> etc, we’ve got much more control on the environment where our codes run. I prepared two <code class="language-plaintext highlighter-rouge">Dockerfile</code>s, one of which is for development and another for production (release). Upon docker images, a virtual env will be created for each project. A lot of tools is useful for environment management in Python, including <code class="language-plaintext highlighter-rouge">conda-env</code>, <code class="language-plaintext highlighter-rouge">pyenv</code>, <code class="language-plaintext highlighter-rouge">peotry</code>, <code class="language-plaintext highlighter-rouge">pipenv</code>, or just a <code class="language-plaintext highlighter-rouge">requirements.txt</code> file.</p> <p>Every time running an experiment, a docker container will be built and only projects dependencies are installed, which both can be totally controlled through <code class="language-plaintext highlighter-rouge">Dockerfile</code> and <code class="language-plaintext highlighter-rouge">requirements.txt</code> (or equivalents).</p> <h2 id="experiment-management"><strong>Experiment Management</strong></h2> <h3 id="codes">Codes</h3> <p>All codes are monitored using <code class="language-plaintext highlighter-rouge">git</code>, with at least one commit per day if any files changed. I once suffered a lot from a unintentional bug which can be found immediately if I used VCS in my early days. I encourage myself to only write codes which can be released.</p> <h3 id="configs">Configs</h3> <p>Configs mainly include hyper-parameters of an experiment. Common items are: dataset root directory, output directory, checkpoints directory, learning rate, batch size, GPU ids, epochs, input shapes, loss weight, etc. I think that a perfect configure system should be:</p> <ul> <li><strong>Serialized</strong>. The configs should be reusable across time and machines.</li> <li><strong>Readable</strong>. Easy to understand for human(both authors and others).</li> </ul> <h4 id="current-options">Current options:</h4> <ul> <li><strong>shell script/command line + <code class="language-plaintext highlighter-rouge">argparse</code></strong>: easy to implement, need extra effort to manage scripts. e.g. <a href="https://github.com/NVlabs/SPADE">SPADE</a></li> <li><strong>YAML files + <code class="language-plaintext highlighter-rouge">config</code> variable</strong>: require utility functions to parse YAML files and update <code class="language-plaintext highlighter-rouge">config</code>. e.g. <a href="https://github.com/hanzhanggit/StackGAN-v2">StackGAN-v2</a></li> <li><strong>Python <code class="language-plaintext highlighter-rouge">dict</code></strong>: easy to implement, but add unnecessary code pieces. e.g. <a href="https://github.com/open-mmlab/mmdetection">mmdetection</a></li> <li><strong>Google’s Gin</strong>: similar to YAML files, require <code class="language-plaintext highlighter-rouge">gin</code> package. e.g. <a href="https://github.com/google/compare_gan">compare_gan</a></li> </ul> <h3 id="checkpoints-logs-and-visualization">Checkpoints, Logs and Visualization</h3> <p>I used to create a <code class="language-plaintext highlighter-rouge">checkpoint</code> folder under my home directory and organize checkpoints based on projects and datasets. On the other hand, a <code class="language-plaintext highlighter-rouge">model</code> folder is created for pretrained models and released models. They are usually shared across projects. Inside projects, different runs are identified with starting time.</p> <p>Logs are stored in the corresponding <code class="language-plaintext highlighter-rouge">checkpoint</code> folder during training, with stack version, hyper-params, train loss per batch and val loss per epoch recorded. I also store middle results in a separate folder for debug. Some scripts for parsing logs are required.</p> <p>Tools like <code class="language-plaintext highlighter-rouge">Tensorboard</code> and <code class="language-plaintext highlighter-rouge">Visdom</code> are useful for visualizing loss changes on the fly during training. Furthermore, they can basically record nearly everything. However, they usually need additional ports and eat memory. It’s recommended to mount the <code class="language-plaintext highlighter-rouge">log</code> folder and then run these visualization tools on another machine.</p> <h3 id="outputs">Outputs</h3> <p>The <code class="language-plaintext highlighter-rouge">output</code> folder is where I store predicted images, numpy arrays, and text files for evaluation (validation set) or submission (test set). In this way, the evaluation process can be decoupled from the training process, and it’s rather suitable when comparing different methods (or models from different runs).</p> <h2 id="misc"><strong>Misc</strong></h2> <h3 id="playground">Playground</h3> <p>The playground refers to a super environment for trying new codes, packages or getting familiar with some functions. It can also be a single jupyter notebook inside the project folder.</p> <h3 id="jobs-and-scripts">Jobs and Scripts</h3> <p>Shell scripts to bundle a series works for an environment, e.g. a <em>preprocessing-training-evaluation-visualization-test</em> pipeline.</p> <h3 id="debug-folder">Debug Folder</h3> <p>Debug folder stores anything provide insights from debugging process. It also serves as a transfer station between training nodes and my personal computer.</p> <h3 id="inline-notes">Inline Notes</h3> <p>I often writing notes as comments inside codes and <code class="language-plaintext highlighter-rouge">git</code> comments. They will be reorganized to note-taking apps during the review process.</p> <h2 id="final-thoughts"><strong>Final Thoughts</strong></h2> <p>I hope that creativity is the only constraint of my research, not engineering. Thanks for your time and feel free to share your thoughts with me.</p>]]></content><author><name></name></author><category term="all"/><category term="research"/><summary type="html"><![CDATA[A guide to implementing deep learning models for scientific research, with a focus on reproducibility, data management, and model evaluation.]]></summary></entry></feed>